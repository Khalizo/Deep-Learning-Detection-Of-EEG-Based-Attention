{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "import torch\n",
    "from ipynb.fs.full.evaluation import *\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from ipynb.fs.full.Data_Processing import *\n",
    "from sklearn import preprocessing\n",
    "import torch.utils.data as data_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler , LabelEncoder\n",
    "torch.set_printoptions(linewidth=120) #Display options for output\n",
    "torch.set_grad_enabled(True) # Already on by default\n",
    "torch.manual_seed(0)\n",
    "from torch_lr_finder import LRFinder\n",
    "import pickle\n",
    "import torch.utils.data as data_utils\n",
    "from collections import namedtuple\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "from pytorchtools import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, num_classes, combined):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.T = 120\n",
    "        self.combined = combined\n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Conv2d(1, 16, (1, 8), padding = 0)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16, False)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.padding1 = nn.ZeroPad2d((16, 17, 0, 1))\n",
    "        self.conv2 = nn.Conv2d(1, 4, (2, 32))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling2 = nn.MaxPool2d(2, 4)\n",
    "        \n",
    "        # Layer 3\n",
    "        self.padding2 = nn.ZeroPad2d((2, 1, 4, 3))\n",
    "        self.conv3 = nn.Conv2d(4, 4, (8, 4))\n",
    "        self.batchnorm3 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling3 = nn.MaxPool2d((2, 4))\n",
    "        \n",
    "        # FC Layer\n",
    "        # NOTE: This dimension will depend on the number of timestamps per sample in your data.\n",
    "        # I have 120 timepoints. \n",
    "        self.fc1 = nn.Linear(4*2*7, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "        x = x.float()\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.dropout(x, 0.1)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        # Layer 2\n",
    "        x = self.padding1(x)\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.dropout(x, 0.1)\n",
    "        x = self.pooling2(x)\n",
    "\n",
    "        # Layer 3\n",
    "        x = self.padding2(x)\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.dropout(x, 0.1)\n",
    "        x = self.pooling3(x)\n",
    "\n",
    "        # FC Layer\n",
    "        x = x.view(-1, 4*2*7)\n",
    "        if self.combined == False:       \n",
    "            x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    \n",
    "    # for 60 timepoints = 4*2*4 and -1\n",
    "    # 120 timepoints = 4* 2* 7 and -1\n",
    "    # https://discuss.pytorch.org/t/runtimeerror-shape-1-400-is-invalid-for-input-of-size/33354\n",
    "    # https://discuss.pytorch.org/t/valueerror-expected-input-batch-size-324-to-match-target-batch-size-4/24498/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN + RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ai.stackexchange.com/questions/3156/how-to-select-number-of-hidden-layers-and-number-of-memory-cells-in-an-lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(CNN, self).__init__()\n",
    "#         self.T = 120\n",
    "        \n",
    "#         # Layer 1\n",
    "#         self.conv1 = nn.Conv2d(1, 16, (1, 8), padding = 0)\n",
    "#         self.batchnorm1 = nn.BatchNorm2d(16, False)\n",
    "        \n",
    "#         # Layer 2\n",
    "#         self.padding1 = nn.ZeroPad2d((16, 17, 0, 1))\n",
    "#         self.conv2 = nn.Conv2d(1, 4, (2, 32))\n",
    "#         self.batchnorm2 = nn.BatchNorm2d(4, False)\n",
    "#         self.pooling2 = nn.MaxPool2d(2, 4)\n",
    "        \n",
    "#         # Layer 3\n",
    "#         self.padding2 = nn.ZeroPad2d((2, 1, 4, 3))\n",
    "#         self.conv3 = nn.Conv2d(4, 4, (8, 4))\n",
    "#         self.batchnorm3 = nn.BatchNorm2d(4, False)\n",
    "#         self.pooling3 = nn.MaxPool2d((2, 4))\n",
    "        \n",
    "#         # FC Layer\n",
    "#         # NOTE: This dimension will depend on the number of timestamps per sample in your data.\n",
    "#         # I have 120 timepoints. \n",
    "#         self.fc1 = nn.Linear(4*2*7, num_classes)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # Layer 1\n",
    "#         x = x.float()\n",
    "#         x = F.elu(self.conv1(x))\n",
    "#         x = self.batchnorm1(x)\n",
    "#         x = F.dropout(x, 0.1)\n",
    "#         x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "#         # Layer 2\n",
    "#         x = self.padding1(x)\n",
    "#         x = F.elu(self.conv2(x))\n",
    "#         x = self.batchnorm2(x)\n",
    "#         x = F.dropout(x, 0.1)\n",
    "#         x = self.pooling2(x)\n",
    "\n",
    "#         # Layer 3\n",
    "#         x = self.padding2(x)\n",
    "#         x = F.elu(self.conv3(x))\n",
    "#         x = self.batchnorm3(x)\n",
    "#         x = F.dropout(x, 0.1)\n",
    "#         x = self.pooling3(x)\n",
    "\n",
    "#         # FC Layer\n",
    "#         x = x.view(-1, 4*2*7)\n",
    "# # #         x = self.fc1(x)\n",
    "#         return x\n",
    "\n",
    "#     def predict(self, loader):\n",
    "#         predictions = []\n",
    "#         with torch.no_grad():\n",
    "#             for data in loader:\n",
    "#                 inputs = data[0].cuda(0)\n",
    "#                 labels = data[1].cuda(0)\n",
    "#                 outputs = net(inputs)\n",
    "#                 _, predicted = torch.max(outputs.data, 1)\n",
    "#                 predictions.append(predicted)\n",
    "#                 total += labels.size(0)\n",
    "#                 correct += (predicted == labels).sum().item()\n",
    "#         acc = (correct/total * 100)\n",
    "#         return predictions\n",
    "    \n",
    "\n",
    "class Combine(nn.Module):\n",
    "    def __init__(self, num_classes, combined):\n",
    "        super(Combine, self).__init__()\n",
    "        self.cnn = EEGNet(num_classes,combined)\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=56, \n",
    "            hidden_size=16, \n",
    "            num_layers=1,\n",
    "            batch_first=True)\n",
    "        self.linear = nn.Linear(16 ,num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, C, timepoints, channels = x.size()\n",
    "        c_in = x\n",
    "        c_out = self.cnn(c_in)\n",
    "        r_in = c_out.view(batch_size, -1 , c_out.shape[1])\n",
    "        r_out, (h_n, h_c) = self.rnn(r_in)\n",
    "        r_out2 = self.linear(r_out[:, -1, :])\n",
    "        \n",
    "        return F.log_softmax(r_out2, dim=1)\n",
    "    \n",
    "#     def predict(loader, net):\n",
    "#         predictions = []\n",
    "#         with torch.no_grad():\n",
    "#             for data in loader:\n",
    "#                 inputs = data[0].cuda(0)\n",
    "#                 labels = data[1].cuda(0)\n",
    "#                 outputs = net(inputs)\n",
    "#                 _, predicted = torch.max(outputs.data, 1)\n",
    "#                 predictions.append(predicted)\n",
    "#                 total += labels.size(0)\n",
    "#                 correct += (predicted == labels).sum().item()\n",
    "#         acc = (correct/total * 100)\n",
    "#         return predictions\n",
    "# X.shape - (#batch size, 1, #timepoints, #channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate function returns values of different criteria like accuracy, precision etc.**\n",
    "In case you face memory overflow issues, use batch size to control how many samples get evaluated at one time. Use a batch_size that is a factor of length of samples. This ensures that you won't miss any samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X, Y, params = [\"acc\"]):\n",
    "    results = []\n",
    "    batch_size = 100\n",
    "    \n",
    "    predicted = []\n",
    "    \n",
    "    for i in range(len(X)//batch_size):\n",
    "        s = i*batch_size\n",
    "        e = i*batch_size+batch_size\n",
    "        \n",
    "        inputs = Variable(torch.from_numpy(X[s:e]))\n",
    "        pred = model(inputs)\n",
    "        \n",
    "        predicted.append(pred.data.cpu().numpy())\n",
    "        \n",
    "        \n",
    "    inputs = Variable(torch.from_numpy(X))\n",
    "    predicted = model(inputs)\n",
    "    \n",
    "    predicted = predicted.data.cpu().numpy()\n",
    "    \n",
    "    for param in params:\n",
    "        if param == 'acc':\n",
    "            results.append(accuracy_score(Y, np.round(predicted)))\n",
    "        if param == \"auc\":\n",
    "            results.append(roc_auc_score(Y, predicted , multi_class=\"ovr\"))\n",
    "        if param == \"recall\":\n",
    "            results.append(recall_score(Y, np.round(predicted), average='macro'))\n",
    "        if param == \"precision\":\n",
    "            results.append(precision_score(Y, np.round(predicted) , average='macro'))\n",
    "        if param == \"fmeasure\":\n",
    "            precision = precision_score(Y, np.round(predicted) , average='macro')\n",
    "            recall = recall_score(Y, np.round(predicted) , average='macro')\n",
    "            results.append(2*precision*recall/ (precision+recall))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate random data**\n",
    "    \n",
    "*Data format:*\n",
    "\n",
    "Datatype - float32 (both X and Y)\n",
    "\n",
    "X.shape - (#samples, 1, #timepoints, #channels)\n",
    "\n",
    "Y.shape - (#samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "  return preds.argmax(dim=1).eq(labels).sum().item()### Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_finder():\n",
    "    model = EEGNet()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-7, weight_decay=1e-2)\n",
    "    lr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\n",
    "    lr_finder.range_test(train_loader, end_lr=100, num_iter=100, step_mode=\"exp\")\n",
    "    lr_finder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(loader, net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            inputs = data[0].cuda(0)\n",
    "            labels = data[1].cuda(0)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    acc = (correct/total * 100)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(loader, net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            inputs = data[0].cuda(0)\n",
    "            labels = data[1].cuda(0)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predictions.append(predicted.cpu().numpy())\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    acc = (correct/total * 100)\n",
    "    return np.concatenate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, train_loader, valid_loader,  num_classes, model, n_epochs, patience, verbose=False,):\n",
    "    \n",
    "    # choose between the EEGNet or EEGNet + RNN\n",
    "    if model == 'EEGNet':\n",
    "        net = EEGNet(num_classes, False).cuda(0)\n",
    "    else: \n",
    "        net = Combine(num_classes, True).cuda(0)\n",
    "        \n",
    "    #store the predictions and the losses\n",
    "    preds_list = [] # track the predictions\n",
    "    labels_list = [] # track the labels\n",
    "    train_losses = [] # to track the train loss as the model trains\n",
    "    valid_losses = [] # to track the validation loss as the model trains\n",
    "    avg_train_losses = [] # to track the average training loss per epoch as the model trains\n",
    "    avg_valid_losses = [] # to track the average validation loss per epoch as the model trains\n",
    "    \n",
    "    #Set the optimiser \n",
    "    optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "    #Initialise the early stopping\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "\n",
    "        model.train() # prep the model for training\n",
    "        for batch in train_loader:\n",
    "\n",
    "            inputs = batch[0].cuda(0)\n",
    "            labels = batch[1].cuda(0)\n",
    "            \n",
    "            preds = net(inputs) #forward pass: compute predicted outputs by passing inputs to the model\n",
    "            criterion=nn.BCEWithLogitsLoss() # calculate the loss\n",
    "            loss = F.cross_entropy(preds, labels.long()) # calculate loss\n",
    "            optimizer.zero_grad()# clear the gradients of all optimized variables\n",
    "            loss.backward()  # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            optimizer.step() # perform a single optimization step (parameter update)\n",
    "            train_losses.append(loss.item()) # record training loss\n",
    "            \n",
    "            #record the predictions and losses\n",
    "            preds_list.append(preds)\n",
    "            labels_list.append(labels)\n",
    "            total_loss += loss.item()\n",
    "            total_correct += get_num_correct(preds, labels)\n",
    "            \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval() # prep model for evaluation\n",
    "        for data, target in valid_loader:\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            # record validation loss\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "        # print training/validation statistics \n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        \n",
    "        epoch_len = len(str(n_epochs))\n",
    "        \n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f}')\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if verbose == True:             \n",
    "            print(\"epoch: {0}, Accuracy {1}, loss: {2}\".format(epoch, total_correct/len(X_train), loss))\n",
    "        \n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise(X_train, X_valid):\n",
    "    # standardize per channel\n",
    "    means = X_train.mean(axis=(0,2), keepdims=True)\n",
    "    stds = X_train.std(axis=(0,2), keepdims=True)\n",
    "    X_train = (X_train - means) / (stds)\n",
    "    X_valid = (X_valid - means) / (stds)\n",
    "    return X_train, X_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(lst): \n",
    "    return sum(lst) / len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_slice_in_list(s,l):\n",
    "    \"\"\"\n",
    "    Function for checking whether a slice is in a list. Mainly used for checking whether the \n",
    "    \"\"\"\n",
    "    len_s = len(s) #so we don't recompute length of s on every iteration\n",
    "    return any(s == l[i:len_s+i] for i in range(len(l) - len_s+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorise(y_train, y_valid):\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    y_valid = le.transform(y_valid)\n",
    "    return y_train, y_valid, le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plots(model_type, y_true, y_pred, user, label, n_epochs, model, eval_type ):\n",
    "    if model_type == 'clf':\n",
    "        # plot confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        saved_file = \"results/CNN/clf/confusion/k fold/{3}/{4}/User_{0}_{1}_epochs_{2}.png\".format(user,label,n_epochs,model, eval_type)\n",
    "        plot_confusion_matrix(cm, set(y_true), saved_file ,normalize=True)\n",
    "        \n",
    "        #plot model\n",
    "    if model_type == 'reg':\n",
    "        saved_file = \"results/CNN/reg/y vs y_pred/{3}/{4}/User_{0}_{1}_epochs_{2}.png\".format(user,label, n_epochs,model,eval_type)\n",
    "        plot_model(y_true, y_pred, user, label,file=saved_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(X_train, X_valid, y_train, y_valid):\n",
    "    \n",
    "    #Convert to 4D \n",
    "    X_train = X_train.reshape(X_train.shape[0],1,X_train.shape[1],X_train.shape[2])\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0],1, X_valid.shape[1],X_valid.shape[2])\n",
    "\n",
    "    # Create train and valid loader\n",
    "    train = data_utils.TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train).long())\n",
    "    valid = data_utils.TensorDataset(torch.Tensor(X_valid), torch.Tensor(y_valid))\n",
    "    train_loader = data_utils.DataLoader(train, batch_size=50, shuffle=True)\n",
    "    valid_loader = data_utils.DataLoader(valid, batch_size=50, shuffle=False)       \n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_predict(X,y, model_type, model, n_epochs, train_verbose, patience):\n",
    "    \"\"\"\n",
    "    Method for running 5 fold cross validation based on a given array of tests\n",
    "    \"\"\"\n",
    "    kf= KFold(n_splits = 5, shuffle = True, random_state =  1)\n",
    "    \n",
    "    if model_type == 'clf':\n",
    "        results = {\"Accuracy\":[], \"Precision\":[], \"Recall\":[], \"F1 Score Macro\":[],\n",
    "              \"F1 Score Micro\":[],\"Balanced Accuracy\":[]}\n",
    "    else:\n",
    "        results = {'RMSE':[], 'R2':[]}\n",
    "        \n",
    "    total_predictions = []\n",
    "    total_true = []\n",
    "    num_classes = 0\n",
    "    accuracy = []\n",
    "    fold = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        \n",
    "        #Train/valid split\n",
    "        X_train, X_valid = np.concatenate(X[train_index]), np.concatenate(X[test_index])\n",
    "        y_train, y_valid = np.concatenate(y[train_index]).astype('int'), np.concatenate(y[test_index]).astype('int')\n",
    "        size = len(X_train) + len(X_valid)\n",
    "        \n",
    "        # check the the classes in the validation set, if there are not in training set then skip\n",
    "        y_valid_classes = list(set(y_valid))\n",
    "        y_train_classes = list(set(y_train))\n",
    "        if is_slice_in_list(y_valid_classes, y_train_classes) == False: continue\n",
    "        \n",
    "        #standardise per channel\n",
    "        X_train, X_valid = standardise(X_train, X_valid)\n",
    "        \n",
    "        #label the categorical variables\n",
    "        y_train, y_valid, le = categorise(y_train, y_valid)\n",
    "        \n",
    "        #get loaders\n",
    "        train_loader, valid_loader = get_loaders(X_train, X_valid, y_train, y_valid)\n",
    "        \n",
    "         # count the number of classes\n",
    "        if len(set(y_train)) > num_classes:\n",
    "            num_classes = len(set(y_train))\n",
    "       \n",
    "        # train the network\n",
    "        time_start = time.time()\n",
    "        net = train(X_train, train_loader, valid_loader,  num_classes, model, n_epochs, patience, verbose=train_verbose)\n",
    "        fold += 1 \n",
    "        print('Fold  {0}! Time elapsed: {1} seconds'.format(fold, time.time()-time_start))\n",
    "        \n",
    "        # make predictions\n",
    "        y_pred = le.inverse_transform(predict(valid_loader, net))\n",
    "        y_true = le.inverse_transform(y_valid)\n",
    "     \n",
    "        #save total predictions and get results\n",
    "        total_predictions.append(y_pred)\n",
    "        total_true.append(y_true) \n",
    "        r = get_results(y_true, y_pred, model_type) #returns a dictionary of results   \n",
    "        valid_acc = get_accuracy(valid_loader, net)\n",
    "        for key in r: # loop through dictionary to add to all the scores to the results dictionary\n",
    "            results[key].append(r[key])\n",
    "        accuracy.append(valid_acc)\n",
    "\n",
    "    for key in results: # average out the results \n",
    "        results[key] = average(results[key])\n",
    "    accuracy = average(accuracy)    \n",
    "        \n",
    "    return results, np.concatenate(total_predictions), np.concatenate(total_true) , num_classes , size , accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get results\n",
    "#         Results = namedtuple(\"Results\",\"label train_acc test_acc user\")\n",
    "#         train_acc = get_accuracy(train_loader, test_loader, \"train\", net)\n",
    "#         test_acc = get_accuracy(train_loader, test_loader, \"test\", net)\n",
    "#         print(\"{0} results on User 1\\ntrain_acc: {1}\\tsample_size: {2}\\ntest_acc: {3}\\tsample_size: {4}\\n\".format(label, \n",
    "#                                                                                                train_acc,  len(X_train),\n",
    "#                                                                                                 test_acc, len(X_test)))\n",
    "#         results.append(Results(label, train_acc, test_acc,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on label attention\n",
      "epoch: 0, Accuracy 0.3536878674505612, loss: 1.1421358585357666\n",
      "epoch: 1, Accuracy 0.3995857830037413, loss: 1.4138399362564087\n",
      "epoch: 2, Accuracy 0.4146846606092998, loss: 1.2007286548614502\n",
      "epoch: 3, Accuracy 0.41481827899518975, loss: 1.4356062412261963\n",
      "epoch: 4, Accuracy 0.4195617316942811, loss: 1.4612804651260376\n",
      "epoch: 5, Accuracy 0.42109834313201494, loss: 1.408135175704956\n",
      "epoch: 6, Accuracy 0.4255077498663816, loss: 0.957423746585846\n",
      "epoch: 7, Accuracy 0.4259086050240513, loss: 1.0638480186462402\n",
      "epoch: 8, Accuracy 0.42757883484767506, loss: 1.0068063735961914\n",
      "epoch: 9, Accuracy 0.4284473543559594, loss: 1.1494348049163818\n",
      "Fold 1 completed 1! Time elapsed: 16.402443885803223 seconds\n",
      "epoch: 0, Accuracy 0.3402087630476905, loss: 1.696742296218872\n",
      "epoch: 1, Accuracy 0.3793987124195262, loss: 1.190195918083191\n",
      "epoch: 2, Accuracy 0.38546159134945934, loss: 1.3518861532211304\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-715-c60926736b04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mrun_cross_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'EEGNet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-715-c60926736b04>\u001b[0m in \u001b[0;36mrun_cross_user\u001b[0;34m(model, train_verbose)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# train and make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkfold_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_verbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m          \u001b[0;31m# get results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-714-e845b9967d16>\u001b[0m in \u001b[0;36mkfold_predict\u001b[0;34m(X, y, model_type, model, n_epochs, train_verbose, patience)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# train the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mtime_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_verbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mfold\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fold 1 completed {0}! Time elapsed: {1} seconds'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtime_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-707-23ab864d1e37>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(X_train, train_loader, num_classes, model, n_epochs, patience, verbose)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Pass batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-700-012459060646>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Layer 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    348\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    349\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 350\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_per_user(model, train_verbose):\n",
    "\n",
    "    time_original = time.time()\n",
    "\n",
    "    labels = [\"attention\", \"interest\", \"effort\"]\n",
    "\n",
    "    n_epochs = 100\n",
    "\n",
    "    patience = 10\n",
    "    window_size_samples = 120\n",
    "    saved_file = \"/cs/home/ybk1/Dissertation/data/saved user and test data/all_users_sampled_{0}_window_annotated_EEG_no_agg.pickle\".format(window_size_samples)\n",
    "    all_tests = load_file(saved_file)\n",
    "    users = list(all_tests.keys())\n",
    "    model_type = 'clf'\n",
    "    results = []\n",
    "    eval_type = 'per user'\n",
    "    \n",
    "    for user in users:\n",
    "        print(\"Working on user {0}\".format(user))\n",
    "\n",
    "        for label in labels:\n",
    "\n",
    "            time_start = time.time()\n",
    "            dt = all_tests[user] # dictionary of all the individual tests per user\n",
    "\n",
    "            X = np.array([np.array(x).astype(np.float32) for x in dt['inputs']]) # array of all the inputs for each test\n",
    "            y = np.array([np.array(x) for x in dt[label]]) #Convert the categories into labels\n",
    "\n",
    "            # K fold predict \n",
    "            r, y_pred, y_true, num_classes, size, accuracy = kfold_predict(X,y,model_type, model, n_epochs, train_verbose, patience)\n",
    "   \n",
    "            # get results and add them to the list\n",
    "            duration = time.time() - time_start\n",
    "            results.append(collate_results(r, user, label, duration, num_classes, size, model_type, n_epochs, window_size_samples))\n",
    "\n",
    "            #Save plots\n",
    "            save_plots(model_type, y_true, y_pred, user, label, n_epochs, model, eval_type )\n",
    "\n",
    "            print(\"Finished analysis on User {0}_{1}\".format(user,label))\n",
    "        print(\"Finished analysis on User {0}\".format(user))\n",
    "    results  = pd.DataFrame(results).to_csv(\"results/CNN/{3}/tabulated/k fold/{2}/{2}_performance_window_size_{0}_{1}_{4}.csv\".format(window_size_samples, n_epochs, model, model_type, eval_type), index=False )\n",
    "    final_duration = time.time()- time_original\n",
    "    print(\"All analyses are complete! Time elapsed: {0}\".format(final_duration))\n",
    "    return results\n",
    "        \n",
    "    \n",
    "def run_cross_user(model , train_verbose):\n",
    "    time_original = time.time()\n",
    "\n",
    "    labels = [\"attention\", \"interest\", \"effort\"]\n",
    "    results = [] # save all results in this list\n",
    "    patience = 20\n",
    "    n_epochs = 10\n",
    "    window_size_samples = 120\n",
    "    saved_file = \"/cs/home/ybk1/Dissertation/data/saved user and test data/all_users_sampled_{0}_window_annotated_EEG_agg.pickle\".format(window_size_samples)\n",
    "    all_tests_agg = load_file(saved_file)\n",
    "    users = all_tests_agg.keys()\n",
    "    model_type = 'clf'\n",
    "    user ='all'\n",
    "    eval_type = 'cross user'\n",
    "   \n",
    "\n",
    "    for label in labels:\n",
    "        print(\"Working on label {0}\".format(label))\n",
    "        time_start = time.time()\n",
    "\n",
    "        # Store each user in a list to prepare for cross-user analysis\n",
    "        X = np.array([all_tests_agg[user]['inputs'].astype(np.float32) for user in all_tests_agg])\n",
    "        y = np.array([all_tests_agg[user][label] for user in all_tests_agg])  \n",
    "\n",
    "        # train and make predictions\n",
    "        r, y_pred, y_true, num_classes, size, clf = kfold_predict(X,y,model_type, model, n_epochs, train_verbose, patience)\n",
    "\n",
    "         # get results\n",
    "        duration = time.time() - time_start\n",
    "        results.append(collate_results(r, user, label, duration, num_classes, size, model_type, n_epochs, window_size_samples))\n",
    "        \n",
    "        #Save plots\n",
    "        save_plots(model_type, y_true, y_pred, user, label, n_epochs, model, eval_type )\n",
    "                 \n",
    "        print(\"Finished analysis on label {0}. Time elapsed {1}\".format(label, time.time()-time_start))\n",
    "    print(\"Finished analysis on User {0}\".format(user))\n",
    "    results  = pd.DataFrame(results).to_csv(\"results/CNN/{3}/tabulated/k fold/{2}/{2}_performance_window_size_{0}_{1}_{4}.csv\".format(window_size_samples, n_epochs, model, model_type, eval_type), index=False )\n",
    "    final_duration = time.time()- time_original\n",
    "    print(\"All analyses are complete! Time elapsed: {0}\".format(final_duration))\n",
    "\n",
    "        \n",
    "run_cross_user('EEGNet', True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         X = dt[\"inputs\"]\n",
    "#         y = dt[label]\n",
    "\n",
    "#         #Convert the categories into labels \n",
    "#         le = LabelEncoder()\n",
    "#         y =  le.fit_transform(y)\n",
    "\n",
    "#         #Train/test split\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "#         # scale the data \n",
    "#         scaler = StandardScaler()\n",
    "#         X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "#         X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "#         #Convert to 4D \n",
    "#         X_train = X_train.reshape(X_train.shape[0],1,X_train.shape[1],X_train.shape[2])\n",
    "#         X_test = X_test.reshape(X_test.shape[0],1, X_test.shape[1],X_test.shape[2])\n",
    "\n",
    "#         #Create train and test loader\n",
    "#         train = data_utils.TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train).long())\n",
    "#         test = data_utils.TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test))\n",
    "#         train_loader = data_utils.DataLoader(train, batch_size=50, shuffle=True)\n",
    "#         test_loader = data_utils.DataLoader(test, batch_size=50, shuffle=False)\n",
    "\n",
    "#         # train the network\n",
    "#         net = train_network(X_train, train_loader, 'CNN', verbose=False)\n",
    "\n",
    "#         #get results\n",
    "#         Results = namedtuple(\"Results\",\"label train_acc test_acc user\")\n",
    "#         train_acc = get_accuracy(train_loader, test_loader, \"train\", net)\n",
    "#         test_acc = get_accuracy(train_loader, test_loader, \"test\", net)\n",
    "#         print(\"{0} results on User 1\\ntrain_acc: {1}\\tsample_size: {2}\\ntest_acc: {3}\\tsample_size: {4}\\n\".format(label, \n",
    "#                                                                                                train_acc,  len(X_train),\n",
    "#                                                                                                 test_acc, len(X_test)))\n",
    "#         results.append(Results(label, train_acc, test_acc,1))\n",
    "\n",
    "# results = pd.DataFrame(results)\n",
    "# results.to_csv(\"results/EEGNet/user1_results_sampled_annotated.csv\", index=False)\n",
    "# # results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
