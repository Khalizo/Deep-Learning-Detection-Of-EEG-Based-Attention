{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nTrialwise Decoding on BCIC IV 2a Dataset\n========================================\n\nThis tutorial shows you how to train and test deep learning models with\nBraindecode in a classical EEG setting: you have trials of data with\nlabels (e.g., Right Hand, Left Hand, etc.).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading and preprocessing the dataset\n-------------------------------------\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading\n~~~~~~~\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we load the data. In this tutorial, we use the functionality of\nbraindecode to load datasets through\n`MOABB <https://github.com/NeuroTechX/moabb>`__ to load the BCI\nCompetition IV 2a data.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>To load your own datasets either via mne or from\n   preprocessed X/y numpy arrays, see `MNE Dataset\n   Tutorial <./plot_mne_dataset_example.html>`__ and `Numpy Dataset\n   Tutorial <./plot_custom_dataset_example.html>`__.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.datasets.moabb import MOABBDataset\nimport mne\n\nsubject_id = 3\ndataset = MOABBDataset(dataset_name=\"BNCI2014001\", subject_ids=[subject_id])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preprocessing\n~~~~~~~~~~~~~\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we apply preprocessing like bandpass filtering to our dataset. You\ncan either apply functions provided by\n`mne.Raw <https://mne.tools/stable/generated/mne.io.Raw.html>`__ or\n`mne.Epochs <https://mne.tools/0.11/generated/mne.Epochs.html#mne.Epochs>`__\nor apply your own functions, either to the MNE object or the underlying\nnumpy array.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>These prepocessings are now directly applied to the loaded\n   data, and not on-the-fly applied as transformations in\n   PyTorch-libraries like\n   `torchvision <https://pytorch.org/docs/stable/torchvision/index.html>`__.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.datautil.preprocess import exponential_moving_standardize\nfrom braindecode.datautil.preprocess import MNEPreproc, NumpyPreproc, preprocess\n\nlow_cut_hz = 4.  # low cut frequency for filtering\nhigh_cut_hz = 38.  # high cut frequency for filtering\n# Parameters for exponential moving standardization\nfactor_new = 1e-3\ninit_block_size = 1000\n\npreprocessors = [\n    # keep only EEG sensors\n    MNEPreproc(fn='pick_types', eeg=True, meg=False, stim=False),\n    # convert from volt to microvolt, directly modifying the numpy array\n    NumpyPreproc(fn=lambda x: x * 1e6),\n    # bandpass filter\n    MNEPreproc(fn='filter', l_freq=low_cut_hz, h_freq=high_cut_hz),\n    # exponential moving standardization\n    NumpyPreproc(fn=exponential_moving_standardize, factor_new=factor_new,\n        init_block_size=init_block_size)\n]\n\n# Transform the data\npreprocess(dataset, preprocessors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cut Compute Windows\n~~~~~~~~~~~~~~~~~~~\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we cut out compute windows, the inputs for the deep networks during\ntraining. In the case of trialwise decoding, we just have to decide if\nwe want to cut out some part before and/or after the trial. For this\ndataset, in our work, it often was beneficial to also cut out 500 ms\nbefore the trial.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom braindecode.datautil.windowers import create_windows_from_events\n\ntrial_start_offset_seconds = -0.5\n# Extract sampling frequency, check that they are same in all datasets\nsfreq = dataset.datasets[0].raw.info['sfreq']\nassert all([ds.raw.info['sfreq'] == sfreq for ds in dataset.datasets])\n# Calculate the trial start offset in samples.\ntrial_start_offset_samples = int(trial_start_offset_seconds * sfreq)\n\n# Create windows using braindecode function for this. It needs parameters to define how\n# trials should be used.\nwindows_dataset = create_windows_from_events(\n    dataset,\n    trial_start_offset_samples=trial_start_offset_samples,\n    trial_stop_offset_samples=0,\n    preload=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split dataset into train and valid\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can easily split the dataset using additional info stored in the\ndescription attribute, in this case ``session`` column. We select\n``session_T`` for training and ``session_E`` for validation.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "splitted = windows_dataset.split('session')\ntrain_set = splitted['session_T']\nvalid_set = splitted['session_E']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create model\n------------\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we create the deep learning model! Braindecode comes with some\npredefined convolutional neural network architectures for raw\ntime-domain EEG. Here, we use the shallow ConvNet model from `Deep\nlearning with convolutional neural networks for EEG decoding and\nvisualization <https://arxiv.org/abs/1703.05051>`__. These models are\npure `PyTorch <https://pytorch.org>`__ deep learning models, therefore\nto use your own model, it just has to be a normal PyTorch\n`nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom braindecode.util import set_random_seeds\nfrom braindecode.models import ShallowFBCSPNet\n\ncuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\ndevice = 'cuda' if cuda else 'cpu'\nif cuda:\n    torch.backends.cudnn.benchmark = True\nseed = 20200220  # random seed to make results reproducible\n# Set random seed to be able to reproduce results\nset_random_seeds(seed=seed, cuda=cuda)\n\nn_classes=4\n# Extract number of chans and time steps from dataset\nn_chans = train_set[0][0].shape[0]\ninput_window_samples = train_set[0][0].shape[1]\n\nmodel = ShallowFBCSPNet(\n    n_chans,\n    n_classes,\n    input_window_samples=input_window_samples,\n    final_conv_length='auto',\n)\n\n# Send model to GPU\nif cuda:\n    model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training\n--------\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we train the network! EEGClassifier is a Braindecode object\nresponsible for managing the training of neural networks. It inherits\nfrom skorch.NeuralNetClassifier, so the training logic is the same as in\n`Skorch <https://skorch.readthedocs.io/en/stable/>`__.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note**: In this tutorial, we use some default parameters that we\nhave found to work well for motor decoding, however we strongly\nencourage you to perform your own hyperparameter optimization using\ncross validation on your training data.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skorch.callbacks import LRScheduler\nfrom skorch.helper import predefined_split\n\nfrom braindecode import EEGClassifier\n# These values we found good for shallow network:\nlr = 0.0625 * 0.01\nweight_decay = 0\n\n# For deep4 they should be:\n# lr = 1 * 0.01\n# weight_decay = 0.5 * 0.001\n\nbatch_size = 64\nn_epochs = 4\n\nclf = EEGClassifier(\n    model,\n    criterion=torch.nn.NLLLoss,\n    optimizer=torch.optim.AdamW,\n    train_split=predefined_split(valid_set),  # using valid_set for validation\n    optimizer__lr=lr,\n    optimizer__weight_decay=weight_decay,\n    batch_size=batch_size,\n    callbacks=[\n        \"accuracy\", (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n    ],\n    device=device,\n)\n# Model training for a specified number of epochs. `y` is None as it is already supplied\n# in the dataset.\nclf.fit(train_set, y=None, epochs=n_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot Results\n------------\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we use the history stored by Skorch throughout training to plot\naccuracy and loss curves.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\nimport pandas as pd\n# Extract loss and accuracy values for plotting from history object\nresults_columns = ['train_loss', 'valid_loss', 'train_accuracy', 'valid_accuracy']\ndf = pd.DataFrame(clf.history[:, results_columns], columns=results_columns,\n                  index=clf.history[:, 'epoch'])\n\n# get percent of misclass for better visual comparison to loss\ndf = df.assign(train_misclass=100 - 100 * df.train_accuracy,\n               valid_misclass=100 - 100 * df.valid_accuracy)\n\nplt.style.use('seaborn')\nfig, ax1 = plt.subplots(figsize=(8, 3))\ndf.loc[:, ['train_loss', 'valid_loss']].plot(\n    ax=ax1, style=['-', ':'], marker='o', color='tab:blue', legend=False, fontsize=14)\n\nax1.tick_params(axis='y', labelcolor='tab:blue', labelsize=14)\nax1.set_ylabel(\"Loss\", color='tab:blue', fontsize=14)\n\nax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n\ndf.loc[:, ['train_misclass', 'valid_misclass']].plot(\n    ax=ax2, style=['-', ':'], marker='o', color='tab:red', legend=False)\nax2.tick_params(axis='y', labelcolor='tab:red', labelsize=14)\nax2.set_ylabel(\"Misclassification Rate [%]\", color='tab:red', fontsize=14)\nax2.set_ylim(ax2.get_ylim()[0], 85)  # make some room for legend\nax1.set_xlabel(\"Epoch\", fontsize=14)\n\n# where some data has already been plotted to ax\nhandles = []\nhandles.append(Line2D([0], [0], color='black', linewidth=1, linestyle='-', label='Train'))\nhandles.append(Line2D([0], [0], color='black', linewidth=1, linestyle=':', label='Valid'))\nplt.legend(handles, [h.get_label() for h in handles], fontsize=14)\nplt.tight_layout()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}