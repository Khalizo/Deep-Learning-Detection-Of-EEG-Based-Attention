{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import namedtuple\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import random\n",
    "from ipynb.fs.full.EEG_Toolbox import *\n",
    "from ipynb.fs.full.evaluation import *\n",
    "from scipy.signal import butter, lfilter, freqz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents \n",
    "\n",
    "- Annotate the the datasets by using the timestamps\n",
    "- Reject datasets that do not have annotations, less than 5 paragraphs or duplicate timestamp\n",
    "\n",
    "\n",
    "### 1) Annotating and building the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_user_number(s):\n",
    "    \"\"\"\n",
    "    Extracts the user number from a given path\n",
    "    :s: path of the file\n",
    "    \"\"\"\n",
    "    s = str(s).split(\"_\")[0] # extract the user from the path\n",
    "    s = s.split(\"/\")[6] # split the path to isolate the user\n",
    "    s = s.replace(\"User\",\"\") # remove the word \"User\"\n",
    "    s = int(re.sub(r'[a-z]+', '', s, re.I)) # remove alphabetical characters\n",
    "    return s \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_test_number(s):\n",
    "    \"\"\"\n",
    "    Extracts the test number from a given path\n",
    "    :s: path of the file\n",
    "    \"\"\"\n",
    "    s = s.split(\"/\")[7]\n",
    "    s = s.split(\"_\")[1]\n",
    "    s = int(re.sub(r'[a-z]+', '', s, re.I))\n",
    "    return s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path(\"/cs/home/ybk1/Dissertation/Experiment Anonymised Version\")\n",
    "# p = Path(\"C://Users//User//OneDrive - University of St Andrews//Modules//CS5099//4. Data Documents//dataset//INSTRUMENTED DIGITAL AND PAPER READINGDATASET//Experiment Anonymised Version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_datasets(p):\n",
    "    \"\"\"\n",
    "    Loops through all the folders to find all the datasets and json files for training\n",
    "    p: path of the root folder\n",
    "    \"\"\"\n",
    "    File = namedtuple('File', 'name path size')\n",
    "    files = []\n",
    "    for item in p.glob('**/*'): # loops thorough all the files in all the sub-directories\n",
    "        if item.match('*rawEEGData.csv')  and \"baseline\" not in item.name:\n",
    "            name = item.name\n",
    "            path = Path.resolve(item).parent\n",
    "            size = item.stat().st_size\n",
    "\n",
    "            files.append(File(name,path, size )) # stores the name, path and size in named tuple\n",
    "    \n",
    "    df = pd.DataFrame(files)\n",
    "    df['user'] = df[\"path\"].apply(extract_user_number)\n",
    "    df.to_csv(\"All EEG files.csv\", index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_json(labels):\n",
    "    \"\"\"\n",
    "    Method for checking the json to see how many paragraphs it contains\n",
    "    and whether the timestamps are distinct\n",
    "    :labels: annotations.json in a DataFrame\n",
    "    \"\"\"\n",
    "    #check whether there are 5 paragraphs\n",
    "    no_para = len(labels)\n",
    "    are_timestamps_distinct = False\n",
    "    if no_para == 0: # checks if the json is empty\n",
    "        return no_para, are_timestamps_distinct\n",
    "    else:\n",
    "        if len(labels) != 5:\n",
    "            print (\"There are less than 5 paragraphs in the dataset\")\n",
    "        else:\n",
    "            print(\"There are 5 paragraphs in the dataset\")\n",
    "\n",
    "        #check whether the timestamps are distincts\n",
    "        cols = [\"timeRangeStart\", \"timeRangeEnd\"]\n",
    "        for col in cols:\n",
    "            array_length= len(array(labels[col]))\n",
    "            set_length = len(set(array(labels[col])))\n",
    "            if array_length == set_length:\n",
    "                print(col + \" has distinct values\")\n",
    "                are_timestamps_distinct = True\n",
    "                break\n",
    "            else: \n",
    "                print(col + \" does not have distinct values\")\n",
    "                are_timestamps_distinct = False\n",
    "                break\n",
    "\n",
    "        print(are_timestamps_distinct)\n",
    "        return no_para, are_timestamps_distinct \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(timestamp, annotation, labels):\n",
    "    \"\"\"\n",
    "    Method for adding the labels to the dataframe\n",
    "    \n",
    "    :return: relevant score of attention/interest/effort within the correct time range\n",
    "    :timestamp: timestamp at the current iteration\n",
    "    :annotation: annotation at the current interation\n",
    "    :labels: annotations.json in array format\n",
    "    \"\"\"\n",
    "    for row in labels:\n",
    "        time_start = row[-2]\n",
    "        time_end = row[-1]\n",
    "        ann_dict = {\"effort\": row[-3], \"attention\": row[-4], \"interest\": row[-5],\"para\": row[-6]}\n",
    "        if timestamp >= time_start and timestamp < time_end: # checks if the timestamp is witin the start and end range\n",
    "            return ann_dict[annotation] # returns the relevant score stored in the dictionary\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(data, labels):\n",
    "    \"\"\"\n",
    "    Method for processing the dataset by creating columns for effort, attention, interest and paragraph using the label function\n",
    "    :data: EEG dataset\n",
    "    :labels: annotations.json in array format\n",
    "    \"\"\"\n",
    "    annotations = [\"effort\", \"attention\", \"interest\", \"para\"]\n",
    "    for ann in annotations: # loops through the labels, and creates new columns based on the values within the timestamp range\n",
    "        data[ann] = data[\"Timestamp\"].apply(label, annotation=ann, labels=labels)\n",
    "    \n",
    "    initial_rows = len(data)\n",
    "    data = data.dropna()\n",
    "    final_rows = len(data)\n",
    "    dropped_rows = initial_rows - final_rows\n",
    "    print(\"Initial rows: {0}\\nFinal rows: {1}\\nDropped rows: {2}\".format(initial_rows, \n",
    "                                                                         final_rows, \n",
    "                                                                         dropped_rows))\n",
    "    return data , initial_rows, final_rows, dropped_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_datasets_to_csv(labeled_data, path):\n",
    "    \"\"\"\n",
    "    Exports the labeled data as three separate csv files for interest, effort and attention\n",
    "    :labeled_data: annotated datasets with columns for interest, effort and attention\n",
    "    :path: path to save the dataset\n",
    "    \"\"\"\n",
    "    X = labeled_data.iloc[:,:9] # inputs i.e channels and timestamp\n",
    "    labels = ['interest', 'effort', 'attention']\n",
    "    for label in labels:\n",
    "        dataset = pd.concat([X, labeled_data[label]], axis=1).to_csv(path +  \"/\" +'EEG_' + label + '_dataset.csv', index=False)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_all_datasets(df):\n",
    "    \"\"\"\n",
    "    Method for building all the training sets by applying annotations at relevant timestamps\n",
    "    and splitting the training sets into attention, interest and effort\n",
    "    :df: DataFrame that includes all of the datasets\n",
    "    :ds: dataset\n",
    "    \"\"\"\n",
    "    ds = array(df)\n",
    "    json = \"annotations.json\" # json file that contains the labels and annotations for the data\n",
    "    File = namedtuple('File', 'path user test no_para are_timestamps_distinct initial_rows final_rows dropped_rows')\n",
    "    files = []\n",
    "    \n",
    "    for row in ds:\n",
    "        ds_path = str(row[1]) # Get the path of the dataset\n",
    "        ds_csv = row[0] # get the name of the csv file\n",
    "        print(\"Working on this csv: {}\".format(ds_csv))\n",
    "        ds_user = row[3] # get the user number\n",
    "        ds_test = extract_test_number(ds_path)\n",
    "        ds_data = pd.read_csv(ds_path + \"/\" + ds_csv)\n",
    "        ds_labels = pd.read_json(ds_path + \"/\" + json)\n",
    "        no_para, are_timestamps_distinct = check_json(ds_labels) # check the json file\n",
    "                \n",
    "        ds_labels = array(ds_labels)\n",
    "        if are_timestamps_distinct == False:\n",
    "            print(\"Don't process\")\n",
    "            initial_rows, final_rows  = len(ds_data), len(ds_data)\n",
    "            dropped_rows = 0\n",
    "            files.append(File(ds_path, ds_user, ds_test, no_para,are_timestamps_distinct,\n",
    "                             initial_rows, final_rows, dropped_rows))          \n",
    "        else:\n",
    "            labeled_data, initial_rows, final_rows, dropped_rows = add_labels(ds_data, ds_labels) # Gets the labeled data\n",
    "#             export_datasets_to_csv(labeled_data, ds_path) # Exports the 3 datasets to the appropriate path\n",
    "            labeled_data.to_csv(ds_path + \"/\" + \"annotated_EEG.csv\", index=False)\n",
    "            files.append(File(ds_path, ds_user, ds_test, no_para, are_timestamps_distinct,\n",
    "                         initial_rows, final_rows, dropped_rows))\n",
    "            \n",
    "    training_files = pd.DataFrame(files)\n",
    "    training_files.to_csv(\"Trainingfiles.csv\", index=False)\n",
    "    return training_files\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_all_datasets_test():\n",
    "    \"\"\"\n",
    "    Test method for building a single group of training sets by applying annotations at relevant timestamps\n",
    "    and splitting the training sets into attention, interest and effort\n",
    "    :df: DataFrame that includes all of the datasets\n",
    "    :ds: dataset\n",
    "    \"\"\"\n",
    "    # User001_test1_BET_01_30-08-2019_recording0_U1567156556_EYETRACKER_cleanFixationData.csv\n",
    "    # /cs/home/ybk1/Dissertation/Experiment Anonymised Version/User001_Group_Test_30-08-2019--10-07-00/User001_test1_BET_01_30-08-2019/User001_test1_BET_01_30-08-2019_recording0\n",
    "    ds_path = \"/cs/home/ybk1/Dissertation/Experiment Anonymised Version/User001_Group_Test_30-08-2019--10-07-00/User001_test1_BET_01_30-08-2019/User001_test1_BET_01_30-08-2019_recording0\"\n",
    "    ds_csv = \"User001_test1_BET_01_30-08-2019_recording0_U1567156556_EEG_rawEEGData.csv\"\n",
    "    json = \"annotations.json\"\n",
    "    \n",
    "    File = namedtuple('File', 'path user no_para are_timestamps_distinct initial_rows final_rows dropped_rows')\n",
    "    files = []\n",
    "        \n",
    "    ds_user = 1 # get the user number\n",
    "    ds_data = pd.read_csv(ds_path + \"/\" + ds_csv)\n",
    "    ds_labels = pd.read_json(ds_path + \"/\" + json)\n",
    "    no_para, are_timestamps_distinct = check_json(ds_labels) # check the json file\n",
    "\n",
    "\n",
    "    ds_labels = array(ds_labels)\n",
    "    if are_timestamps_distinct == False:\n",
    "        print(\"Don't process\")\n",
    "        initial_rows, final_rows  = len(ds_data), len(ds_data)\n",
    "        dropped_rows = 0\n",
    "        files.append(File(ds_path, ds_user, no_para,are_timestamps_distinct,\n",
    "                         initial_rows, final_rows, dropped_rows))\n",
    "    else:\n",
    "        labeled_data, initial_rows, final_rows, dropped_rows = add_labels(ds_data, ds_labels) # Gets the labeled data\n",
    "#         export_datasets_to_csv(labeled_data, ds_path) # Exports the 3 datasets to the appropriate path\n",
    "        labeled_data.to_csv(ds_path + \"/\" + \"annotated_EEG.csv\", index=False)\n",
    "        files.append(File(ds_path, ds_user, no_para, are_timestamps_distinct,\n",
    "                         initial_rows, final_rows, dropped_rows))\n",
    "        \n",
    "    training_files = pd.DataFrame(files)\n",
    "    return training_files\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run this block to rebuild the datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_datasets = get_all_datasets(p)\n",
    "# build_all_datasets(all_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_all_datasets_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Cleaning the dataset build: extract viable datasets from the training files \n",
    "\n",
    "- remove the files that have zero final rows\n",
    "- remove files that have empty JSON's\n",
    "- remove files that only have 4 paragraphs\n",
    "- remove files with indistince timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_training_files():\n",
    "    \"\"\"\n",
    "    Method for removing:\n",
    "    - Files with zero final rows\n",
    "    - remove files with empty Json's\n",
    "    - Files with only 4 paragraphs\n",
    "    - Files with indistinct timestamp\n",
    "    \"\"\"\n",
    "    tf = pd.read_csv(\"Trainingfiles.csv\")\n",
    "    print(\"Initial length of training files: {}\".format(len(tf)))\n",
    "    \n",
    "    zero_final = tf['final_rows'] == 0 # remove the files with zero final row\n",
    "    empty_json = tf['no_para'] == 0 # remove files with empty JSON's\n",
    "    four_para = tf['no_para'] == 4 # remove files with only four paragraphs\n",
    "    indistinct_timestamp = tf['are_timestamps_distinct'] == False # remove files with indistinct timestamps\n",
    "    files_rm = [zero_final, empty_json, four_para, indistinct_timestamp] # store them all in a list\n",
    "    for file_rm in files_rm:\n",
    "        tf = tf.drop(tf[file_rm].index) # drop files sequentially\n",
    "    \n",
    "    tf = tf[(tf['user'] != 21) & (tf['user'] != 16)] # remove users 16 and 21 as they only have 2 and 1 test respectively\n",
    "    print(\"Final length of training files: {}\".format(len(tf)))\n",
    "    tf.to_csv(\"clean_trainingfiles.csv\", index=False)\n",
    "    return tf\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling the data \n",
    "\n",
    "In this part, the data will be sampled by created at "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples_from_test(df, slider, sample_size):\n",
    "    \"\"\"\n",
    "    Method for creating samples within a dataset\n",
    "    :df: test that is being sampled\n",
    "    :slider: the amount by which the window slides during sampling. The lower the number, the more samples.\n",
    "    \"\"\"\n",
    "    df = df.drop([\"Timestamp\", \" AdjustedUnix\"], axis=1) #remove unnecessary columns\n",
    "    Sample = namedtuple('Sample', 'inputs effort attention interest')\n",
    "    sampled_tests = []\n",
    "    \n",
    "    \n",
    "    # Group by paragraph and add each paragraph into an array\n",
    "    paragraphs = df.groupby('para') \n",
    "    paragraphs = [paragraphs.get_group(x) for x in paragraphs.groups]\n",
    "    \n",
    "    incorrect_length = 0\n",
    "    # Loop trough each paragraph to create samples\n",
    "    for para in paragraphs:\n",
    "        \n",
    "        if not len(para) > sample_size: # check the length of paragraph if it is bigger than the sample size\n",
    "            print (\"invalid\")\n",
    "            continue\n",
    "            \n",
    "        new_sample_length = len(para[0:sample_size])\n",
    "        counter = 0\n",
    "        while  new_sample_length >= sample_size:\n",
    "            \"\"\"\n",
    "            **Sliding window algorithm**\n",
    "            - Create new samples based on sample size and iterate using the slider size for size of overlap\n",
    "            - Create separate values for inputs, effort, attention, interest to add to a tuple\n",
    "            \"\"\"\n",
    "            new_sample = para[counter : counter + sample_size]\n",
    "            new_sample_length = len(new_sample)\n",
    "            \n",
    "            #checks new_sample length\n",
    "            if new_sample_length == sample_size:\n",
    "                # Extract the sample specific data and apply band pass filtering\n",
    "                inputs = array(new_sample.iloc[:, :8])\n",
    "#                 inputs = chebyBandpassFilter(inputs, [0.9, 1, 60, 63], gstop=40, gpass=1, fs=2048)[1]\n",
    "                \n",
    "                effort, attention, interest = new_sample[[\"effort\", \"attention\", \"interest\"]].T.values\n",
    "                sampled_tests.append(Sample(inputs, int(max(effort)), int(max(attention)), int(max(interest))))\n",
    "            else:\n",
    "                incorrect_length += 1\n",
    "                continue             \n",
    "         \n",
    "            # increase by slider\n",
    "            counter += slider\n",
    "        \n",
    "        \n",
    "    sampled_tests_df = pd.DataFrame(sampled_tests) #sampled list data frame\n",
    "    print(\"This sampled test has {0} samples\".format(len(sampled_tests_df)))\n",
    "    inputs_and_labels = {}\n",
    "    \n",
    "    inputs_list = sampled_tests_df['inputs'].values\n",
    "    inputs_list = np.rollaxis(np.dstack(inputs_list),-1) #combine all the inputs into 3D array\n",
    "    inputs_and_labels['inputs'] = inputs_list # add inputs into dictionary\n",
    "    labels = [\"effort\", \"attention\", \"interest\"]\n",
    "    for label in labels: \n",
    "        inputs_and_labels[label] = array(sampled_tests_df[label].values) #add lables to dictionary\n",
    "    \n",
    "    \n",
    "    return inputs_and_labels   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_samples_or_tests(clean_tf, slider, sample_size, user):\n",
    "    \"\"\"\n",
    "    Method for generating samples for all the tests with a default sample size of 120 and slide of 120. \n",
    "    Combines all tests of a user into a tuple consisting of inputs, attention, interest, effort. \n",
    "    Saves all tests in a dictionary\n",
    "    :clean_tf: clean training files\n",
    "    \"\"\"\n",
    "    #store all of user's test in a dictionary\n",
    "    user_test_paths = array(clean_tf[clean_tf['user'] == user][\"path\"])\n",
    "    user_tests = {}\n",
    "\n",
    "    file = \"annotated_EEG.csv\"\n",
    "    \n",
    "    # Loop through all the tests, generate samples and then append them to an array in the dictionary\n",
    "    inputs_and_labels = {\"inputs\":[], \"attention\":[], \"effort\":[], \"interest\":[]}\n",
    "    for test_path in user_test_paths:\n",
    "        print(\"Processing user {0} , test {1}\".format(extract_user_number(test_path), extract_test_number(test_path)))\n",
    "        test_file = test_path + \"/\" + file\n",
    "        test_dataset = pd.read_csv(test_file)\n",
    "        if len(test_dataset) == 0:\n",
    "            continue\n",
    "    #convert the test into windowed format with samples     \n",
    "        sampled_test_dataset = get_samples_from_test(test_dataset, slider, sample_size)\n",
    "    #add all the tests to the dictionary, inputs and labels\n",
    "        for key in inputs_and_labels:\n",
    "            inputs_and_labels[key].append(sampled_test_dataset[key])\n",
    "            \n",
    "    #loop through he dictionary and concatenate the list\n",
    "#     for key in inputs_and_labels:\n",
    "#         inputs_and_labels[key] = np.concatenate(inputs_and_labels[key], axis=0)\n",
    "#         print(\"Shape of {0}: {1}\".format(key,inputs_and_labels[key].shape ))\n",
    "\n",
    "    print(len(inputs_and_labels['inputs']))\n",
    "    print(\"Adding dictionary...\")\n",
    "    return inputs_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_test_per_user(agg=False):\n",
    "    df = pd.read_csv(\"clean_trainingfiles.csv\")\n",
    "    users = set(array(df['user']))\n",
    "    all_users = {}\n",
    "    \n",
    "    file = \"annotated_EEG.csv\"\n",
    "    for user in users:\n",
    "        test_paths = array(df[df['user'] == user][\"path\"])\n",
    "        test_list = []\n",
    "        for test_path in test_paths:\n",
    "            test_file = test_path + \"/\" + file\n",
    "            test_dataset = pd.read_csv(test_file)\n",
    "            test_list.append(test_dataset)\n",
    "            \n",
    "        if agg == True: test_list  = pd.concat(test_list)\n",
    "            \n",
    "        all_users[user] = test_list\n",
    "        print(\"Processed user {0}:\\tDataframe size: {1}\".format(user, len(test_list)))\n",
    "        \n",
    "    print(\"Saving dictionary...\")\n",
    "    saved_file = \"/cs/home/ybk1/Dissertation/data/all_tests_EEG_{0}.pickle\".format(agg)\n",
    "    with open(saved_file, 'wb') as handle:            \n",
    "        pickle.dump(all_users, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return all_users "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Apply Band pass filtering to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "all_users_path = \"/cs/home/ybk1/Dissertation/data/all_tests_EEG.pickle\"\n",
    "all_users_agg = load_file(all_users_path)\n",
    "\n",
    "\n",
    "lowcut = 0.1\n",
    "highcut = 40.0\n",
    "fs = 500.0\n",
    "order = 4\n",
    "\n",
    "def filter_df(df, lowcut, highcut, fs, order):\n",
    "    inputs = df.iloc[:,:8].columns\n",
    "    for channel in inputs: \n",
    "        df[channel] = pd.Series(butter_bandpass_filter(np.array(df[channel]), lowcut, highcut, fs, order=order))\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the frequency response for a few different orders.\n",
    "\n",
    "def plot_freq_response():\n",
    "    \n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    for order in [3, 4,  6]:\n",
    "        b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "        w, h = freqz(b, a, worN=2000)\n",
    "        plt.plot((fs * 0.5 / np.pi) * w, abs(h), label=\"order = %d\" % order)\n",
    "\n",
    "    plt.plot([0, 0.5 * fs], [np.sqrt(0.5), np.sqrt(0.5)],\n",
    "             '--', label='sqrt(0.5)')\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Gain')\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualisation\n",
    "\n",
    "- calculate the sampling frequency\n",
    "- Plot the EEG signal\n",
    "- Visualise the sampling of the data\n",
    "\n",
    "\n",
    "### a) Visualise the EEG signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotEEG():\n",
    "    import matplotlib.pyplot as plt\n",
    "    df = get_training_file()\n",
    "    channels = df.iloc[:, :8].columns\n",
    "    subset = df[:100]\n",
    "    print(len(channels))\n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    for i in range(len(channels)):\n",
    "        ax = fig.add_subplot(4,2,i+1)\n",
    "        ax.plot(subset['Timestamp'], subset[channels[i]])\n",
    "        ax.set_title(channels[i])\n",
    "        ax.set_xlabel(\"Timestamp\")\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Visualising the class distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_class_dist():\n",
    "    sampled_file = \"User_1_sampled_annotated_EEG.pickle\"\n",
    "    with open(sampled_file, 'rb') as handle:\n",
    "        user_1_tests = pickle.load(handle)\n",
    "    labels = [\"attention\", \"effort\", \"interest\"]\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=1 , figsize=(10,18))\n",
    "    fig = plt.figure(figsize=(30,20))\n",
    "\n",
    "    for i in range(3):\n",
    "        ax = pd.Series(user_1_tests[labels[i]]).value_counts().plot(kind=\"barh\" , title=labels[i] , ax=axes[i])\n",
    "        ax.set_xlabel(\"frequency\")\n",
    "        ax.set_ylabel(\"Score\")\n",
    "\n",
    "    # df1.plot(ax=axes[0])\n",
    "    # df2.plot(ax=axes[1])\n",
    "    # df2.plot(ax=axes[1])\n",
    "\n",
    "    #     fig = plt.figure()\n",
    "    #     ax = fig.add_subplot(4,2,i+1)\n",
    "    #     pd.Series(user_1_tests[label]).value_counts().plot(kind=\"barh\" , title=label)\n",
    "    #     ax.set_xlabel(\"frequency\")\n",
    "    #     ax.set_ylabel(\"Score\")\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
