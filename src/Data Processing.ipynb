{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import namedtuple\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traversing the folder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check the json data\n",
    "- check the whether the time start and time end are distinct\n",
    "- check whether there are 5 paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_user_number(s):\n",
    "    \"\"\"\n",
    "    Extracts the user number from a given path\n",
    "    \"\"\"\n",
    "    s = str(s).split(\"_\")[0] # extract the user from the path\n",
    "    s = s.split(\"/\")[6] # split the path to isolate the user\n",
    "    s = s.replace(\"User\",\"\") # remove the word \"User\"\n",
    "    s = int(re.sub(r'[a-z]+', '', s, re.I)) # remove alphabetical characters\n",
    "    return s \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path(\"/cs/home/ybk1/Dissertation/Experiment Anonymised Version\")\n",
    "# p = Path(\"C://Users//User//OneDrive - University of St Andrews//Modules//CS5099//4. Data Documents//dataset//INSTRUMENTED DIGITAL AND PAPER READINGDATASET//Experiment Anonymised Version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_datasets(p):\n",
    "    \"\"\"\n",
    "    Loops through all the folders to find all the datasets and json files for training\n",
    "    p: path of the root folder\n",
    "    \"\"\"\n",
    "    File = namedtuple('File', 'name path size')\n",
    "    files = []\n",
    "    for item in p.glob('**/*'): # loops thorough all the files in all the sub-directories\n",
    "        if item.match('*rawEEGData.csv')  and \"baseline\" not in item.name:\n",
    "            name = item.name\n",
    "            path = Path.resolve(item).parent\n",
    "            size = item.stat().st_size\n",
    "\n",
    "            files.append(File(name,path, size )) # stores the name, path and size in named tuple\n",
    "    \n",
    "    df = pd.DataFrame(files)\n",
    "    df['user'] = df[\"path\"].apply(extract_user_number)\n",
    "    df.to_csv(\"All EEG files.csv\", index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = get_all_datasets(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20    23\n",
       "23    21\n",
       "17    20\n",
       "7     19\n",
       "9     19\n",
       "15    18\n",
       "2     17\n",
       "25    16\n",
       "10    16\n",
       "3     16\n",
       "4     16\n",
       "5     16\n",
       "6     16\n",
       "8     16\n",
       "13    16\n",
       "11    16\n",
       "12    16\n",
       "24    16\n",
       "16    16\n",
       "18    16\n",
       "19    16\n",
       "21    16\n",
       "22    16\n",
       "1     16\n",
       "14    15\n",
       "Name: user, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_tests_per_user = all_datasets['user'].value_counts()\n",
    "no_tests_per_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the value counts we can see that: \n",
    "- user 20, 23, 17, 7, 9, 15, 2 - all have duplicates\n",
    "- user 14 is one short\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the datasets using the alldatasets csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_dir = \"/cs/home/ybk1/Dissertation/Experiment Anonymised Version/User001_Group_Test_30-08-2019--10-07-00/User001_test1_BET_01_30-08-2019/User001_test1_BET_01_30-08-2019_recording0/\"\n",
    "# s_csv = \"User001_test1_BET_01_30-08-2019_recording0_U1567156556_EEG_rawEEGData.csv\"\n",
    "# s_json = \"annotations.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_data = pd.read_csv(s_dir + s_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd_labels = pd.read_json(s_dir + s_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_json(labels):\n",
    "    \"\"\"\n",
    "    Method for checking the json to see how many paragraphs it contains\n",
    "    and whether the timestamps are distinct\n",
    "    \"\"\"\n",
    "    #check whether there are 5 paragraphs\n",
    "    no_para = len(labels)    \n",
    "    if len(labels) != 5:\n",
    "        print (\"There are less than 5 Paragraraphs in the dataset\")\n",
    "    else:\n",
    "        print(\"There are 5 paragraphs in the dataset\")\n",
    "    \n",
    "    #check whether the timestamps are distincts\n",
    "    are_timestamps_distinct = False\n",
    "    cols = [\"timeRangeStart\", \"timeRangeEnd\"]\n",
    "    for col in cols:\n",
    "        array_length= len(array(labels[col]))\n",
    "        set_length = len(set(array(labels[col])))\n",
    "        if array_length == set_length:\n",
    "            print(col + \" has distinct values\")\n",
    "            is_distinct = True\n",
    "        else: \n",
    "            print(col + \" does not have distinct values\")\n",
    "        \n",
    "    return no_para, are_timestamps_distinct \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5 paragraphs in the dataset\n",
      "timeRangeStart has distinct values\n",
      "timeRangeEnd has distinct values\n"
     ]
    }
   ],
   "source": [
    "check_json(pd_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "labels = array(pd.read_json(s_dir + s_json))\n",
    "labels\n",
    "\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first create zero columns\n",
    "# then iterate through the each row and check if it is there\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(timestamp, annotation, labels):\n",
    "    \"\"\"\n",
    "    Method for adding the labels to the dataframe\n",
    "    return: relevant score of attention/interest/effort within the correct time range\n",
    "    \"\"\"\n",
    "    for row in labels:\n",
    "        time_start = row[-2]\n",
    "        time_end = row[-1]\n",
    "        ann_dict = {\"effort\": row[-3], \"attention\": row[-4], \"interest\": row[-5],\"para\": row[-6]}\n",
    "        if timestamp >= time_start and timestamp < time_end: # checks if the timestamp is witin the start and end range\n",
    "            return ann_dict[annotation] # returns the relevant score stored in the dictionary\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data, labels):\n",
    "    \"\"\"\n",
    "    Method for processing the dataset by adding \n",
    "    \"\"\"\n",
    "    annotations = [\"effort\", \"attention\", \"interest\", \"para\"]\n",
    "    for ann in annotations: \n",
    "        data[ann] = data[\"Timestamp\"].apply(label, annotation=ann, labels=labels)\n",
    "    print(\"Processed: {}\".format(len(data)))\n",
    "    print(\"Empty rows \\n{}\".format(data.isnull().sum()))\n",
    "    data = data.dropna()\n",
    "    data.to_csv(\"pc.csv\")\n",
    "    print(len(data))\n",
    "    return data   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datasets(processed):\n",
    "    X = processed.iloc[:,:9]\n",
    "    labels = ['interest', 'effort', 'attention']\n",
    "    for label in labels:\n",
    "        dataset = pd.concat([X, processed[label]], axis=1).to_csv('EEG_' + label + '_dataset.csv')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 7955\n",
      "Empty rows \n",
      "C1                 0\n",
      "C2                 0\n",
      "C3                 0\n",
      "C4                 0\n",
      "C5                 0\n",
      "C6                 0\n",
      "C7                 0\n",
      "C8                 0\n",
      "Timestamp          0\n",
      " AdjustedUnix      0\n",
      "effort           411\n",
      "attention        411\n",
      "interest         411\n",
      "para             411\n",
      "dtype: int64\n",
      "7544\n"
     ]
    }
   ],
   "source": [
    "processed = process_data(s_data)\n",
    "generate_datasets(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_all_datasets(df):\n",
    "    \"\"\"\n",
    "    Method for building all the training sets by applying annotations at relevant timestamps\n",
    "    and split the training sets into attention, interest and effort\n",
    "    df: DataFrame that includes all of the datasets\n",
    "    \"\"\"\n",
    "    ds = array(df)\n",
    "    json = \"annotations.json\" # json file that contains the labels and annotations for the data\n",
    "    \n",
    "    for row in ds:\n",
    "        ds_path = row[1] # Get the path of the dataset\n",
    "        ds_csv = row[0] # get the name of the csv file\n",
    "        ds_data = pd.read_csv(ds_path + ds_csv)\n",
    "        ds_labels = pd.read_json(ds_path + ds_json)\n",
    "        no_para, are_timestamps_distinct = check_json(ds_labels) # check the json file\n",
    "        \n",
    "        if are_timestamps_distinct == False:\n",
    "            print(\"Don't process\")\n",
    "        else: \n",
    "            print(\"Process\")\n",
    "            \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
