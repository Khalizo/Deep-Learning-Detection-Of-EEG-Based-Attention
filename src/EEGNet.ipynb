{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from ipynb.fs.full.Data_Processing import extract_test_number, get_user_samples\n",
    "from sklearn import preprocessing\n",
    "import torch.utils.data as data_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler , LabelEncoder\n",
    "torch.set_printoptions(linewidth=120) #Display options for output\n",
    "torch.set_grad_enabled(True) # Already on by default\n",
    "torch.manual_seed(0)\n",
    "from torch_lr_finder import LRFinder\n",
    "import pickle\n",
    "import torch.utils.data as data_utils\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.T = 120\n",
    "        \n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Conv2d(1, 16, (1, 8), padding = 0)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16, False)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.padding1 = nn.ZeroPad2d((16, 17, 0, 1))\n",
    "        self.conv2 = nn.Conv2d(1, 4, (2, 32))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling2 = nn.MaxPool2d(2, 4)\n",
    "        \n",
    "        # Layer 3\n",
    "        self.padding2 = nn.ZeroPad2d((2, 1, 4, 3))\n",
    "        self.conv3 = nn.Conv2d(4, 4, (8, 4))\n",
    "        self.batchnorm3 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling3 = nn.MaxPool2d((2, 4))\n",
    "        \n",
    "        # FC Layer\n",
    "        # NOTE: This dimension will depend on the number of timestamps per sample in your data.\n",
    "        # I have 120 timepoints. \n",
    "        self.fc1 = nn.Linear(4*2*7, 5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "        x = x.float()\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.dropout(x, 0.1)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        # Layer 2\n",
    "        x = self.padding1(x)\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.dropout(x, 0.1)\n",
    "        x = self.pooling2(x)\n",
    "\n",
    "        # Layer 3\n",
    "        x = self.padding2(x)\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.dropout(x, 0.1)\n",
    "        x = self.pooling3(x)\n",
    "\n",
    "        # FC Layer\n",
    "        x = x.view(-1, 4*2*7)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate function returns values of different criteria like accuracy, precision etc.**\n",
    "In case you face memory overflow issues, use batch size to control how many samples get evaluated at one time. Use a batch_size that is a factor of length of samples. This ensures that you won't miss any samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X, Y, params = [\"acc\"]):\n",
    "    results = []\n",
    "    batch_size = 100\n",
    "    \n",
    "    predicted = []\n",
    "    \n",
    "    for i in range(len(X)//batch_size):\n",
    "        s = i*batch_size\n",
    "        e = i*batch_size+batch_size\n",
    "        \n",
    "        inputs = Variable(torch.from_numpy(X[s:e]))\n",
    "        pred = model(inputs)\n",
    "        \n",
    "        predicted.append(pred.data.cpu().numpy())\n",
    "        \n",
    "        \n",
    "    inputs = Variable(torch.from_numpy(X))\n",
    "    predicted = model(inputs)\n",
    "    \n",
    "    predicted = predicted.data.cpu().numpy()\n",
    "    \n",
    "    for param in params:\n",
    "        if param == 'acc':\n",
    "            results.append(accuracy_score(Y, np.round(predicted)))\n",
    "        if param == \"auc\":\n",
    "            results.append(roc_auc_score(Y, predicted , multi_class=\"ovr\"))\n",
    "        if param == \"recall\":\n",
    "            results.append(recall_score(Y, np.round(predicted), average='macro'))\n",
    "        if param == \"precision\":\n",
    "            results.append(precision_score(Y, np.round(predicted) , average='macro'))\n",
    "        if param == \"fmeasure\":\n",
    "            precision = precision_score(Y, np.round(predicted) , average='macro')\n",
    "            recall = recall_score(Y, np.round(predicted) , average='macro')\n",
    "            results.append(2*precision*recall/ (precision+recall))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate random data**\n",
    "    \n",
    "*Data format:*\n",
    "\n",
    "Datatype - float32 (both X and Y)\n",
    "\n",
    "X.shape - (#samples, 1, #timepoints, #channels)\n",
    "\n",
    "Y.shape - (#samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "  return preds.argmax(dim=1).eq(labels).sum().item()### Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_finder():\n",
    "    model = EEGNet()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-7, weight_decay=1e-2)\n",
    "    lr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\n",
    "    lr_finder.range_test(train_loader, end_lr=100, num_iter=100, step_mode=\"exp\")\n",
    "    lr_finder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(train_loader, test_loader, dataset_type):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        if dataset_type == \"test\":\n",
    "            loader = test_loader\n",
    "        elif dataset_type == \"train\":\n",
    "            loader = train_loader\n",
    "        for data in loader:\n",
    "            inputs = data[0].cuda(0)\n",
    "            labels = data[1].cuda(0)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    acc = (correct/total * 100)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(X_train, train_loader, verbose=False ):\n",
    "    net = EEGNet().cuda(0)\n",
    "    optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "\n",
    "    preds_list = []\n",
    "    labels_list = []\n",
    "    for epoch in range(500):\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "        #Get Batch\n",
    "\n",
    "            inputs = batch[0].cuda(0)\n",
    "            labels = batch[1].cuda(0)\n",
    "\n",
    "\n",
    "            preds = net(inputs) #Pass batch\n",
    "        #     criterion=nn.BCEWithLogitsLoss()\n",
    "            loss = F.cross_entropy(preds, labels.long()) #calculate loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()  #calculate gradients\n",
    "            optimizer.step() #update weights\n",
    "\n",
    "\n",
    "            preds_list.append(preds)\n",
    "            labels_list.append(labels)\n",
    "            total_loss += loss.item()\n",
    "            total_correct += get_num_correct(preds, labels)\n",
    "            \n",
    "        if verbose == True:             \n",
    "    #     Validation accuracy\n",
    "    #     params = [\"acc\", \"fmeasure\"]\n",
    "    #     print (params)\n",
    "    #     print (\"Train - \", evaluate(net, X_train, y_train_new, params))\n",
    "            print(\"epoch: {0}, Accuracy {1}, loss: {2}\".format(epoch, total_correct/len(X_train), loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention results on User 1\n",
      "train_acc: 26.373626373626376\tsample_size: 546\n",
      "test_acc: 27.73722627737226\tsample_size: 137\n",
      "\n",
      "interest results on User 1\n",
      "train_acc: 34.24908424908425\tsample_size: 546\n",
      "test_acc: 33.57664233576642\tsample_size: 137\n",
      "\n",
      "effort results on User 1\n",
      "train_acc: 23.626373626373624\tsample_size: 546\n",
      "test_acc: 21.16788321167883\tsample_size: 137\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>attention</td>\n",
       "      <td>26.373626</td>\n",
       "      <td>27.737226</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>interest</td>\n",
       "      <td>34.249084</td>\n",
       "      <td>33.576642</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>effort</td>\n",
       "      <td>23.626374</td>\n",
       "      <td>21.167883</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  train_acc   test_acc  user\n",
       "0  attention  26.373626  27.737226     1\n",
       "1   interest  34.249084  33.576642     1\n",
       "2     effort  23.626374  21.167883     1"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_file = \"User_1_sampled_annotated_EEG.pickle\"\n",
    "with open(sampled_file, 'rb') as handle:\n",
    "    dt = pickle.load(handle)\n",
    "\n",
    "labels = [\"attention\", \"interest\", \"effort\"]\n",
    "\n",
    "results = []\n",
    "for label in labels:\n",
    "    \n",
    "    X = dt[\"inputs\"]\n",
    "    y = dt[label]\n",
    "\n",
    "    #Convert the categories into labels \n",
    "    le = LabelEncoder()\n",
    "    y =  le.fit_transform(y)\n",
    "\n",
    "    #Train/test split\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "    # scale the data \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "    X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "    #Convert to 4D \n",
    "    X_train = X_train.reshape(X_train.shape[0],1,X_train.shape[1],X_train.shape[2])\n",
    "    X_test = X_test.reshape(X_test.shape[0],1, X_test.shape[1],X_test.shape[2])\n",
    "\n",
    "    #Create train and test loader\n",
    "    train = data_utils.TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train).long())\n",
    "    test = data_utils.TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test))\n",
    "    train_loader = data_utils.DataLoader(train, batch_size=50, shuffle=True)\n",
    "    test_loader = data_utils.DataLoader(test, batch_size=50, shuffle=False)\n",
    "    \n",
    "    # train the network\n",
    "    train_network(X_train, train_loader, verbose=False)\n",
    "    \n",
    "    #get results\n",
    "    Results = namedtuple(\"Results\",\"label train_acc test_acc user\")\n",
    "    train_acc = get_accuracy(train_loader, test_loader, \"train\")\n",
    "    test_acc = get_accuracy(train_loader, test_loader, \"test\")\n",
    "    print(\"{0} results on User 1\\ntrain_acc: {1}\\tsample_size: {2}\\ntest_acc: {3}\\tsample_size: {4}\\n\".format(label, \n",
    "                                                                                           train_acc,  len(X_train),\n",
    "                                                                                            test_acc, len(X_test)))\n",
    "    results.append(Results(label, train_acc, test_acc,1))\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results.to_csv(\"results/user1_results_120sample_120_slide_bandpass_2048.csv\", index=False)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
