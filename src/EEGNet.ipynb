{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "import torch\n",
    "from ipynb.fs.full.evaluation import *\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from ipynb.fs.full.Data_Processing import *\n",
    "from sklearn import preprocessing\n",
    "import torch.utils.data as data_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler , LabelEncoder\n",
    "torch.set_printoptions(linewidth=120) #Display options for output\n",
    "torch.set_grad_enabled(True) # Already on by default\n",
    "torch.manual_seed(0)\n",
    "from torch_lr_finder import LRFinder\n",
    "import pickle\n",
    "import torch.utils.data as data_utils\n",
    "from collections import namedtuple\n",
    "import time\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, num_classes, combined):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.T = 120\n",
    "        self.combined = combined\n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Conv2d(1, 16, (1, 8), padding = 0)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16, False)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.padding1 = nn.ZeroPad2d((16, 17, 0, 1))\n",
    "        self.conv2 = nn.Conv2d(1, 4, (2, 32))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling2 = nn.MaxPool2d(2, 4)\n",
    "        \n",
    "        # Layer 3\n",
    "        self.padding2 = nn.ZeroPad2d((2, 1, 4, 3))\n",
    "        self.conv3 = nn.Conv2d(4, 4, (8, 4))\n",
    "        self.batchnorm3 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling3 = nn.MaxPool2d((2, 4))\n",
    "        \n",
    "        # FC Layer\n",
    "        # NOTE: This dimension will depend on the number of timestamps per sample in your data.\n",
    "        # I have 120 timepoints. \n",
    "        self.fc1 = nn.Linear(4*2*7, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "        x = x.float()\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.dropout(x, 0.1)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        # Layer 2\n",
    "        x = self.padding1(x)\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.dropout(x, 0.1)\n",
    "        x = self.pooling2(x)\n",
    "\n",
    "        # Layer 3\n",
    "        x = self.padding2(x)\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.dropout(x, 0.1)\n",
    "        x = self.pooling3(x)\n",
    "\n",
    "        # FC Layer\n",
    "        x = x.view(-1, 4*2*7)\n",
    "        if self.combined == False:       \n",
    "            x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    \n",
    "    # for 60 timepoints = 4*2*4 and -1\n",
    "    # 120 timepoints = 4* 2* 7 and -1\n",
    "    # https://discuss.pytorch.org/t/runtimeerror-shape-1-400-is-invalid-for-input-of-size/33354\n",
    "    # https://discuss.pytorch.org/t/valueerror-expected-input-batch-size-324-to-match-target-batch-size-4/24498/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN + RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ai.stackexchange.com/questions/3156/how-to-select-number-of-hidden-layers-and-number-of-memory-cells-in-an-lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(CNN, self).__init__()\n",
    "#         self.T = 120\n",
    "        \n",
    "#         # Layer 1\n",
    "#         self.conv1 = nn.Conv2d(1, 16, (1, 8), padding = 0)\n",
    "#         self.batchnorm1 = nn.BatchNorm2d(16, False)\n",
    "        \n",
    "#         # Layer 2\n",
    "#         self.padding1 = nn.ZeroPad2d((16, 17, 0, 1))\n",
    "#         self.conv2 = nn.Conv2d(1, 4, (2, 32))\n",
    "#         self.batchnorm2 = nn.BatchNorm2d(4, False)\n",
    "#         self.pooling2 = nn.MaxPool2d(2, 4)\n",
    "        \n",
    "#         # Layer 3\n",
    "#         self.padding2 = nn.ZeroPad2d((2, 1, 4, 3))\n",
    "#         self.conv3 = nn.Conv2d(4, 4, (8, 4))\n",
    "#         self.batchnorm3 = nn.BatchNorm2d(4, False)\n",
    "#         self.pooling3 = nn.MaxPool2d((2, 4))\n",
    "        \n",
    "#         # FC Layer\n",
    "#         # NOTE: This dimension will depend on the number of timestamps per sample in your data.\n",
    "#         # I have 120 timepoints. \n",
    "#         self.fc1 = nn.Linear(4*2*7, num_classes)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # Layer 1\n",
    "#         x = x.float()\n",
    "#         x = F.elu(self.conv1(x))\n",
    "#         x = self.batchnorm1(x)\n",
    "#         x = F.dropout(x, 0.1)\n",
    "#         x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "#         # Layer 2\n",
    "#         x = self.padding1(x)\n",
    "#         x = F.elu(self.conv2(x))\n",
    "#         x = self.batchnorm2(x)\n",
    "#         x = F.dropout(x, 0.1)\n",
    "#         x = self.pooling2(x)\n",
    "\n",
    "#         # Layer 3\n",
    "#         x = self.padding2(x)\n",
    "#         x = F.elu(self.conv3(x))\n",
    "#         x = self.batchnorm3(x)\n",
    "#         x = F.dropout(x, 0.1)\n",
    "#         x = self.pooling3(x)\n",
    "\n",
    "#         # FC Layer\n",
    "#         x = x.view(-1, 4*2*7)\n",
    "# # #         x = self.fc1(x)\n",
    "#         return x\n",
    "\n",
    "#     def predict(self, loader):\n",
    "#         predictions = []\n",
    "#         with torch.no_grad():\n",
    "#             for data in loader:\n",
    "#                 inputs = data[0].cuda(0)\n",
    "#                 labels = data[1].cuda(0)\n",
    "#                 outputs = net(inputs)\n",
    "#                 _, predicted = torch.max(outputs.data, 1)\n",
    "#                 predictions.append(predicted)\n",
    "#                 total += labels.size(0)\n",
    "#                 correct += (predicted == labels).sum().item()\n",
    "#         acc = (correct/total * 100)\n",
    "#         return predictions\n",
    "    \n",
    "\n",
    "class Combine(nn.Module):\n",
    "    def __init__(self, num_classes, combined):\n",
    "        super(Combine, self).__init__()\n",
    "        self.cnn = EEGNet(num_classes,combined)\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=56, \n",
    "            hidden_size=16, \n",
    "            num_layers=1,\n",
    "            batch_first=True)\n",
    "        self.linear = nn.Linear(16 ,num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, C, timepoints, channels = x.size()\n",
    "        c_in = x\n",
    "        c_out = self.cnn(c_in)\n",
    "        r_in = c_out.view(batch_size, -1 , c_out.shape[1])\n",
    "        r_out, (h_n, h_c) = self.rnn(r_in)\n",
    "        r_out2 = self.linear(r_out[:, -1, :])\n",
    "        \n",
    "        return F.log_softmax(r_out2, dim=1)\n",
    "    \n",
    "#     def predict(loader, net):\n",
    "#         predictions = []\n",
    "#         with torch.no_grad():\n",
    "#             for data in loader:\n",
    "#                 inputs = data[0].cuda(0)\n",
    "#                 labels = data[1].cuda(0)\n",
    "#                 outputs = net(inputs)\n",
    "#                 _, predicted = torch.max(outputs.data, 1)\n",
    "#                 predictions.append(predicted)\n",
    "#                 total += labels.size(0)\n",
    "#                 correct += (predicted == labels).sum().item()\n",
    "#         acc = (correct/total * 100)\n",
    "#         return predictions\n",
    "# X.shape - (#batch size, 1, #timepoints, #channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate function returns values of different criteria like accuracy, precision etc.**\n",
    "In case you face memory overflow issues, use batch size to control how many samples get evaluated at one time. Use a batch_size that is a factor of length of samples. This ensures that you won't miss any samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X, Y, params = [\"acc\"]):\n",
    "    results = []\n",
    "    batch_size = 100\n",
    "    \n",
    "    predicted = []\n",
    "    \n",
    "    for i in range(len(X)//batch_size):\n",
    "        s = i*batch_size\n",
    "        e = i*batch_size+batch_size\n",
    "        \n",
    "        inputs = Variable(torch.from_numpy(X[s:e]))\n",
    "        pred = model(inputs)\n",
    "        \n",
    "        predicted.append(pred.data.cpu().numpy())\n",
    "        \n",
    "        \n",
    "    inputs = Variable(torch.from_numpy(X))\n",
    "    predicted = model(inputs)\n",
    "    \n",
    "    predicted = predicted.data.cpu().numpy()\n",
    "    \n",
    "    for param in params:\n",
    "        if param == 'acc':\n",
    "            results.append(accuracy_score(Y, np.round(predicted)))\n",
    "        if param == \"auc\":\n",
    "            results.append(roc_auc_score(Y, predicted , multi_class=\"ovr\"))\n",
    "        if param == \"recall\":\n",
    "            results.append(recall_score(Y, np.round(predicted), average='macro'))\n",
    "        if param == \"precision\":\n",
    "            results.append(precision_score(Y, np.round(predicted) , average='macro'))\n",
    "        if param == \"fmeasure\":\n",
    "            precision = precision_score(Y, np.round(predicted) , average='macro')\n",
    "            recall = recall_score(Y, np.round(predicted) , average='macro')\n",
    "            results.append(2*precision*recall/ (precision+recall))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate random data**\n",
    "    \n",
    "*Data format:*\n",
    "\n",
    "Datatype - float32 (both X and Y)\n",
    "\n",
    "X.shape - (#samples, 1, #timepoints, #channels)\n",
    "\n",
    "Y.shape - (#samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "  return preds.argmax(dim=1).eq(labels).sum().item()### Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_finder():\n",
    "    model = EEGNet()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-7, weight_decay=1e-2)\n",
    "    lr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\n",
    "    lr_finder.range_test(train_loader, end_lr=100, num_iter=100, step_mode=\"exp\")\n",
    "    lr_finder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(loader, net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            inputs = data[0].cuda(0)\n",
    "            labels = data[1].cuda(0)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    acc = (correct/total * 100)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(loader, net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            inputs = data[0].cuda(0)\n",
    "            labels = data[1].cuda(0)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predictions.append(predicted.cpu().numpy())\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    acc = (correct/total * 100)\n",
    "    return np.concatenate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(X_train, train_loader, num_classes, model, n_epochs,  verbose=False,):\n",
    "    \n",
    "    if model == 'EEGNet':\n",
    "        net = EEGNet(num_classes, False).cuda(0)\n",
    "    else: \n",
    "        net = Combine(num_classes, True).cuda(0)\n",
    "    \n",
    "    optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "\n",
    "    preds_list = []\n",
    "    labels_list = []\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "        #Get Batch\n",
    "\n",
    "            inputs = batch[0].cuda(0)\n",
    "            labels = batch[1].cuda(0)\n",
    "            \n",
    "            preds = net(inputs) #Pass batch\n",
    "        #     criterion=nn.BCEWithLogitsLoss()\n",
    "            loss = F.cross_entropy(preds, labels.long()) #calculate loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()  #calculate gradients\n",
    "            optimizer.step() #update weights\n",
    "\n",
    "\n",
    "            preds_list.append(preds)\n",
    "            labels_list.append(labels)\n",
    "            total_loss += loss.item()\n",
    "            total_correct += get_num_correct(preds, labels)\n",
    "            \n",
    "        if verbose == True:             \n",
    "    #     Validation accuracy\n",
    "    #     params = [\"acc\", \"fmeasure\"]\n",
    "    #     print (params)\n",
    "    #     print (\"Train - \", evaluate(net, X_train, y_train_new, params))\n",
    "            print(\"epoch: {0}, Accuracy {1}, loss: {2}\".format(epoch, total_correct/len(X_train), loss))\n",
    "        \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 8)"
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipynb.fs.full.evaluation import *\n",
    "data = load_file(\"examples/User_1_sampled_annotated_EEG.pickle\")\n",
    "data['inputs'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1397, 60, 8)"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['inputs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampled_file = \"examples/User_1_sampled_annotated_EEG.pickle\"\n",
    "# with open(sampled_file, 'rb') as handle:\n",
    "#     dt = pickle.load(handle)\n",
    "\n",
    "# labels = [\"attention\", \"interest\", \"effort\"]\n",
    "\n",
    "\n",
    "# results = []\n",
    "# for label in labels:\n",
    "    \n",
    "#     X = dt[\"inputs\"]\n",
    "#     y = dt[label]\n",
    "\n",
    "#     #Convert the categories into labels \n",
    "#     le = LabelEncoder()\n",
    "#     y =  le.fit_transform(y)\n",
    "\n",
    "#     #Train/test split\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "#     # scale the data \n",
    "#     scaler = StandardScaler()\n",
    "#     X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "#     X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "#     #Convert to 4D \n",
    "#     X_train = X_train.reshape(X_train.shape[0],1,X_train.shape[1],X_train.shape[2])\n",
    "#     X_test = X_test.reshape(X_test.shape[0],1, X_test.shape[1],X_test.shape[2])\n",
    "\n",
    "#     #Create train and test loader\n",
    "#     train = data_utils.TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train).long())\n",
    "#     test = data_utils.TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test))\n",
    "#     train_loader = data_utils.DataLoader(train, batch_size=50, shuffle=True)\n",
    "#     test_loader = data_utils.DataLoader(test, batch_size=50, shuffle=False)\n",
    "    \n",
    "#     # train the network\n",
    "#     net = train_network(X_train, train_loader, 'CNN', verbose=False)\n",
    "    \n",
    "#     #get results\n",
    "#     Results = namedtuple(\"Results\",\"label train_acc test_acc user\")\n",
    "#     train_acc = get_accuracy(train_loader, test_loader, \"train\", net)\n",
    "#     test_acc = get_accuracy(train_loader, test_loader, \"test\", net)\n",
    "#     print(\"{0} results on User 1\\ntrain_acc: {1}\\tsample_size: {2}\\ntest_acc: {3}\\tsample_size: {4}\\n\".format(label, \n",
    "#                                                                                            train_acc,  len(X_train),\n",
    "#                                                                                             test_acc, len(X_test)))\n",
    "#     results.append(Results(label, train_acc, test_acc,1))\n",
    "\n",
    "# results = pd.DataFrame(results)\n",
    "# results.to_csv(\"results/EEGNet/user1_results_sampled_annotated.csv\", index=False)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise(X_train, X_valid):\n",
    "    # standardize per channel\n",
    "    means = X_train.mean(axis=(0,2), keepdims=True)\n",
    "    stds = X_train.std(axis=(0,2), keepdims=True)\n",
    "    X_train = (X_train - means) / (stds)\n",
    "    X_valid = (X_valid - means) / (stds)\n",
    "    return X_train, X_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(lst): \n",
    "    return sum(lst) / len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_slice_in_list(s,l):\n",
    "    \"\"\"\n",
    "    Function for checking whether a slice is in a list. Mainly used for checking whether the \n",
    "    \"\"\"\n",
    "    len_s = len(s) #so we don't recompute length of s on every iteration\n",
    "    return any(s == l[i:len_s+i] for i in range(len(l) - len_s+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorise(y_train, y_valid):\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    y_valid = le.transform(y_valid)\n",
    "    return y_train, y_valid, le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plots(model_type, y_true, y_pred, user, label, n_epochs, model, eval_type ):\n",
    "    if model_type == 'clf':\n",
    "        # plot confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        saved_file = \"results/CNN/clf/confusion/k fold/{3}/{4}/User_{0}_{1}_epochs_{2}.png\".format(user,label,n_epochs,model, eval_type)\n",
    "        plot_confusion_matrix(cm, set(y_true), saved_file ,normalize=True)\n",
    "        \n",
    "        #plot model\n",
    "    if model_type == 'reg':\n",
    "        saved_file = \"results/CNN/reg/y vs y_pred/{3}/{4}/User_{0}_{1}_epochs_{2}.png\".format(user,label, n_epochs,model,eval_type)\n",
    "        plot_model(y_true, y_pred, user, label,file=saved_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(X_train, X_valid, y_train, y_valid):\n",
    "    \n",
    "    #Convert to 4D \n",
    "    X_train = X_train.reshape(X_train.shape[0],1,X_train.shape[1],X_train.shape[2])\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0],1, X_valid.shape[1],X_valid.shape[2])\n",
    "\n",
    "    # Create train and valid loader\n",
    "    train = data_utils.TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train).long())\n",
    "    valid = data_utils.TensorDataset(torch.Tensor(X_valid), torch.Tensor(y_valid))\n",
    "    train_loader = data_utils.DataLoader(train, batch_size=50, shuffle=True)\n",
    "    valid_loader = data_utils.DataLoader(valid, batch_size=50, shuffle=False)       \n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_predict(X,y, model_type, model, n_epochs):\n",
    "    \"\"\"\n",
    "    Method for running 5 fold cross validation based on a given array of tests\n",
    "    \"\"\"\n",
    "    kf= KFold(n_splits = 5, shuffle = True, random_state =  1)\n",
    "    \n",
    "    if model_type == 'clf':\n",
    "        results = {\"Accuracy\":[], \"Precision\":[], \"Recall\":[], \"F1 Score Macro\":[],\n",
    "              \"F1 Score Micro\":[],\"Balanced Accuracy\":[]}\n",
    "    else:\n",
    "        results = {'RMSE':[], 'R2':[]}\n",
    "        \n",
    "    total_predictions = []\n",
    "    total_true = []\n",
    "    num_classes = 0\n",
    "    accuracy = []\n",
    "    fold = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        \n",
    "        #Train/valid split\n",
    "        X_train, X_valid = np.concatenate(X[train_index]), np.concatenate(X[test_index])\n",
    "        y_train, y_valid = np.concatenate(y[train_index]).astype('int'), np.concatenate(y[test_index]).astype('int')\n",
    "        size = len(X_train) + len(X_valid)\n",
    "        \n",
    "        # check the the classes in the validation set, if there are not in training set then skip\n",
    "        y_valid_classes = list(set(y_valid))\n",
    "        y_train_classes = list(set(y_train))\n",
    "        if is_slice_in_list(y_valid_classes, y_train_classes) == False: continue\n",
    "        \n",
    "        #standardise per channel\n",
    "        X_train, X_valid = standardise(X_train, X_valid)\n",
    "        \n",
    "        #label the categorical variables\n",
    "        y_train, y_valid, le = categorise(y_train, y_valid)\n",
    "        \n",
    "        #get loaders\n",
    "        train_loader, valid_loader = get_loaders(X_train, X_valid, y_train, y_valid)\n",
    "        \n",
    "         # count the number of classes\n",
    "        if len(set(y_train)) > num_classes:\n",
    "            num_classes = len(set(y_train))\n",
    "       \n",
    "        # train the network\n",
    "        time_start = time.time()\n",
    "        net = train_network(X_train, train_loader, num_classes, model, n_epochs, verbose=False)\n",
    "        fold += 1 \n",
    "        print('Fold 1 completed {0}! Time elapsed: {1} seconds'.format(fold, time.time()-time_start))\n",
    "        \n",
    "        # make predictions\n",
    "        y_pred = le.inverse_transform(predict(valid_loader, net))\n",
    "        y_true = le.inverse_transform(y_valid)\n",
    "     \n",
    "        #save total predictions and get results\n",
    "        total_predictions.append(y_pred)\n",
    "        total_true.append(y_true) \n",
    "        r = get_results(y_true, y_pred, model_type) #returns a dictionary of results   \n",
    "        valid_acc = get_accuracy(valid_loader, net)\n",
    "        for key in r: # loop through dictionary to add to all the scores to the results dictionary\n",
    "            results[key].append(r[key])\n",
    "        accuracy.append(valid_acc)\n",
    "\n",
    "    for key in results: # average out the results \n",
    "        results[key] = average(results[key])\n",
    "    accuracy = average(accuracy)    \n",
    "        \n",
    "    return results, np.concatenate(total_predictions), np.concatenate(total_true) , num_classes , size , accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get results\n",
    "#         Results = namedtuple(\"Results\",\"label train_acc test_acc user\")\n",
    "#         train_acc = get_accuracy(train_loader, test_loader, \"train\", net)\n",
    "#         test_acc = get_accuracy(train_loader, test_loader, \"test\", net)\n",
    "#         print(\"{0} results on User 1\\ntrain_acc: {1}\\tsample_size: {2}\\ntest_acc: {3}\\tsample_size: {4}\\n\".format(label, \n",
    "#                                                                                                train_acc,  len(X_train),\n",
    "#                                                                                                 test_acc, len(X_test)))\n",
    "#         results.append(Results(label, train_acc, test_acc,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on label attention\n",
      "Training fold 1! Time elapsed: 16.307141304016113 seconds\n",
      "Training fold 2! Time elapsed: 17.350783824920654 seconds\n",
      "Training fold 3! Time elapsed: 17.204396724700928 seconds\n",
      "Training fold 4! Time elapsed: 20.78155541419983 seconds\n",
      "Training fold 5! Time elapsed: 18.60366916656494 seconds\n",
      "0.2226906831115162\n",
      "Normalized confusion matrix\n",
      "[[0.03231821 0.23741454 0.28589186 0.29148539 0.15288999]\n",
      " [0.02735828 0.26460932 0.2959072  0.27161304 0.14051215]\n",
      " [0.04978277 0.28222303 0.32041999 0.23805214 0.10952209]\n",
      " [0.03598732 0.26626888 0.33078501 0.25191124 0.11504755]\n",
      " [0.03336951 0.28594683 0.36597938 0.21134021 0.10336408]]\n",
      "Finished analysis on label attention\n",
      "Working on label interest\n"
     ]
    }
   ],
   "source": [
    "def run_per_user(model):\n",
    "\n",
    "    time_original = time.time()\n",
    "\n",
    "    labels = [\"attention\", \"interest\", \"effort\"]\n",
    "\n",
    "    n_epochs = 100\n",
    "\n",
    "    window_size_samples = 120\n",
    "    saved_file = \"/cs/home/ybk1/Dissertation/data/saved user and test data/all_users_sampled_{0}_window_annotated_EEG_no_agg.pickle\".format(window_size_samples)\n",
    "    all_tests = load_file(saved_file)\n",
    "    users = list(all_tests.keys())\n",
    "    model_type = 'clf'\n",
    "    results = []\n",
    "    eval_type = 'per user'\n",
    "    \n",
    "    for user in users:\n",
    "        print(\"Working on user {0}\".format(user))\n",
    "\n",
    "        for label in labels:\n",
    "\n",
    "            time_start = time.time()\n",
    "            dt = all_tests[user] # dictionary of all the individual tests per user\n",
    "\n",
    "            X = np.array([np.array(x).astype(np.float32) for x in dt['inputs']]) # array of all the inputs for each test\n",
    "            y = np.array([np.array(x) for x in dt[label]]) #Convert the categories into labels\n",
    "\n",
    "            # K fold predict \n",
    "            r, y_pred, y_true, num_classes, size, accuracy = kfold_predict(X,y,model_type, model, n_epochs)\n",
    "   \n",
    "            # get results and add them to the list\n",
    "            duration = time.time() - time_start\n",
    "            results.append(collate_results(r, user, label, duration, num_classes, size, model_type, n_epochs, window_size_samples))\n",
    "\n",
    "            #Save plots\n",
    "            save_plots(model_type, y_true, y_pred, user, label, n_epochs, model, eval_type )\n",
    "\n",
    "            print(\"Finished analysis on User {0}_{1}\".format(user,label))\n",
    "        print(\"Finished analysis on User {0}\".format(user))\n",
    "    results  = pd.DataFrame(results).to_csv(\"results/CNN/{3}/tabulated/k fold/{2}/{2}_performance_window_size_{0}_{1}_{4}.csv\".format(window_size_samples, n_epochs, model, model_type, eval_type), index=False )\n",
    "    final_duration = time.time()- time_original\n",
    "    print(\"All analyses are complete! Time elapsed: {0}\".format(final_duration))\n",
    "    return results\n",
    "        \n",
    "    \n",
    "def run_cross_user(model):\n",
    "    time_original = time.time()\n",
    "\n",
    "    labels = [\"attention\", \"interest\", \"effort\"]\n",
    "    results = [] # save all results in this list\n",
    "    \n",
    "    n_epochs = 10\n",
    "    window_size_samples = 120\n",
    "    saved_file = \"/cs/home/ybk1/Dissertation/data/saved user and test data/all_users_sampled_{0}_window_annotated_EEG_agg.pickle\".format(window_size_samples)\n",
    "    all_tests_agg = load_file(saved_file)\n",
    "    users = all_tests_agg.keys()\n",
    "    model_type = 'clf'\n",
    "    user ='all'\n",
    "    eval_type = 'cross user'\n",
    "   \n",
    "\n",
    "    for label in labels:\n",
    "        print(\"Working on label {0}\".format(label))\n",
    "        time_start = time.time()\n",
    "\n",
    "        # Store each user in a list to prepare for cross-user analysis\n",
    "        X = np.array([all_tests_agg[user]['inputs'].astype(np.float32) for user in all_tests_agg])\n",
    "        y = np.array([all_tests_agg[user][label] for user in all_tests_agg])  \n",
    "\n",
    "        # train and make predictions\n",
    "        r, y_pred, y_true, num_classes, size, clf = kfold_predict(X,y,model_type, model, n_epochs)\n",
    "        print(r['Accuracy'])\n",
    "\n",
    "         # get results\n",
    "        duration = time.time() - time_start\n",
    "        results.append(collate_results(r, user, label, duration, num_classes, size, model_type, n_epochs, window_size_samples))\n",
    "        \n",
    "        #Save plots\n",
    "        save_plots(model_type, y_true, y_pred, user, label, n_epochs, model, eval_type )\n",
    "                 \n",
    "        print(\"Finished analysis on label {0}. Time elapsed {1}\".format(label, time.time))\n",
    "    print(\"Finished analysis on User {0}\".format(user))\n",
    "    results  = pd.DataFrame(results).to_csv(\"results/CNN/{3}/tabulated/k fold/{2}/{2}_performance_window_size_{0}_{1}_{4}.csv\".format(window_size_samples, n_epochs, model, model_type, eval_type), index=False )\n",
    "    final_duration = time.time()- time_original\n",
    "    print(\"All analyses are complete! Time elapsed: {0}\".format(final_duration))\n",
    "\n",
    "        \n",
    "run_cross_user('EEGNet')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         X = dt[\"inputs\"]\n",
    "#         y = dt[label]\n",
    "\n",
    "#         #Convert the categories into labels \n",
    "#         le = LabelEncoder()\n",
    "#         y =  le.fit_transform(y)\n",
    "\n",
    "#         #Train/test split\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "#         # scale the data \n",
    "#         scaler = StandardScaler()\n",
    "#         X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "#         X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "#         #Convert to 4D \n",
    "#         X_train = X_train.reshape(X_train.shape[0],1,X_train.shape[1],X_train.shape[2])\n",
    "#         X_test = X_test.reshape(X_test.shape[0],1, X_test.shape[1],X_test.shape[2])\n",
    "\n",
    "#         #Create train and test loader\n",
    "#         train = data_utils.TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train).long())\n",
    "#         test = data_utils.TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test))\n",
    "#         train_loader = data_utils.DataLoader(train, batch_size=50, shuffle=True)\n",
    "#         test_loader = data_utils.DataLoader(test, batch_size=50, shuffle=False)\n",
    "\n",
    "#         # train the network\n",
    "#         net = train_network(X_train, train_loader, 'CNN', verbose=False)\n",
    "\n",
    "#         #get results\n",
    "#         Results = namedtuple(\"Results\",\"label train_acc test_acc user\")\n",
    "#         train_acc = get_accuracy(train_loader, test_loader, \"train\", net)\n",
    "#         test_acc = get_accuracy(train_loader, test_loader, \"test\", net)\n",
    "#         print(\"{0} results on User 1\\ntrain_acc: {1}\\tsample_size: {2}\\ntest_acc: {3}\\tsample_size: {4}\\n\".format(label, \n",
    "#                                                                                                train_acc,  len(X_train),\n",
    "#                                                                                                 test_acc, len(X_test)))\n",
    "#         results.append(Results(label, train_acc, test_acc,1))\n",
    "\n",
    "# results = pd.DataFrame(results)\n",
    "# results.to_csv(\"results/EEGNet/user1_results_sampled_annotated.csv\", index=False)\n",
    "# # results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
