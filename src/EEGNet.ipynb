{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "import torch\n",
    "from ipynb.fs.full.evaluation import *\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from ipynb.fs.full.Data_Processing import *\n",
    "from sklearn import preprocessing\n",
    "import torch.utils.data as data_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler , LabelEncoder\n",
    "torch.set_printoptions(linewidth=120) #Display options for output\n",
    "torch.set_grad_enabled(True) # Already on by default\n",
    "torch.manual_seed(0)\n",
    "from torch_lr_finder import LRFinder\n",
    "import pickle\n",
    "import torch.utils.data as data_utils\n",
    "from collections import namedtuple\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "from pytorchtools import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "if cuda:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, num_classes, combined, fc_size):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.T = 120\n",
    "        self.combined = combined\n",
    "        self.fc_size = fc_size\n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Conv2d(1, 16, (1, 8), padding = 0)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16, False)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.padding1 = nn.ZeroPad2d((16, 17, 0, 1))\n",
    "        self.conv2 = nn.Conv2d(1, 4, (2, 32))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling2 = nn.MaxPool2d(2, 4)\n",
    "        \n",
    "        # Layer 3\n",
    "        self.padding2 = nn.ZeroPad2d((2, 1, 4, 3))\n",
    "        self.conv3 = nn.Conv2d(4, 4, (8, 4))\n",
    "        self.batchnorm3 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling3 = nn.MaxPool2d((2, 4))\n",
    "        \n",
    "        # FC Layer\n",
    "        # NOTE: This dimension will depend on the number of timestamps per sample in your data.\n",
    "        # I have 120 timepoints. \n",
    "        self.fc1 = nn.Linear(fc_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "        x = x.float()\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.dropout(x, 0.9)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        # Layer 2\n",
    "        x = self.padding1(x)\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.dropout(x, 0.9)\n",
    "        x = self.pooling2(x)\n",
    "\n",
    "        # Layer 3\n",
    "        x = self.padding2(x)\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.dropout(x, 0.9)\n",
    "        x = self.pooling3(x)\n",
    " \n",
    "        # FC Layer\n",
    "        x = x.view(-1, self.fc_size)\n",
    "        if self.combined == False:       \n",
    "            x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    \n",
    "    # for 60 timepoints = 4*2*4 and -1\n",
    "    # 120 timepoints = 4* 2* 7 and -1\n",
    "    # https://discuss.pytorch.org/t/runtimeerror-shape-1-400-is-invalid-for-input-of-size/33354\n",
    "    # https://discuss.pytorch.org/t/valueerror-expected-input-batch-size-324-to-match-target-batch-size-4/24498/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN + RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ai.stackexchange.com/questions/3156/how-to-select-number-of-hidden-layers-and-number-of-memory-cells-in-an-lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "\n",
    "class Combine(nn.Module):\n",
    "    def __init__(self, num_classes, combined, fc_size):\n",
    "        super(Combine, self).__init__()\n",
    "        self.cnn = EEGNet(num_classes,combined, fc_size)\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=fc_size, \n",
    "            hidden_size=16, \n",
    "            num_layers=1,\n",
    "            batch_first=True)\n",
    "        self.linear = nn.Linear(16 ,num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, C, timepoints, channels = x.size()\n",
    "        c_in = x\n",
    "        c_out = self.cnn(c_in)\n",
    "        r_in = c_out.view(batch_size, -1 , c_out.shape[1])\n",
    "        r_out, (h_n, h_c) = self.rnn(r_in)\n",
    "        r_out2 = self.linear(r_out[:, -1, :])\n",
    "        \n",
    "        return F.log_softmax(r_out2, dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate function returns values of different criteria like accuracy, precision etc.**\n",
    "In case you face memory overflow issues, use batch size to control how many samples get evaluated at one time. Use a batch_size that is a factor of length of samples. This ensures that you won't miss any samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X, Y, params = [\"acc\"]):\n",
    "    results = []\n",
    "    batch_size = 100\n",
    "    \n",
    "    predicted = []\n",
    "    \n",
    "    for i in range(len(X)//batch_size):\n",
    "        s = i*batch_size\n",
    "        e = i*batch_size+batch_size\n",
    "        \n",
    "        inputs = Variable(torch.from_numpy(X[s:e]))\n",
    "        pred = model(inputs)\n",
    "        \n",
    "        predicted.append(pred.data.cpu().numpy())\n",
    "        \n",
    "        \n",
    "    inputs = Variable(torch.from_numpy(X))\n",
    "    predicted = model(inputs)\n",
    "    \n",
    "    predicted = predicted.data.cpu().numpy()\n",
    "    \n",
    "    for param in params:\n",
    "        if param == 'acc':\n",
    "            results.append(accuracy_score(Y, np.round(predicted)))\n",
    "        if param == \"auc\":\n",
    "            results.append(roc_auc_score(Y, predicted , multi_class=\"ovr\"))\n",
    "        if param == \"recall\":\n",
    "            results.append(recall_score(Y, np.round(predicted), average='macro'))\n",
    "        if param == \"precision\":\n",
    "            results.append(precision_score(Y, np.round(predicted) , average='macro'))\n",
    "        if param == \"fmeasure\":\n",
    "            precision = precision_score(Y, np.round(predicted) , average='macro')\n",
    "            recall = recall_score(Y, np.round(predicted) , average='macro')\n",
    "            results.append(2*precision*recall/ (precision+recall))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate random data**\n",
    "    \n",
    "*Data format:*\n",
    "\n",
    "Datatype - float32 (both X and Y)\n",
    "\n",
    "X.shape - (#samples, 1, #timepoints, #channels)\n",
    "\n",
    "Y.shape - (#samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "  return preds.argmax(dim=1).eq(labels).sum().item()### Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_finder():\n",
    "    model = EEGNet().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-7, weight_decay=1e-2)\n",
    "    lr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\n",
    "    lr_finder.range_test(train_loader, end_lr=100, num_iter=100, step_mode=\"exp\")\n",
    "    lr_finder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(loader, net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            inputs = data[0].to(device)\n",
    "            labels = data[1].to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    acc = (correct/total * 100)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(loader, net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            inputs = data[0].to(device)\n",
    "            labels = data[1].to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predictions.append(predicted.cpu().numpy())\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    acc = (correct/total * 100)\n",
    "    return np.concatenate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, train_loader, valid_loader,  num_classes, model, n_epochs, patience, train_verbose, fc_size):\n",
    "    \n",
    "    # choose between the EEGNet or EEGNet + RNN\n",
    "    if model == 'EEGNet':\n",
    "        net = EEGNet(num_classes, False, fc_size).to(device)\n",
    "    if model == 'Hybrid':\n",
    "        net = Combine(num_classes, True, fc_size).to(device)\n",
    "        \n",
    "    #store the predictions and the losses\n",
    "    preds_list = [] # track the predictions\n",
    "    labels_list = [] # track the labels\n",
    "    train_losses = [] # to track the train loss as the model trains\n",
    "    valid_losses = [] # to track the validation loss as the model trains\n",
    "    avg_train_losses = [] # to track the average training loss per epoch as the model trains\n",
    "    avg_valid_losses = [] # to track the average validation loss per epoch as the model trains\n",
    "    \n",
    "    #Set the optimiser \n",
    "    optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "    #Initialise the early stopping\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=train_verbose, path='checkpoint2.pt')\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "\n",
    "        net.train() # prep the model for training\n",
    "        for batch in train_loader:\n",
    "\n",
    "            inputs = batch[0].to(device)\n",
    "            labels = batch[1].to(device)\n",
    "            \n",
    "            preds = net(inputs) #forward pass: compute predicted outputs by passing inputs to the model\n",
    "            criterion=nn.BCEWithLogitsLoss() # calculate the loss\n",
    "            loss = F.cross_entropy(preds, labels.long()) # calculate loss\n",
    "            optimizer.zero_grad()# clear the gradients of all optimized variables\n",
    "            loss.backward()  # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            optimizer.step() # perform a single optimization step (parameter update)\n",
    "            train_losses.append(loss.item()) # record training loss\n",
    "            \n",
    "            #record the predictions and losses\n",
    "            preds_list.append(preds)\n",
    "            labels_list.append(labels)\n",
    "            total_loss += loss.item()\n",
    "            total_correct += get_num_correct(preds, labels)\n",
    "            \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        net.eval() # prep model for evaluation\n",
    "        for batch in valid_loader:\n",
    "            inputs = batch[0].to(device)\n",
    "            labels = batch[1].to(device)\n",
    "            \n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            preds = net(inputs)\n",
    "            # calculate the loss\n",
    "            loss = F.cross_entropy(preds, labels.long())\n",
    "            # record validation loss\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "        # print training/validation statistics \n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        \n",
    "        epoch_len = len(str(n_epochs))\n",
    "        \n",
    "        print_msg = (f'epoch: [{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f} '+\n",
    "                     f'train_accuracy: {total_correct/len(X_train):.5f}' )\n",
    "        \n",
    "        if train_verbose == True: print(print_msg)\n",
    "        \n",
    "        # clear lists to track next epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        \n",
    "        # early_stopping needs the validation loss to check if it has decresed, \n",
    "        # and if it has, it will make a checkpoint of the current model\n",
    "        early_stopping(valid_loss, net)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break       \n",
    "        \n",
    "    # load the last checkpoint with the best model\n",
    "    net.load_state_dict(torch.load('checkpoint2.pt'))\n",
    "    \n",
    "    return net, avg_train_losses, avg_valid_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise(X_train, X_valid):\n",
    "    # standardize per channel\n",
    "    means = X_train.mean(axis=(0,2), keepdims=True)\n",
    "    stds = X_train.std(axis=(0,2), keepdims=True)\n",
    "    X_train = (X_train - means) / (stds)\n",
    "    X_valid = (X_valid - means) / (stds)\n",
    "    return X_train, X_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(lst): \n",
    "    return sum(lst) / len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorise(y_train, y_valid):\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    y_valid = le.transform(y_valid)\n",
    "    return y_train, y_valid, le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plots(model_type, y_true, y_pred, user, label, n_epochs, model, eval_type, train_loss, valid_loss, bandpass, multiple,sigma):\n",
    "    if model_type == 'clf':\n",
    "        # plot confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        saved_file = \"results/CNN/clf/confusion/k fold/{3}/{4}/User_{0}_{1}_epochs_{2}_bandpass_{5}_multiple_{6}_sigma_{7}.png\".format(user,label,\n",
    "                                                                                                                            n_epochs,model, \n",
    "                                                                                                                            eval_type, \n",
    "                                                                                                                            bandpass, multiple,sigma)\n",
    "        plot_confusion_matrix(cm, set(y_true), saved_file ,normalize=True)\n",
    "        \n",
    "        #plot model\n",
    "    if model_type == 'reg':\n",
    "        saved_file = \"results/CNN/reg/y vs y_pred/{3}/{4}/User_{0}_{1}_epochs_{2}_bandpass_{5}_multiple_{6}_sigma_{7}.png\".format(user,label, n_epochs,model,\n",
    "                                                                                                                                  eval_type, \n",
    "                                                                                                                                  bandpass, multiple,sigma)\n",
    "        plot_model(y_true, y_pred, user, label,file=saved_file)\n",
    "    \n",
    "    saved_file = \"results/CNN/clf/loss curves/k fold/{3}/{4}/User_{0}_{1}_epochs_{2}_bandpass_{5}_multiple_{6}_sigma_{7}.png\".format(user,label,n_epochs,\n",
    "                                                                                                                                     model, eval_type, bandpass,\n",
    "                                                                                                                                    multiple,sigma)\n",
    "    plot_loss_early_stop(train_loss, valid_loss, saved_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(X_train, X_valid, y_train, y_valid):\n",
    "    \n",
    "    #Convert to 4D \n",
    "    X_train = X_train.reshape(X_train.shape[0],1,X_train.shape[1],X_train.shape[2])\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0],1, X_valid.shape[1],X_valid.shape[2])\n",
    "\n",
    "    # Create train and valid loader\n",
    "    train = data_utils.TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train).long())\n",
    "    valid = data_utils.TensorDataset(torch.Tensor(X_valid), torch.Tensor(y_valid))\n",
    "    train_loader = data_utils.DataLoader(train, batch_size=50, shuffle=True)\n",
    "    valid_loader = data_utils.DataLoader(valid, batch_size=50, shuffle=False)       \n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(data, multiple):\n",
    "    \"\"\"\n",
    "    Method for multiplying a dataset\n",
    "    :data: chosen dataset\n",
    "    :multiply: chosen number to multiply the dataset\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    for i in range (multiple):\n",
    "        data_list.append(data)\n",
    "    data = np.concatenate(data_list)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Gaussian noise\n",
    "\n",
    "# https://het.as.utexas.edu/HET/Software/Numpy/reference/generated/numpy.random.normal.html\n",
    "\n",
    "def add_gaussian_noise(clean_signal, multiple, sigma):\n",
    "    \"\"\"\n",
    "    Method for multiplying a given dataset and adding gausian noise\n",
    "    :clean_signal: clean dataset without noise\n",
    "    :multiple: chosen number to multiply the dataset\n",
    "    :sigma: standard deviation\n",
    "    \"\"\"\n",
    "    print(\"Starting size: {0}\".format(clean_signal.shape))\n",
    "    #multiply the dataset\n",
    "    clean_signal = multiply(clean_signal, multiple)\n",
    "\n",
    "    # add noise to the dataset based on the shape of the multiplied dataset\n",
    "    mu = 0 # average needs to be zero to generate gaussian noise\n",
    "    noise = np.random.normal(mu, sigma, clean_signal.shape)\n",
    "    noisy_signal = clean_signal + noise\n",
    "    print(\"End size: {0}\".format(noisy_signal.shape))\n",
    "    return noisy_signal\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_predict(X,y, model_type, model, n_epochs, train_verbose, patience, fc_size, multiple, sigma, augment):\n",
    "    \"\"\"\n",
    "    Method for running 5 fold cross validation based on a given array of tests\n",
    "    \"\"\"\n",
    "    kf= KFold(n_splits = 5, shuffle = True, random_state =  1)\n",
    "    \n",
    "    if model_type == 'clf':\n",
    "        results = {\"Accuracy\":[], \"Precision\":[], \"Recall\":[], \"F1 Score Macro\":[],\n",
    "              \"F1 Score Micro\":[],\"Balanced Accuracy\":[]}\n",
    "    else:\n",
    "        results = {'RMSE':[], 'R2':[]}\n",
    "        \n",
    "    total_predictions = []\n",
    "    total_true = []\n",
    "    num_classes = 0\n",
    "    accuracy = []\n",
    "    fold = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        \n",
    "        #Train/valid split\n",
    "        X_train, X_valid = np.concatenate(X[train_index]), np.concatenate(X[test_index])\n",
    "        y_train, y_valid = np.concatenate(y[train_index]).astype('int'), np.concatenate(y[test_index]).astype('int')\n",
    "        \n",
    "        \n",
    "        # check the the classes in the validation set, if there are not in training set then skip\n",
    "        y_valid_classes = list(set(y_valid))\n",
    "        y_train_classes = list(set(y_train))\n",
    "        \n",
    "        if check_if_valid_labels_are_in_train(y_train_classes, y_valid_classes) == False: \n",
    "            continue\n",
    "   \n",
    "        \n",
    "        #standardise per channel\n",
    "        X_train, X_valid = standardise(X_train, X_valid)\n",
    "        \n",
    "        #label the categorical variables\n",
    "        y_train, y_valid, le = categorise(y_train, y_valid)\n",
    "        \n",
    "        # augment training samples\n",
    "        if augment == True:\n",
    "            X_train = add_gaussian_noise(X_train, multiple, sigma)\n",
    "            y_train = multiply(y_train, multiple)\n",
    "        \n",
    "        size = len(X_train) + len(X_valid)\n",
    "        #get loaders\n",
    "        train_loader, valid_loader = get_loaders(X_train, X_valid, y_train, y_valid)\n",
    "        \n",
    "         # count the number of classes\n",
    "        if len(set(y_train)) > num_classes:\n",
    "            num_classes = len(set(y_train))\n",
    "       \n",
    "        # train the network\n",
    "        time_start = time.time()\n",
    "        net, train_loss, valid_loss = train(X_train, train_loader, valid_loader, \n",
    "                    num_classes, model, n_epochs, patience, train_verbose, fc_size)\n",
    "        fold += 1 \n",
    "        print('Fold  {0}! Time elapsed: {1} seconds'.format(fold, time.time()-time_start))\n",
    "        \n",
    "        # make predictions\n",
    "        y_pred = le.inverse_transform(predict(valid_loader, net))\n",
    "        y_true = le.inverse_transform(y_valid)\n",
    "     \n",
    "        #save total predictions and get results\n",
    "        total_predictions.append(y_pred)\n",
    "        total_true.append(y_true) \n",
    "        r = get_results(y_true, y_pred, model_type) #returns a dictionary of results   \n",
    "        valid_acc = get_accuracy(valid_loader, net)\n",
    "        for key in r: # loop through dictionary to add to all the scores to the results dictionary\n",
    "            results[key].append(r[key])\n",
    "        accuracy.append(valid_acc)\n",
    "\n",
    "    for key in results: # average out the results \n",
    "        results[key] = average(results[key])\n",
    "    accuracy = average(accuracy)\n",
    "    total_predictions = np.concatenate(total_predictions)\n",
    "    total_true = np.concatenate(total_true)\n",
    "        \n",
    "        \n",
    "    return results, total_predictions , total_true , num_classes , size , accuracy, train_loss, valid_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#get results\n",
    "#         Results = namedtuple(\"Results\",\"label train_acc test_acc user\")\n",
    "#         train_acc = get_accuracy(train_loader, test_loader, \"train\", net)\n",
    "#         test_acc = get_accuracy(train_loader, test_loader, \"test\", net)\n",
    "#         print(\"{0} results on User 1\\ntrain_acc: {1}\\tsample_size: {2}\\ntest_acc: {3}\\tsample_size: {4}\\n\".format(label, \n",
    "#                                                                                                train_acc,  len(X_train),\n",
    "#                                                                                                 test_acc, len(X_test)))\n",
    "#         results.append(Results(label, train_acc, test_acc,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# bandpass = load_file(\"/cs/home/ybk1/Dissertation/data/saved user and test data/all_users_sampled_120_window_annotated_EEG_agg_bandpass_True_slider_120.pickle\")\n",
    "# no_p = load_file(\"/cs/home/ybk1/Dissertation/data/saved user and test data/all_users_sampled_120_window_annotated_EEG_agg_bandpass_False_slider_120.pickle\")\n",
    "\n",
    "# bandpass_lgbm = load_file(\"/cs/home/ybk1/Dissertation/data/saved user and test data/all_users_sampled_120_window_annotated_EEG_agg_bandpass.pickle\")\n",
    "# bandpass[1]['inputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_per_user(model , train_verbose, window_size_samples, fc_size, bandpass, multiple, sigma, augment,class_type):\n",
    "\n",
    "    time_original = time.time()\n",
    "\n",
    "    labels = [\"attention\", \"interest\", \"effort\"]\n",
    "\n",
    "    n_epochs = 100\n",
    "\n",
    "    patience = 10\n",
    "    window_size_samples = 120\n",
    "    saved_file = \"/cs/home/ybk1/Dissertation/data/saved user and test data/all_users_sampled_{0}_window_annotated_EEG_no_agg_bandpass_{1}_slider_{0}.pickle\".format(window_size_samples, bandpass)\n",
    "    all_tests = load_file(saved_file)\n",
    "    users = list(all_tests.keys())\n",
    "    model_type = 'clf'\n",
    "    results = []\n",
    "    eval_type = 'per user'\n",
    "    \n",
    "    for user in users:\n",
    "        print(\"Working on user {0}\".format(user))\n",
    "\n",
    "        for label in labels:\n",
    "\n",
    "            time_start = time.time()\n",
    "            dt = all_tests[user] # dictionary of all the individual tests per user\n",
    "\n",
    "            X = np.array([np.array(x).astype(np.float32) for x in dt['inputs']]) # array of all the inputs for each test\n",
    "            y = np.array([np.array(x) for x in dt[label]]) #Convert the categories into labels\n",
    "\n",
    "            # K fold predict \n",
    "            r, y_pred, y_true, num_classes, size, accuracy, train_loss, valid_loss = kfold_predict(X,y, model_type, \n",
    "                                                                                                   model, n_epochs, \n",
    "                                                                                                    train_verbose, patience, \n",
    "                                                                                                   fc_size, multiple, sigma, augment)\n",
    "   \n",
    "            # get results and add them to the list\n",
    "            duration = time.time() - time_start\n",
    "            results.append(collate_results(r, user, label, duration, num_classes, \n",
    "                                           size, model_type, n_epochs, window_size_samples, model, multiple, sigma, bandpass,class_type))\n",
    "\n",
    "            #Save plots\n",
    "            save_plots(model_type, y_true, y_pred, user, label, n_epochs, model, eval_type, train_loss, valid_loss, bandpass, multiple,sigma)\n",
    "\n",
    "            print(\"Finished analysis on User {0}_{1}\".format(user,label))\n",
    "        print(\"Finished analysis on User {0}\".format(user))\n",
    "    results  = pd.DataFrame(results)\n",
    "    results.to_csv(\"results/CNN/{3}/tabulated/k fold/{2}/{2}_performance_window_size_{0}_{1}_{4}_multiple_{5}_sigma{6}.csv\".format(window_size_samples, \n",
    "                                                                                                                                       n_epochs, model, model_type, \n",
    "                                                                                                                                       eval_type, multiple, sigma), index=False )\n",
    "    final_duration = time.time()- time_original\n",
    "    print(\"All analyses are complete! Time elapsed: {0}\".format(final_duration))\n",
    "    return results\n",
    "        \n",
    "    \n",
    "def run_cross_user(model , train_verbose, window_size_samples, fc_size, bandpass, multiple, sigma, augment,class_type):\n",
    "    time_original = time.time()\n",
    "\n",
    "    labels = [\"attention\", 'interest', 'effort']\n",
    "    results = [] # save all results in this list\n",
    "    patience = 10\n",
    "    n_epochs = 100\n",
    "    saved_file = \"/cs/home/ybk1/Dissertation/data/saved user and test data/all_users_sampled_{0}_window_annotated_EEG_agg_bandpass_{1}_slider_{0}.pickle\".format(window_size_samples, bandpass)\n",
    "    all_tests_agg = load_file(saved_file)\n",
    "    users = all_tests_agg.keys()\n",
    "    model_type = 'clf'\n",
    "    user ='all'\n",
    "    eval_type = 'cross user'\n",
    "   \n",
    "\n",
    "    for label in labels:\n",
    "        print(\"Working on label {0}\".format(label))\n",
    "        time_start = time.time()\n",
    "\n",
    "        # Store each user in a list to prepare for cross-user analysis\n",
    "        X = np.array([all_tests_agg[user]['inputs'].astype(np.float32) for user in all_tests_agg])\n",
    "        y = np.array([all_tests_agg[user][label] for user in all_tests_agg])  \n",
    "\n",
    "        # train and make predictions\n",
    "        r, y_pred, y_true, num_classes, size, accuracy, train_loss, valid_loss = kfold_predict(X,y,model_type, model,\n",
    "                                                                                               n_epochs, train_verbose, \n",
    "                                                                                               patience, fc_size, multiple,sigma, augment)\n",
    "\n",
    "         # get results\n",
    "        duration = time.time() - time_start\n",
    "        results.append(collate_results(r, user, label, duration, num_classes, \n",
    "                                       size, model_type, n_epochs, window_size_samples,model, multiple, sigma, bandpass, class_type))\n",
    "        \n",
    "        #Save plots\n",
    "        save_plots(model_type, y_true, y_pred, user, label, n_epochs, model, eval_type, train_loss, valid_loss, bandpass, multiple,sigma)\n",
    "                 \n",
    "        print(\"Finished analysis on label {0}. Time elapsed {1}\".format(label, time.time()-time_start))\n",
    "    print(\"Finished analysis on User {0}\".format(user))\n",
    "    results_file = \"results/CNN/{3}/tabulated/k fold/{2}/{2}_performance_window_size_{0}_{1}_{4}_dropout_0.9_all_labels_bandpass_{5}_multiple_{6}_sigma_{7}.csv\".format(window_size_samples, n_epochs, model,\n",
    "                                                                                                                                model_type, eval_type, bandpass, multiple, sigma)\n",
    "    results  = pd.DataFrame(results)\n",
    "    results.to_csv(results_file, index=False )\n",
    "    final_duration = time.time()- time_original\n",
    "    print(\"All analyses are complete! Time elapsed: {0}\".format(final_duration))\n",
    "    return results\n",
    "\n",
    "# windows = [15, 30, 60, 120, 250]\n",
    "# fc_sizes = [8, 16, 32, 56, 120]\n",
    "# for window, fc_size in zip(windows, fc_sizes):\n",
    "\n",
    "window = 120\n",
    "fc_size = 56\n",
    "augment = True\n",
    "multiples = [5, 20, 30]\n",
    "sigmas = [0.001, 0.01, 0.1, 0.5]\n",
    "bandpass = False\n",
    "class_type = 'multi'\n",
    "# models = ['Hybrid', 'EEGNet']\n",
    "model = 'Hybrid'\n",
    "results = []\n",
    "for multiple in multiples:\n",
    "    for sigma in sigmas:\n",
    "        results.append(run_cross_user(model, False, window, fc_size,bandpass, multiple, sigma, augment, class_type))\n",
    "        results.append(run_per_user(model, False, window, fc_size,bandpass, multiple, sigma, augment, class_type))\n",
    "\n",
    "results = pd.concat(results).to_csv(\"results/bulk/Hybrid_data_augmentation_test_small.csv\", index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
