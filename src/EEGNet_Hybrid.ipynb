{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "import torch\n",
    "from ipynb.fs.full.evaluation import *\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from ipynb.fs.full.Data_Processing import *\n",
    "from sklearn import preprocessing\n",
    "import torch.utils.data as data_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler , LabelEncoder\n",
    "torch.set_printoptions(linewidth=120) #Display options for output\n",
    "torch.set_grad_enabled(True) # Already on by default\n",
    "torch.manual_seed(0)\n",
    "from torch_lr_finder import LRFinder\n",
    "import pickle\n",
    "import torch.utils.data as data_utils\n",
    "from collections import namedtuple\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "from pytorchtools import EarlyStopping\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "if cuda:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, num_classes, combined, fc_size,dropout):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.T = 120\n",
    "        self.combined = combined\n",
    "        self.fc_size = fc_size\n",
    "        self.dropout = dropout\n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Conv2d(1, 16, (1, 8), padding = 0)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16, False)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.padding1 = nn.ZeroPad2d((16, 17, 0, 1))\n",
    "        self.conv2 = nn.Conv2d(1, 4, (2, 32))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling2 = nn.MaxPool2d(2, 4)\n",
    "        \n",
    "        # Layer 3\n",
    "        self.padding2 = nn.ZeroPad2d((2, 1, 4, 3))\n",
    "        self.conv3 = nn.Conv2d(4, 4, (8, 4))\n",
    "        self.batchnorm3 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling3 = nn.MaxPool2d((2, 4))\n",
    "        \n",
    "        # FC Layer\n",
    "        # NOTE: This dimension will depend on the number of timestamps per sample in your data.\n",
    "        # I have 120 timepoints.\n",
    "    \n",
    "        self.fc1 = nn.Linear(fc_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "        x = x.float()\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.dropout(x, self.dropout)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        # Layer 2\n",
    "        x = self.padding1(x)\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.dropout(x, self.dropout)\n",
    "        x = self.pooling2(x)\n",
    "\n",
    "        # Layer 3\n",
    "        x = self.padding2(x)\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.dropout(x, self.dropout)\n",
    "        x = self.pooling3(x)\n",
    " \n",
    "        # FC Layer\n",
    "        x = x.view(-1, self.fc_size)\n",
    "        if self.combined == False:      \n",
    "            x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    \n",
    "    # for 60 timepoints = 4*2*4 and -1\n",
    "    # 120 timepoints = 4* 2* 7 and -1\n",
    "    # https://discuss.pytorch.org/t/runtimeerror-shape-1-400-is-invalid-for-input-of-size/33354\n",
    "    # https://discuss.pytorch.org/t/valueerror-expected-input-batch-size-324-to-match-target-batch-size-4/24498/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN + RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ai.stackexchange.com/questions/3156/how-to-select-number-of-hidden-layers-and-number-of-memory-cells-in-an-lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "\n",
    "class Combine(nn.Module):\n",
    "    def __init__(self, num_classes, combined, fc_size,dropout):\n",
    "        super(Combine, self).__init__()\n",
    "        self.cnn = EEGNet(num_classes,combined, fc_size,dropout)\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=fc_size, \n",
    "            hidden_size=16, \n",
    "            num_layers=1,\n",
    "            batch_first=True)\n",
    "        self.linear = nn.Linear(16 ,num_classes)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, C, timepoints, channels = x.size()\n",
    "        c_in = x\n",
    "        c_out = self.cnn(c_in)\n",
    "        r_in = c_out.view(batch_size, -1 , c_out.shape[1])\n",
    "        r_out, (h_n, h_c) = self.rnn(r_in)\n",
    "        r_out2 = self.linear(r_out[:, -1, :])\n",
    "        return F.log_softmax(r_out2, dim=1)\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate function returns values of different criteria like accuracy, precision etc.**\n",
    "In case you face memory overflow issues, use batch size to control how many samples get evaluated at one time. Use a batch_size that is a factor of length of samples. This ensures that you won't miss any samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate random data**\n",
    "    \n",
    "*Data format:*\n",
    "\n",
    "Datatype - float32 (both X and Y)\n",
    "\n",
    "X.shape - (#samples, 1, #timepoints, #channels)\n",
    "\n",
    "Y.shape - (#samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()### Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_finder(model,train_loader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-7, weight_decay=1e-2)\n",
    "    lr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\n",
    "    lr_finder.range_test(train_loader, end_lr=100, num_iter=100, step_mode=\"exp\")\n",
    "    lr_finder.plot()\n",
    "    plt.savefig(\"results/Figures/learning_rate_finder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(loader, net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            inputs = data[0].to(device)\n",
    "            labels = data[1].to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    acc = (correct/total * 100)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(loader, net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            inputs = data[0].to(device)\n",
    "            labels = data[1].to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predictions.append(predicted.cpu().numpy())\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    acc = (correct/total * 100)\n",
    "    return np.concatenate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, train_loader, valid_loader,  num_classes, model, n_epochs, patience, train_verbose, fc_size,dropout,model_type):\n",
    "    \n",
    "    # choose between the EEGNet or EEGNet + RNN\n",
    "    if model == 'EEGNet':\n",
    "        net = EEGNet(num_classes, False, fc_size,dropout).to(device)\n",
    "    if model == 'Hybrid':\n",
    "        net = Combine(num_classes, True, fc_size,dropout).to(device)\n",
    "        \n",
    "    #store the predictions and the losses\n",
    "    preds_list = [] # track the predictions\n",
    "    labels_list = [] # track the labels\n",
    "    train_losses = [] # to track the train loss as the model trains\n",
    "    valid_losses = [] # to track the validation loss as the model trains\n",
    "    avg_train_losses = [] # to track the average training loss per epoch as the model trains\n",
    "    avg_valid_losses = [] # to track the average validation loss per epoch as the model trains\n",
    "    \n",
    "    #Set the optimiser \n",
    "    optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "    #Initialise the early stopping\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=train_verbose, path='checkpoint3.pt')\n",
    "\n",
    "    #Implement learning rate finder\n",
    "#     learning_rate_finder(net,train_loader)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "\n",
    "        net.train() # prep the model for training\n",
    "        for batch in train_loader:\n",
    "\n",
    "            inputs = batch[0].to(device)\n",
    "            labels = batch[1].to(device)\n",
    "            \n",
    "            preds = net(inputs) #forward pass: compute predicted outputs by passing inputs to the model\n",
    "        \n",
    "    \n",
    "#             criterion=nn.BCEWithLogitsLoss() # calculate the loss\n",
    "            if model_type == 'clf':\n",
    "                loss = F.cross_entropy(preds, labels.long()) # calculate loss\n",
    "            else:\n",
    "                loss_func = nn.MSELoss()\n",
    "                loss = loss_func(preds,labels.float())\n",
    "            optimizer.zero_grad()# clear the gradients of all optimized variables\n",
    "            loss.backward()  # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            optimizer.step() # perform a single optimization step (parameter update)\n",
    "            train_losses.append(loss.item()) # record training loss\n",
    "            \n",
    "            #record the predictions and losses\n",
    "            preds_list.append(preds)\n",
    "            labels_list.append(labels)\n",
    "            total_loss += loss.item()\n",
    "            if model == 'clf':\n",
    "                total_correct += get_num_correct(preds, labels)\n",
    "            \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        net.eval() # prep model for evaluation\n",
    "        for batch in valid_loader:\n",
    "            inputs = batch[0].to(device)\n",
    "            labels = batch[1].to(device)\n",
    "            \n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            preds = net(inputs)\n",
    "            # calculate the loss\n",
    "            if model_type == 'clf':\n",
    "                loss = F.cross_entropy(preds, labels.long()) # calculate loss\n",
    "            else:\n",
    "                loss_func = nn.MSELoss()\n",
    "                loss = loss_func(preds,labels.float())\n",
    "            # record validation loss\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "        # print training/validation statistics \n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        \n",
    "        epoch_len = len(str(n_epochs))\n",
    "        \n",
    "        print_msg = (f'epoch: [{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f} '+\n",
    "                     f'train_accuracy: {total_correct/len(X_train):.5f}' )\n",
    "        \n",
    "        if train_verbose == True: print(print_msg)\n",
    "        \n",
    "        # clear lists to track next epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        \n",
    "        # early_stopping needs the validation loss to check if it has decresed, \n",
    "        # and if it has, it will make a checkpoint of the current model\n",
    "        early_stopping(valid_loss, net)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break       \n",
    "        \n",
    "    # load the last checkpoint with the best model\n",
    "    net.load_state_dict(torch.load('checkpoint3.pt'))\n",
    "    \n",
    "    return net, avg_train_losses, avg_valid_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plots(model_type, y_true, y_pred, user, label, n_epochs, model, eval_type, train_loss, valid_loss, bandpass, multiple,sigma, class_type):\n",
    "    if model_type == 'clf':\n",
    "        # plot confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        saved_file = \"results/CNN/clf/confusion/k fold/{3}/{4}/User_{0}_{1}_epochs_{2}_bandpass_{5}_multiple_{6}_sigma_{7}_{8}.png\".format(user,label,\n",
    "                                                                                                                            n_epochs,model, \n",
    "                                                                                                                            eval_type, \n",
    "                                                                                                                            bandpass, multiple,sigma, class_type)\n",
    "        plot_confusion_matrix(cm, set(y_true), saved_file ,normalize=True)\n",
    "        \n",
    "        #plot model\n",
    "    if model_type == 'reg':\n",
    "        saved_file = \"results/CNN/reg/y vs y_pred/{3}/{4}/User_{0}_{1}_epochs_{2}_bandpass_{5}_multiple_{6}_sigma_{7}_{8}.png\".format(user,label, n_epochs,model,\n",
    "                                                                                                                                  eval_type, \n",
    "                                                                                                                                  bandpass, multiple,sigma, class_type)\n",
    "        plot_model(y_true, y_pred, user, label,file=saved_file)\n",
    "    \n",
    "    saved_file = \"results/CNN/clf/loss curves/k fold/{3}/{4}/User_{0}_{1}_epochs_{2}_bandpass_{5}_multiple_{6}_sigma_{7}{8}.png\".format(user,label,n_epochs,\n",
    "                                                                                                                                     model, eval_type, bandpass,\n",
    "                                                                                                                                    multiple,sigma, class_type)\n",
    "    plot_loss_early_stop(train_loss, valid_loss, saved_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(X_train, X_valid, y_train, y_valid):\n",
    "    \n",
    "    #Convert to 4D \n",
    "    X_train = X_train.reshape(X_train.shape[0],1,X_train.shape[1],X_train.shape[2])\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0],1, X_valid.shape[1],X_valid.shape[2])\n",
    "\n",
    "    # Create train and valid loader\n",
    "    train = data_utils.TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train).long())\n",
    "    valid = data_utils.TensorDataset(torch.Tensor(X_valid), torch.Tensor(y_valid))\n",
    "    train_loader = data_utils.DataLoader(train, batch_size=50, shuffle=True)\n",
    "    valid_loader = data_utils.DataLoader(valid, batch_size=50, shuffle=False)       \n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(data, multiple):\n",
    "    \"\"\"\n",
    "    Method for multiplying a dataset\n",
    "    :data: chosen dataset\n",
    "    :multiply: chosen number to multiply the dataset\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    for i in range (multiple):\n",
    "        data_list.append(data)\n",
    "    data = np.concatenate(data_list)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Gaussian noise\n",
    "\n",
    "# https://het.as.utexas.edu/HET/Software/Numpy/reference/generated/numpy.random.normal.html\n",
    "\n",
    "def add_gaussian_noise(clean_signal, multiple, sigma):\n",
    "    \"\"\"\n",
    "    Method for multiplying a given dataset and adding gausian noise\n",
    "    :clean_signal: clean dataset without noise\n",
    "    :multiple: chosen number to multiply the dataset\n",
    "    :sigma: standard deviation\n",
    "    \"\"\"\n",
    "    print(\"Starting size: {0}\".format(clean_signal.shape))\n",
    "    #multiply the dataset\n",
    "    clean_signal = multiply(clean_signal, multiple)\n",
    "\n",
    "    # add noise to the dataset based on the shape of the multiplied dataset\n",
    "    mu = 0 # average needs to be zero to generate gaussian noise\n",
    "    noise = np.random.normal(mu, sigma, clean_signal.shape)\n",
    "    noisy_signal = clean_signal + noise\n",
    "    print(\"End size: {0}\".format(noisy_signal.shape))\n",
    "    return noisy_signal\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_predict(X,y, model_type, model, n_epochs, train_verbose, patience, fc_size, multiple, sigma, augment, class_type,dropout):\n",
    "    \"\"\"\n",
    "    Method for running 5 fold cross validation based on a given array of tests\n",
    "    \"\"\"\n",
    "    kf= KFold(n_splits = 5, shuffle = True, random_state =  1)\n",
    "    \n",
    "    if model_type == 'clf':\n",
    "        results = {\"Accuracy\":[], \"Precision\":[], \"Recall\":[], \"F1 Score Macro\":[],\n",
    "              \"F1 Score Micro\":[],\"Balanced Accuracy\":[]}\n",
    "    else:\n",
    "        results = {'RMSE':[], 'R2':[]}\n",
    "        \n",
    "    total_predictions = []\n",
    "    total_true = []\n",
    "    num_classes = 0\n",
    "    accuracy = []\n",
    "    fold = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        \n",
    "        #Train/valid split\n",
    "        X_train, X_valid = np.concatenate(X[train_index]), np.concatenate(X[test_index])\n",
    "        y_train, y_valid = np.concatenate(y[train_index]).astype('int'), np.concatenate(y[test_index]).astype('int')\n",
    "        \n",
    "        \n",
    "        # check the the classes in the validation set, if there are not in training set then skip\n",
    "        y_valid_classes = list(set(y_valid))\n",
    "        y_train_classes = list(set(y_train))\n",
    "        \n",
    "        if check_if_valid_labels_are_in_train(y_train_classes, y_valid_classes) == False: \n",
    "            continue\n",
    "   \n",
    "        #standardise per channel\n",
    "        X_train, X_valid = standardise(X_train, X_valid)\n",
    "        \n",
    "        #convert to binary if binary classification\n",
    "        if class_type == 'binary':\n",
    "            y_train = convert_to_binary(y_train)\n",
    "            y_valid = convert_to_binary(y_valid)\n",
    "                \n",
    "        if model_type == 'clf':\n",
    "        #label the categorical variables\n",
    "            y_train, y_valid, le = categorise(y_train, y_valid)\n",
    "        \n",
    "        # augment training samples\n",
    "        if augment == True:\n",
    "            X_train = add_gaussian_noise(X_train, multiple, sigma)\n",
    "            y_train = multiply(y_train, multiple)\n",
    "        \n",
    "        size = len(X_train) + len(X_valid)\n",
    "        #get loaders\n",
    "        train_loader, valid_loader = get_loaders(X_train, X_valid, y_train, y_valid)\n",
    "        \n",
    "         # count the number of classes\n",
    "        if len(set(y_train)) > num_classes:\n",
    "            num_classes = len(set(y_train))\n",
    "       \n",
    "        # train the network\n",
    "        time_start = time.time()\n",
    "        net, train_loss, valid_loss = train(X_train, train_loader, valid_loader, \n",
    "                    num_classes, model, n_epochs, patience, train_verbose, fc_size,dropout, model_type)\n",
    "        fold += 1 \n",
    "        print('Fold  {0}! Time elapsed: {1} seconds'.format(fold, time.time()-time_start))\n",
    "        \n",
    "        # make predictions\n",
    "        if model_type == 'clf':        \n",
    "            y_pred = le.inverse_transform(predict(valid_loader, net))\n",
    "            y_true = le.inverse_transform(y_valid)\n",
    "        else:\n",
    "            y_pred = predict(valid_loader, net)\n",
    "            y_true = y_valid\n",
    "     \n",
    "        #save total predictions and get results\n",
    "        total_predictions.append(y_pred)\n",
    "        total_true.append(y_true) \n",
    "        r = get_results(y_true, y_pred, model_type) #returns a dictionary of results   \n",
    "        valid_acc = get_accuracy(valid_loader, net)\n",
    "        for key in r: # loop through dictionary to add to all the scores to the results dictionary\n",
    "            results[key].append(r[key])\n",
    "        accuracy.append(valid_acc)\n",
    "\n",
    "    for key in results: # average out the results \n",
    "        results[key] = average(results[key])\n",
    "    accuracy = average(accuracy)\n",
    "    total_predictions = np.concatenate(total_predictions)\n",
    "    total_true = np.concatenate(total_true)\n",
    "        \n",
    "        \n",
    "    return results, total_predictions , total_true , num_classes , size , accuracy, train_loss, valid_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on label attention\n",
      "Early stopping\n",
      "Fold  1! Time elapsed: 20.675695657730103 seconds\n",
      "Early stopping\n",
      "Fold  2! Time elapsed: 21.388845205307007 seconds\n",
      "Early stopping\n",
      "Fold  3! Time elapsed: 21.068920612335205 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-fee9104afb48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m \u001b[0mrun_cross_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbandpass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-fee9104afb48>\u001b[0m in \u001b[0;36mrun_cross_user\u001b[0;34m(model, train_verbose, window_size_samples, fc_size, bandpass, multiple, sigma, augment, class_type, dropout, model_type)\u001b[0m\n\u001b[1;32m     79\u001b[0m         r, y_pred, y_true, num_classes, size, accuracy, train_loss, valid_loss = kfold_predict(X,y,model_type, model,\n\u001b[1;32m     80\u001b[0m                                                                                                \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_verbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                                                                                                patience, fc_size, multiple,sigma, augment, class_type, dropout)\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m          \u001b[0;31m# get results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-2a90dda29e21>\u001b[0m in \u001b[0;36mkfold_predict\u001b[0;34m(X, y, model_type, model, n_epochs, train_verbose, patience, fc_size, multiple, sigma, augment, class_type, dropout)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mtime_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         net, train_loss, valid_loss = train(X_train, train_loader, valid_loader, \n\u001b[0;32m---> 60\u001b[0;31m                     num_classes, model, n_epochs, patience, train_verbose, fc_size,dropout, model_type)\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mfold\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fold  {0}! Time elapsed: {1} seconds'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtime_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-a120939ad8c9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X_train, train_loader, valid_loader, num_classes, model, n_epochs, patience, train_verbose, fc_size, dropout, model_type)\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# clear the gradients of all optimized variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# backward pass: compute gradient of the loss with respect to model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# perform a single optimization step (parameter update)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# record training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_per_user(model , train_verbose, window_size_samples, fc_size, bandpass, multiple, sigma, augment,class_type, dropout):\n",
    "\n",
    "    time_original = time.time()\n",
    "\n",
    "    labels = [\"attention\", \"interest\", \"effort\"]\n",
    "\n",
    "    n_epochs = 100\n",
    "\n",
    "    patience = 10\n",
    "    window_size_samples = 120\n",
    "    saved_file = \"saved user and test data/all_users_sampled_{0}_window_annotated_EEG_no_agg_bandpass_{1}_slider_{0}.pickle\".format(window_size_samples, bandpass)\n",
    "    all_tests = load_file(saved_file)\n",
    "    users = list(all_tests.keys())\n",
    "    model_type = 'clf'\n",
    "    results = []\n",
    "    eval_type = 'per user'\n",
    "    \n",
    "    for user in users:\n",
    "        print(\"Working on user {0}\".format(user))\n",
    "\n",
    "        for label in labels:\n",
    "\n",
    "            time_start = time.time()\n",
    "            dt = all_tests[user] # dictionary of all the individual tests per user\n",
    "\n",
    "            X = np.array([np.array(x).astype(np.float32) for x in dt['inputs']]) # array of all the inputs for each test\n",
    "            y = np.array([np.array(x) for x in dt[label]]) #Convert the categories into labels\n",
    "\n",
    "            # K fold predict \n",
    "            r, y_pred, y_true, num_classes, size, accuracy, train_loss, valid_loss = kfold_predict(X,y, model_type, \n",
    "                                                                                                   model, n_epochs, \n",
    "                                                                                                    train_verbose, patience, \n",
    "                                                                                                   fc_size, multiple, sigma, augment, class_type, dropout)\n",
    "   \n",
    "            # get results and add them to the list\n",
    "            duration = time.time() - time_start\n",
    "            results.append(collate_results(r, user, label, duration, num_classes, \n",
    "                                           size, model_type, n_epochs, window_size_samples, model, multiple, sigma, bandpass,class_type))\n",
    "\n",
    "            #Save plots\n",
    "            save_plots(model_type, y_true, y_pred, user, label, n_epochs, model, eval_type, train_loss, valid_loss, bandpass, multiple,sigma , class_type)\n",
    "\n",
    "            print(\"Finished analysis on User {0}_{1}\".format(user,label))\n",
    "        print(\"Finished analysis on User {0}\".format(user))\n",
    "    results  = pd.DataFrame(results)\n",
    "    results.to_csv(\"results/CNN/{3}/tabulated/k fold/{2}/{2}_performance_window_size_{0}_{1}_{4}_multiple_{5}_sigma{6}_{7}.csv\".format(window_size_samples, \n",
    "                                                                                                                                       n_epochs, model, model_type, \n",
    "                                                                                                                                       eval_type, multiple, sigma, class_type), index=False )\n",
    "    final_duration = time.time()- time_original\n",
    "    print(\"All analyses are complete! Time elapsed: {0}\".format(final_duration))\n",
    "    return results\n",
    "        \n",
    "    \n",
    "def run_cross_user(model , train_verbose, window_size_samples, fc_size, bandpass, multiple, sigma, augment,class_type, dropout, model_type):\n",
    "    time_original = time.time()\n",
    "\n",
    "#     labels = [\"attention\", 'interest', 'effort']\n",
    "    labels = ['attention','interest','effort']\n",
    "    results = [] # save all results in this list\n",
    "    patience = 10\n",
    "    n_epochs = 500\n",
    "    saved_file = \"saved user and test data/all_users_sampled_{0}_window_annotated_EEG_agg_bandpass_{1}_slider_{0}.pickle\".format(window_size_samples, bandpass)\n",
    "    all_tests_agg = load_file(saved_file)\n",
    "    users = all_tests_agg.keys()\n",
    "\n",
    "    user ='all'\n",
    "    eval_type = 'cross user'\n",
    "   \n",
    "\n",
    "    for label in labels:\n",
    "         print(\"Running - Model_type: {4}, ClassType:{3}, Model: {0}, User: {1}, label: {2}\".format(model_type, user,label, class_type, model_type))\n",
    "        time_start = time.time()\n",
    "\n",
    "        # Store each user in a list to prepare for cross-user analysis\n",
    "        X = np.array([all_tests_agg[user]['inputs'].astype(np.float32) for user in all_tests_agg])\n",
    "        y = np.array([all_tests_agg[user][label] for user in all_tests_agg])  \n",
    "\n",
    "        # train and make predictions\n",
    "        r, y_pred, y_true, num_classes, size, accuracy, train_loss, valid_loss = kfold_predict(X,y,model_type, model,\n",
    "                                                                                               n_epochs, train_verbose, \n",
    "                                                                                               patience, fc_size, multiple,sigma, augment, class_type, dropout)\n",
    "\n",
    "         # get results\n",
    "        duration = time.time() - time_start\n",
    "        results.append(collate_results(r, user, label, duration, num_classes, \n",
    "                                       size, model_type, n_epochs, window_size_samples,model, multiple, sigma, bandpass, class_type))\n",
    "        \n",
    "        #Save plots\n",
    "        save_plots(model_type, y_true, y_pred, user, label, n_epochs, model, eval_type, train_loss, valid_loss, bandpass, multiple,sigma, class_type)\n",
    "                 \n",
    "        print(\"Finished analysis on label {0}. Time elapsed {1}\".format(label, time.time()-time_start))\n",
    "    print(\"Finished analysis on User {0}\".format(user))\n",
    "    results_file = \"results/CNN/{3}/tabulated/k fold/{2}/{2}_performance_window_size_{0}_{1}_{4}_dropout_{9}_all_labels_bandpass_{5}_multiple_{6}_sigma_{7}_{8}.csv\".format(window_size_samples, n_epochs, model,\n",
    "                                                                                                                                model_type, eval_type, bandpass, multiple, sigma, class_type,dropout)\n",
    "    results  = pd.DataFrame(results)\n",
    "    results.to_csv(results_file, index=False )\n",
    "    final_duration = time.time()- time_original\n",
    "    print(\"All analyses are complete! Time elapsed: {0}\".format(final_duration))\n",
    "    return results\n",
    "\n",
    "# windows = [15, 30, 60, 120, 250]\n",
    "# fc_sizes = [8, 16, 32, 56, 120]\n",
    "# for window, fc_size in zip(windows, fc_sizes):\n",
    "\n",
    "window = 120\n",
    "fc_size = 56\n",
    "augment = False\n",
    "multiples = [5, 20, 30]\n",
    "sigmas = [0.001, 0.01, 0.1, 0.5]\n",
    "# dropouts = [0.1, 0.25, 0.50, 0.75, 0.8, 0.9]\n",
    "dropout = 0.9\n",
    "bandpass = False\n",
    "class_type = 'binary'\n",
    "multiple = None \n",
    "sigma = None\n",
    "model = 'Hybrid' # or \"EEGNet\"\n",
    "model_type = 'clf'\n",
    "# models= 'Hybrid'\n",
    "results = []\n",
    "\n",
    "\n",
    "run_cross_user(model, False, window, fc_size,bandpass, multiple, sigma, augment, class_type, dropout, model_type)\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
