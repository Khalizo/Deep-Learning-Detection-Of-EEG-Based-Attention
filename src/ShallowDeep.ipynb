{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running - Model_type: clf, ClassType:binary, Model: deep, User: all, label: attention\n",
      "Train:  [ 0  1  4  5  7  8  9 10 11 12 14 15 16 17] Validation:  [ 2  3  6 13]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/home/ybk1/python/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed created! Time elapsed: 197.41358160972595 seconds\n",
      "Train:  [ 0  1  2  3  5  6  8  9 10 11 12 13 16 17] Validation:  [ 4  7 14 15]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-82186f0b68c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0msd2\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mShallowDeep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'clf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cross user'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'binary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m \u001b[0msd2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_shallow_deep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-82186f0b68c4>\u001b[0m in \u001b[0;36mrun_shallow_deep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cross user'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cross_user_sd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbandpass\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'both'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-82186f0b68c4>\u001b[0m in \u001b[0;36mrun_cross_user_sd\u001b[0;34m(self, model_type, bandpass, class_type)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m                 \u001b[0;31m# train and make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m                 \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkfold_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_depth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m                  \u001b[0;31m# get results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-82186f0b68c4>\u001b[0m in \u001b[0;36mkfold_predict\u001b[0;34m(self, X, y, model_type, n_epochs, model_depth, class_type)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0msave_stdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/cs/tmp/ybk1/trash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mtrainset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_from_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last_window\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0mvalidset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_from_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last_window\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_stdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/lib/python3.7/site-packages/braindecode/datasets/xy.py\u001b[0m in \u001b[0;36mcreate_from_X_y\u001b[0;34m(X, y, drop_last_window, sfreq, ch_names, window_size_samples, window_stride_samples)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mwindow_size_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow_size_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mwindow_stride_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow_stride_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mdrop_last_window\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_last_window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwindows_datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/lib/python3.7/site-packages/braindecode/datautil/windowers.py\u001b[0m in \u001b[0;36mcreate_fixed_length_windows\u001b[0;34m(concat_ds, start_offset_samples, stop_offset_samples, window_size_samples, window_stride_samples, drop_last_window, mapping, preload, drop_bad_windows)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_events\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mtmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_size_samples\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sfreq\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             metadata=metadata, preload=preload)\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdrop_bad_windows\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-184>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, raw, events, event_id, tmin, tmax, baseline, picks, preload, reject, flat, proj, decim, reject_tmin, reject_tmax, detrend, on_missing, reject_by_annotation, metadata, event_repeated, verbose)\u001b[0m\n",
      "\u001b[0;32m~/python/lib/python3.7/site-packages/mne/epochs.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, raw, events, event_id, tmin, tmax, baseline, picks, preload, reject, flat, proj, decim, reject_tmin, reject_tmax, detrend, on_missing, reject_by_annotation, metadata, event_repeated, verbose)\u001b[0m\n\u001b[1;32m   2093\u001b[0m             \u001b[0mreject_tmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreject_tmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetrend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdetrend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2094\u001b[0m             \u001b[0mproj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_missing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreload_at_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2095\u001b[0;31m             event_repeated=event_repeated, verbose=verbose)\n\u001b[0m\u001b[1;32m   2096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2097\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-175>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, info, data, events, event_id, tmin, tmax, baseline, raw, picks, reject, flat, decim, reject_tmin, reject_tmax, detrend, proj, on_missing, preload_at_end, selection, drop_log, filename, metadata, event_repeated, verbose)\u001b[0m\n",
      "\u001b[0;32m~/python/lib/python3.7/site-packages/mne/epochs.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_events\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%d matching events found'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mn_events\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No desired events found.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/python/lib/python3.7/logging/__init__.py\u001b[0m in \u001b[0;36minfo\u001b[0;34m(self, msg, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1376\u001b[0m         \"\"\"\n\u001b[1;32m   1377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misEnabledFor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/python/lib/python3.7/logging/__init__.py\u001b[0m in \u001b[0;36m_log\u001b[0;34m(self, level, msg, args, exc_info, extra, stack_info)\u001b[0m\n\u001b[1;32m   1500\u001b[0m             \u001b[0;31m#IronPython can use logging.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1502\u001b[0;31m                 \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindCaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1503\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"(unknown file)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"(unknown function)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/python/lib/python3.7/logging/__init__.py\u001b[0m in \u001b[0;36mfindCaller\u001b[0;34m(self, stack_info)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0mfile\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m         \"\"\"\n\u001b[0;32m-> 1451\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrentframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;31m#On some versions of IronPython, currentframe() returns None if\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m         \u001b[0;31m#IronPython isn't run with -X:Frames.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/python/lib/python3.7/logging/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_getframe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0mcurrentframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcurrentframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler , LabelEncoder\n",
    "import sys, io\n",
    "import pandas as pd \n",
    "from ipynb.fs.full.Data_Processing import *\n",
    "from ipynb.fs.full.evaluation import *\n",
    "from braindecode.datasets.xy import create_from_X_y\n",
    "from braindecode.training.losses import CroppedLoss\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from braindecode.util import set_random_seeds\n",
    "from braindecode.models import ShallowFBCSPNet , Deep4Net \n",
    "from skorch.callbacks import LRScheduler, EarlyStopping\n",
    "from skorch.helper import predefined_split\n",
    "from braindecode import EEGClassifier , EEGRegressor\n",
    "from collections import namedtuple\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "if cuda:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed = 20200220  # random seed to make results reproducible\n",
    "# Set random seed to be able to reproduce results\n",
    "set_random_seeds(seed=seed, cuda=cuda)\n",
    "\n",
    "class ShallowDeep:\n",
    "    def __init__(self, model_type, bandpass, eval_type, class_type):\n",
    "        self.model_type = model_type\n",
    "        self.bandpass = bandpass\n",
    "        self.eval_type = eval_type\n",
    "        self.class_type = class_type\n",
    "   \n",
    "\n",
    "\n",
    "    def choose_cnn (self, model_depth, model_type, trainset, validset , n_classes , device, cuda , n_epochs):\n",
    "        # Extract number of chans and time steps from dataset\n",
    "        n_chans = trainset[0][0].shape[0]\n",
    "        input_window_samples = trainset[0][0].shape[1]\n",
    "\n",
    "        if model_type =='reg':\n",
    "            n_classes = 1\n",
    "            \n",
    "        if model_depth == 'shallow':\n",
    "            lr = 0.0625 * 0.01\n",
    "            weight_decay = 0\n",
    "            model =  ShallowFBCSPNet(n_chans, n_classes, input_window_samples=input_window_samples, final_conv_length=\"auto\")\n",
    "        else:\n",
    "            lr = 1 * 0.01\n",
    "            weight_decay = 0.5 * 0.001\n",
    "            \"\"\"\n",
    "            For 30 samples, filter time_length = 1\n",
    "            For 60 > samples, filter time length is left empty\n",
    "            for 15 samples, filter_time length = 1, filter_length_2 = 1, filter_length_3 = 1\n",
    "            \"\"\"\n",
    "            model =  Deep4Net(n_chans, n_classes, input_window_samples=input_window_samples,\n",
    "                      final_conv_length='auto', pool_time_length=1, filter_time_length = 1,pool_time_stride=1)\n",
    "        if cuda:\n",
    "            model = model.cuda(0)\n",
    "\n",
    "        batch_size = 32\n",
    "\n",
    "        if model_type == 'clf':\n",
    "            clf = EEGClassifier(\n",
    "            model,\n",
    "            criterion=torch.nn.NLLLoss,\n",
    "            optimizer=torch.optim.AdamW,\n",
    "            train_split=predefined_split(validset),  # using valid_set for validation\n",
    "            optimizer__lr=lr,\n",
    "            optimizer__weight_decay=weight_decay,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[\n",
    "                \"accuracy\", \n",
    "                (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)), \n",
    "               (\"EarlyStopping\", EarlyStopping(monitor = 'valid_loss', threshold = 0.00001)),\n",
    "            ],\n",
    "            device=device,)\n",
    "            return clf\n",
    "        else:\n",
    "\n",
    "            # remove softmax\n",
    "            new_model = torch.nn.Sequential()\n",
    "            for name, module_ in model.named_children():\n",
    "                if \"softmax\" in name:\n",
    "                    continue\n",
    "                new_model.add_module(name, module_)\n",
    "\n",
    "            model = new_model\n",
    "\n",
    "            regressor = EEGRegressor(\n",
    "            model,\n",
    "            cropped = False,\n",
    "            criterion=CroppedLoss,\n",
    "            criterion__loss_function=torch.nn.functional.mse_loss,\n",
    "            optimizer=torch.optim.AdamW,\n",
    "            train_split=predefined_split(validset),\n",
    "            optimizer__lr=lr,\n",
    "            optimizer__weight_decay=weight_decay,\n",
    "            iterator_train__shuffle=True,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[\n",
    "                \"neg_root_mean_squared_error\",\n",
    "                # seems n_epochs -1 leads to desired behavior of lr=0 after end of training?\n",
    "                (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)), \n",
    "               (\"EarlyStopping\", EarlyStopping(monitor = 'valid_loss', threshold = 0.00001)),\n",
    "            ],\n",
    "            device=device)\n",
    "            return regressor\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def kfold_predict (self, X,y, model_type, n_epochs, model_depth,class_type):\n",
    "        kf= KFold(n_splits = 5, shuffle = True, random_state =  1)\n",
    "\n",
    "        if model_type == 'clf':\n",
    "            results = {\"Accuracy\":[], \"Precision\":[], \"Recall\":[], \"F1 Score Macro\":[],\n",
    "                  \"F1 Score Micro\":[],\"Balanced Accuracy\":[]}\n",
    "        else:\n",
    "            results = {'RMSE':[], 'R2':[]}\n",
    "\n",
    "        total_predictions = []\n",
    "        total_true = []\n",
    "        num_classes = 0\n",
    "        clf = None\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            print(\"Train: \", train_index, \"Validation: \", test_index)\n",
    "\n",
    "            #Train/test split\n",
    "            X_train, X_valid = np.concatenate(X[train_index]), np.concatenate(X[test_index])\n",
    "            y_train, y_valid = np.concatenate(y[train_index]).astype('int'), np.concatenate(y[test_index]).astype('int')\n",
    "\n",
    "            # check the the classes in the validation set\n",
    "            y_valid_classes = list(set(y_valid))\n",
    "            y_train_classes = list(set(y_train))\n",
    "            if check_if_valid_labels_are_in_train(y_train_classes, y_valid_classes) == False: \n",
    "                continue\n",
    "\n",
    "\n",
    "            size = len(X_train) + len(X_valid) #get dataset size\n",
    "\n",
    "            #standardise per channel\n",
    "            X_train, X_valid = standardise(X_train, X_valid)\n",
    "            \n",
    "            #convert to binary if binary classification\n",
    "            if class_type == 'binary':\n",
    "                y_train = convert_to_binary(y_train)\n",
    "                y_valid = convert_to_binary(y_valid)\n",
    "\n",
    "            #label the categorical variables\n",
    "            if model_type == 'clf':\n",
    "                y_train, y_valid, le = categorise(y_train, y_valid)\n",
    "\n",
    "            # Convert training and validation sets into a suitable format\n",
    "            save_stdout = sys.stdout\n",
    "            sys.stdout = open('/cs/tmp/ybk1/trash', 'w')\n",
    "            trainset = create_from_X_y(X_train, y_train, drop_last_window=False)\n",
    "            validset = create_from_X_y(X_valid, y_valid, drop_last_window=False)\n",
    "            sys.stdout = save_stdout\n",
    "\n",
    "            # count the number of classes\n",
    "            if len(set(y_train)) > num_classes:\n",
    "                num_classes = len(set(y_train))\n",
    "\n",
    "            # commence the training process\n",
    "            time_start = time.time()\n",
    "            save_stdout = sys.stdout\n",
    "            sys.stdout = open('/cs/tmp/ybk1/trash', 'w')\n",
    "            cnn = self.choose_cnn (model_depth, model_type, trainset, validset , num_classes , device, cuda, n_epochs).fit(trainset, y=None, epochs=n_epochs)\n",
    "\n",
    "            sys.stdout = save_stdout\n",
    "            print('Training completed created! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "\n",
    "            # make predictions\n",
    "            if model_type == 'clf':\n",
    "                y_pred = le.inverse_transform(cnn.predict(X_valid))\n",
    "                y_true = le.inverse_transform(y_valid)\n",
    "            else:\n",
    "                y_pred = cnn.predict(X_valid)\n",
    "                y_true = y_valid\n",
    "\n",
    "            total_predictions.append(y_pred)\n",
    "            total_true.append(y_true) \n",
    "            r = get_results(y_true, y_pred, model_type)\n",
    "\n",
    "            for key in r: # loop through dictionary to add to all the scores to the results dictionary\n",
    "                results[key].append(r[key])\n",
    "\n",
    "\n",
    "        for key in results: # finallly average out the results \n",
    "            results[key] = average(results[key])\n",
    "\n",
    "        return results, np.concatenate(total_predictions), np.concatenate(total_true), num_classes, size, cnn\n",
    "\n",
    "    def save_plots(self,y_true, y_pred, user, label, model_depth, bandpass, window_size_samples, model_type, cnn,class_type):\n",
    "        if model_type == 'clf':\n",
    "            # plot confusion matrix\n",
    "            cm = confusion_matrix(y_true, y_pred)\n",
    "            saved_file = \"results/CNN/{5}/confusion/k fold/{2}/per user/User_{0}_Label_{1}_bandpass_{3}_window_{4}_class_type{6}.png\".format(user, label, model_depth, bandpass, window_size_samples, model_type, class_type)\n",
    "            plot_confusion_matrix(cm, set(y_true), saved_file ,normalize=True)\n",
    "\n",
    "        #plot loss curve\n",
    "        plot_loss_curve(cnn)\n",
    "        plt.savefig(\"results/CNN/{5}/loss curves/k fold/{2}/per user/User_{0}_Label_{1}_bandpass_{3}_window_{4}_class_type{6}.png\".format(user, label, model_depth, bandpass, window_size_samples, model_type, class_type))\n",
    "\n",
    "        if model_type == 'reg':\n",
    "            saved_file = \"results/CNN/{5}/y vs y_pred/{2}/per user/User_{0}_Label_{1}_bandpass_{3}_window_{4}_class_type{6}.png\".format(user, label, model_depth, bandpass, window_size_samples, model_type, class_type)\n",
    "            plot_model(y_true, y_pred, user, label,file=saved_file)   \n",
    "\n",
    "    def run_per_user_sd(self, model_type, bandpass, class_type):\n",
    "        \"\"\"\n",
    "        Method for running the CNN per user\n",
    "        \"\"\"\n",
    "        multiple = None\n",
    "        sigma = None\n",
    "\n",
    "        model_depths = ['deep', 'shallow']\n",
    "        results = []\n",
    "        for model_depth in model_depths:\n",
    "\n",
    "            time_original = time.time()\n",
    "\n",
    "\n",
    "            labels = ['attention','interest','effort']\n",
    "            window_size_samples = 120\n",
    "            n_epochs = 100\n",
    "        #     saved_file = \"/cs/home/ybk1/Dissertation/data/all_users_sampled_with_individual_tests_30_window_annotated_EEG.pickle\"\n",
    "            saved_file = \"saved user and test data/all_users_sampled_{0}_window_annotated_EEG_no_agg_bandpass_{1}_slider_{0}.pickle\".format(window_size_samples, bandpass)\n",
    "            all_tests = load_file(saved_file)\n",
    "            users = all_tests.keys()\n",
    "\n",
    "            for user in users:\n",
    "                torch.backends.cudnn.benchmark = True\n",
    "\n",
    "                for label in labels:\n",
    "                    print(\"Running - Model_type: {4}, ClassType:{3}, Model: {0}, User: {1}, label: {2}\".format(model_depth, user,label, class_type, model_type))\n",
    "\n",
    "                    time_start = time.time()\n",
    "                    dt = all_tests[user] # dictionary of all the individual tests per user\n",
    "\n",
    "                    X = np.array([np.array(x).transpose(0,2,1).astype(np.float32) for x in dt['inputs']])     \n",
    "                    y = np.array([np.array(x) for x in dt[label]]) #Convert the categories into labels\n",
    "\n",
    "                    # train and make predictions\n",
    "                    r, y_pred, y_true, num_classes, size, cnn = self.kfold_predict(X,y, model_type, n_epochs, model_depth,class_type)\n",
    "                    print(r['Accuracy'])\n",
    "\n",
    "                     # get results\n",
    "                    duration = time.time() - time_start\n",
    "                    results.append(collate_results(r, user, label, duration, \n",
    "                                                   num_classes, size, model_type, \n",
    "                                                   n_epochs, window_size_samples, \n",
    "                                                   model_depth, multiple, sigma, bandpass, class_type))\n",
    "\n",
    "                    self.save_plots(y_true, y_pred, user, label, model_depth, bandpass, window_size_samples, model_type, cnn, class_type)\n",
    "\n",
    "                    print(\"Finished analysis on User {0}_{1}\".format(user,label))\n",
    "                print(\"Finished analysis on User {0}\".format(user))\n",
    "                \n",
    "        results  = pd.DataFrame(results)\n",
    "        results.to_csv(\"results/bulk/shallow_deep_performance_window_size_{0}_per_user_model_type_{1}bandpass_{3}_class-type{3}.csv\".format(window_size_samples, model_type, bandpass, class_type), index=False )\n",
    "        final_duration = time.time()- time_original\n",
    "        print(\"All analyses are complete! Time elapsed: {0}\".format(final_duration))\n",
    "        return results\n",
    "\n",
    "    def run_cross_user_sd(self, model_type, bandpass, class_type):\n",
    "        multiple = None\n",
    "        sigma = None\n",
    "        \n",
    "        model_depths = ['deep','shallow']\n",
    "        for model_depth in model_depths:\n",
    "\n",
    "            time_original = time.time()\n",
    "\n",
    "            window_size_samples = 120\n",
    "\n",
    "            n_epochs = 100\n",
    "            results = []\n",
    "            labels = ['attention','interest','effort']\n",
    "\n",
    "            saved_file = \"saved user and test data/all_users_sampled_{0}_window_annotated_EEG_agg_bandpass_{1}_slider_{0}.pickle\".format(window_size_samples, bandpass)\n",
    "            all_tests_agg = load_file(saved_file)\n",
    "            users = all_tests_agg.keys()\n",
    "            user ='all'\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "\n",
    "            for label in labels:\n",
    "                print(\"Running - Model_type: {4}, ClassType:{3}, Model: {0}, User: {1}, label: {2}\".format(model_depth, user,label, class_type, model_type))\n",
    "\n",
    "                time_start = time.time()\n",
    "\n",
    "\n",
    "                # convert the inputs  into #samples, channels, #timepoints format\n",
    "                X = np.array([all_tests_agg[user]['inputs'].transpose(0,2,1).astype(np.float32) for user in all_tests_agg])\n",
    "                y = np.array([all_tests_agg[user][label] for user in all_tests_agg])  \n",
    "\n",
    "                # train and make predictions\n",
    "                r, y_pred, y_true, num_classes, size, cnn = self.kfold_predict(X,y, model_type, n_epochs, model_depth,class_type)\n",
    "\n",
    "                 # get results\n",
    "                duration = time.time() - time_start\n",
    "                results.append(collate_results(r, user, label, duration, \n",
    "                                               num_classes, size, model_type, \n",
    "                                               n_epochs, window_size_samples, \n",
    "                                               model_depth, multiple, sigma, bandpass, class_type))\n",
    "\n",
    "                #save plots\n",
    "                self.save_plots(y_true, y_pred, user, label, model_depth, bandpass, window_size_samples, model_type, cnn,class_type)\n",
    "\n",
    "\n",
    "                print(\"Finished analysis on label {0}\".format(label))\n",
    "        print(\"Finished analysis on User {0}\".format(user))\n",
    "        results  = pd.DataFrame(results)\n",
    "        results.to_csv(\"results/CNN/{3}/tabulated/k fold/{1}/{1}CNN_Valid_performance_window_size_{0}_cross_user_bandpass_{2}_classtype_{4}.csv\".format(window_size_samples , \n",
    "                                                                                                                                                        model_depth,bandpass, model_type, class_type), index=False )\n",
    "        final_duration = time.time()- time_original\n",
    "        print(\"All analyses are complete! Time elapsed: {0}\".format(final_duration))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def run_shallow_deep(self):\n",
    "        if self.eval_type == 'per user':\n",
    "            results = self.run_per_user_sd(self.model_type, self.bandpass, self.class_type)\n",
    "            return results\n",
    "        elif self.eval_type == 'cross user':\n",
    "            results = self.run_cross_user_sd(self.model_type, self.bandpass,self.class_type)\n",
    "            return results\n",
    "        elif self.eval_type == 'both':\n",
    "\n",
    "            results = []\n",
    "            results.append(self.run_cross_user_sd(self.model_type, self.bandpass))\n",
    "            results.append(self.run_per_user_sd(self.model_type, self.bandpass))\n",
    "\n",
    "            results = pd.concat(results)\n",
    "            return results\n",
    "\n",
    "sd2 =ShallowDeep('clf', False, 'cross user', 'binary')\n",
    "sd2.run_shallow_deep()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
