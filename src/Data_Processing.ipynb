{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import namedtuple\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traversing the folder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check the json data\n",
    "- check the whether the time start and time end are distinct\n",
    "- check whether there are 5 paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_user_number(s):\n",
    "    \"\"\"\n",
    "    Extracts the user number from a given path\n",
    "    :s: path of the file\n",
    "    \"\"\"\n",
    "    s = str(s).split(\"_\")[0] # extract the user from the path\n",
    "    s = s.split(\"/\")[6] # split the path to isolate the user\n",
    "    s = s.replace(\"User\",\"\") # remove the word \"User\"\n",
    "    s = int(re.sub(r'[a-z]+', '', s, re.I)) # remove alphabetical characters\n",
    "    return s \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_test_number(s):\n",
    "    \"\"\"\n",
    "    Extracts the test number from a given path\n",
    "    :s: path of the file\n",
    "    \"\"\"\n",
    "    s = s.split(\"/\")[7]\n",
    "    s = s.split(\"_\")[1]\n",
    "    s = int(re.sub(r'[a-z]+', '', s, re.I))\n",
    "    return s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path(\"/cs/home/ybk1/Dissertation/Experiment Anonymised Version\")\n",
    "# p = Path(\"C://Users//User//OneDrive - University of St Andrews//Modules//CS5099//4. Data Documents//dataset//INSTRUMENTED DIGITAL AND PAPER READINGDATASET//Experiment Anonymised Version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_datasets(p):\n",
    "    \"\"\"\n",
    "    Loops through all the folders to find all the datasets and json files for training\n",
    "    p: path of the root folder\n",
    "    \"\"\"\n",
    "    File = namedtuple('File', 'name path size')\n",
    "    files = []\n",
    "    for item in p.glob('**/*'): # loops thorough all the files in all the sub-directories\n",
    "        if item.match('*rawEEGData.csv')  and \"baseline\" not in item.name:\n",
    "            name = item.name\n",
    "            path = Path.resolve(item).parent\n",
    "            size = item.stat().st_size\n",
    "\n",
    "            files.append(File(name,path, size )) # stores the name, path and size in named tuple\n",
    "    \n",
    "    df = pd.DataFrame(files)\n",
    "    df['user'] = df[\"path\"].apply(extract_user_number)\n",
    "    df.to_csv(\"All EEG files.csv\", index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the value counts we can see that: \n",
    "- user 20, 23, 17, 7, 9, 15, 2 - all have duplicates\n",
    "- user 14 is one short\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the datasets using the alldatasets csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_json(labels):\n",
    "    \"\"\"\n",
    "    Method for checking the json to see how many paragraphs it contains\n",
    "    and whether the timestamps are distinct\n",
    "    :labels: annotations.json in a DataFrame\n",
    "    \"\"\"\n",
    "    #check whether there are 5 paragraphs\n",
    "    no_para = len(labels)\n",
    "    are_timestamps_distinct = False\n",
    "    if no_para == 0: # checks if the json is empty\n",
    "        return no_para, are_timestamps_distinct\n",
    "    else:\n",
    "        if len(labels) != 5:\n",
    "            print (\"There are less than 5 paragraphs in the dataset\")\n",
    "        else:\n",
    "            print(\"There are 5 paragraphs in the dataset\")\n",
    "\n",
    "        #check whether the timestamps are distincts\n",
    "        cols = [\"timeRangeStart\", \"timeRangeEnd\"]\n",
    "        for col in cols:\n",
    "            array_length= len(array(labels[col]))\n",
    "            set_length = len(set(array(labels[col])))\n",
    "            if array_length == set_length:\n",
    "                print(col + \" has distinct values\")\n",
    "                are_timestamps_distinct = True\n",
    "                break\n",
    "            else: \n",
    "                print(col + \" does not have distinct values\")\n",
    "                are_timestamps_distinct = False\n",
    "                break\n",
    "\n",
    "        print(are_timestamps_distinct)\n",
    "        return no_para, are_timestamps_distinct \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(timestamp, annotation, labels):\n",
    "    \"\"\"\n",
    "    Method for adding the labels to the dataframe\n",
    "    \n",
    "    :return: relevant score of attention/interest/effort within the correct time range\n",
    "    :timestamp: timestamp at the current iteration\n",
    "    :annotation: annotation at the current interation\n",
    "    :labels: annotations.json in array format\n",
    "    \"\"\"\n",
    "    for row in labels:\n",
    "        time_start = row[-2]\n",
    "        time_end = row[-1]\n",
    "        ann_dict = {\"effort\": row[-3], \"attention\": row[-4], \"interest\": row[-5],\"para\": row[-6]}\n",
    "        if timestamp >= time_start and timestamp < time_end: # checks if the timestamp is witin the start and end range\n",
    "            return ann_dict[annotation] # returns the relevant score stored in the dictionary\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(data, labels):\n",
    "    \"\"\"\n",
    "    Method for processing the dataset by creating columns for effort, attention, interest and paragraph\n",
    "    :data: EEG dataset\n",
    "    :labels: annotations.json in array format\n",
    "    \"\"\"\n",
    "    annotations = [\"effort\", \"attention\", \"interest\", \"para\"]\n",
    "    for ann in annotations: # loops through the labels, and creates new columns based on the values within the timestamp range\n",
    "        data[ann] = data[\"Timestamp\"].apply(label, annotation=ann, labels=labels)\n",
    "    \n",
    "    initial_rows = len(data)\n",
    "    data = data.dropna()\n",
    "    final_rows = len(data)\n",
    "    dropped_rows = initial_rows - final_rows\n",
    "    print(\"Initial rows: {0}\\nFinal rows: {1}\\nDropped rows: {2}\".format(initial_rows, \n",
    "                                                                         final_rows, \n",
    "                                                                         dropped_rows))\n",
    "    return data , initial_rows, final_rows, dropped_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_datasets_to_csv(labeled_data, path):\n",
    "    \"\"\"\n",
    "    Exports the labeled data as three separate csv files for interest, effort and attention\n",
    "    :labeled_data: annotated datasets with columns for interest, effort and attention\n",
    "    :path: path to save the dataset\n",
    "    \"\"\"\n",
    "    X = labeled_data.iloc[:,:9] # inputs i.e channels and timestamp\n",
    "    labels = ['interest', 'effort', 'attention']\n",
    "    for label in labels:\n",
    "        dataset = pd.concat([X, labeled_data[label]], axis=1).to_csv(path +  \"/\" +'EEG_' + label + '_dataset.csv', index=False)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_all_datasets(df):\n",
    "    \"\"\"\n",
    "    Method for building all the training sets by applying annotations at relevant timestamps\n",
    "    and splitting the training sets into attention, interest and effort\n",
    "    :df: DataFrame that includes all of the datasets\n",
    "    :ds: dataset\n",
    "    \"\"\"\n",
    "    ds = array(df)\n",
    "    json = \"annotations.json\" # json file that contains the labels and annotations for the data\n",
    "    File = namedtuple('File', 'path user test no_para are_timestamps_distinct initial_rows final_rows dropped_rows')\n",
    "    files = []\n",
    "    \n",
    "    for row in ds:\n",
    "        ds_path = str(row[1]) # Get the path of the dataset\n",
    "        ds_csv = row[0] # get the name of the csv file\n",
    "        print(\"Working on this csv: {}\".format(ds_csv))\n",
    "        ds_user = row[3] # get the user number\n",
    "        ds_test = extract_test_number(ds_path)\n",
    "        ds_data = pd.read_csv(ds_path + \"/\" + ds_csv)\n",
    "        ds_labels = pd.read_json(ds_path + \"/\" + json)\n",
    "        no_para, are_timestamps_distinct = check_json(ds_labels) # check the json file\n",
    "                \n",
    "        ds_labels = array(ds_labels)\n",
    "        if are_timestamps_distinct == False:\n",
    "            print(\"Don't process\")\n",
    "            initial_rows, final_rows  = len(ds_data), len(ds_data)\n",
    "            dropped_rows = 0\n",
    "            files.append(File(ds_path, ds_user, ds_test, no_para,are_timestamps_distinct,\n",
    "                             initial_rows, final_rows, dropped_rows))          \n",
    "        else:\n",
    "            labeled_data, initial_rows, final_rows, dropped_rows = add_labels(ds_data, ds_labels) # Gets the labeled data\n",
    "#             export_datasets_to_csv(labeled_data, ds_path) # Exports the 3 datasets to the appropriate path\n",
    "            labeled_data.to_csv(ds_path + \"/\" + \"annotated_EEG.csv\", index=False)\n",
    "            files.append(File(ds_path, ds_user, ds_test, no_para, are_timestamps_distinct,\n",
    "                         initial_rows, final_rows, dropped_rows))\n",
    "            \n",
    "    training_files = pd.DataFrame(files)\n",
    "    training_files.to_csv(\"Trainingfiles.csv\", index=False)\n",
    "    return training_files\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_all_datasets_test():\n",
    "    \"\"\"\n",
    "    Test method for building a single group of training sets by applying annotations at relevant timestamps\n",
    "    and splitting the training sets into attention, interest and effort\n",
    "    :df: DataFrame that includes all of the datasets\n",
    "    :ds: dataset\n",
    "    \"\"\"\n",
    "    # User001_test1_BET_01_30-08-2019_recording0_U1567156556_EYETRACKER_cleanFixationData.csv\n",
    "    # /cs/home/ybk1/Dissertation/Experiment Anonymised Version/User001_Group_Test_30-08-2019--10-07-00/User001_test1_BET_01_30-08-2019/User001_test1_BET_01_30-08-2019_recording0\n",
    "    ds_path = \"/cs/home/ybk1/Dissertation/Experiment Anonymised Version/User001_Group_Test_30-08-2019--10-07-00/User001_test1_BET_01_30-08-2019/User001_test1_BET_01_30-08-2019_recording0\"\n",
    "    ds_csv = \"User001_test1_BET_01_30-08-2019_recording0_U1567156556_EEG_rawEEGData.csv\"\n",
    "    json = \"annotations.json\"\n",
    "    \n",
    "    File = namedtuple('File', 'path user no_para are_timestamps_distinct initial_rows final_rows dropped_rows')\n",
    "    files = []\n",
    "        \n",
    "    ds_user = 1 # get the user number\n",
    "    ds_data = pd.read_csv(ds_path + \"/\" + ds_csv)\n",
    "    ds_labels = pd.read_json(ds_path + \"/\" + json)\n",
    "    no_para, are_timestamps_distinct = check_json(ds_labels) # check the json file\n",
    "\n",
    "\n",
    "    ds_labels = array(ds_labels)\n",
    "    if are_timestamps_distinct == False:\n",
    "        print(\"Don't process\")\n",
    "        initial_rows, final_rows  = len(ds_data), len(ds_data)\n",
    "        dropped_rows = 0\n",
    "        files.append(File(ds_path, ds_user, no_para,are_timestamps_distinct,\n",
    "                         initial_rows, final_rows, dropped_rows))\n",
    "    else:\n",
    "        labeled_data, initial_rows, final_rows, dropped_rows = add_labels(ds_data, ds_labels) # Gets the labeled data\n",
    "#         export_datasets_to_csv(labeled_data, ds_path) # Exports the 3 datasets to the appropriate path\n",
    "        labeled_data.to_csv(ds_path + \"/\" + \"annotated_EEG.csv\", index=False)\n",
    "        files.append(File(ds_path, ds_user, no_para, are_timestamps_distinct,\n",
    "                         initial_rows, final_rows, dropped_rows))\n",
    "        \n",
    "    training_files = pd.DataFrame(files)\n",
    "    return training_files\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run this block to rebuild the datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_datasets = get_all_datasets(p)\n",
    "# build_all_datasets(all_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_all_datasets_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract viable datasets from the training files \n",
    "\n",
    "- remove the files that have zero final rows\n",
    "- remove files that have empty JSON's\n",
    "- remove files that only have 4 paragraphs\n",
    "- remove files with indistince timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_training_files():\n",
    "    \"\"\"\n",
    "    Method for removing:\n",
    "    - Files with zero final rows\n",
    "    - remove files with empty Json's\n",
    "    - Files with only 4 paragraphs\n",
    "    - Files with indistinct timestamp\n",
    "    \"\"\"\n",
    "    tf = pd.read_csv(\"Trainingfiles.csv\")\n",
    "    print(\"Initial length of training files: {}\".format(len(tf)))\n",
    "    \n",
    "    zero_final = tf['final_rows'] == 0 # remove the files with zero final row\n",
    "    empty_json = tf['no_para'] == 0 # remove files with empty JSON's\n",
    "    four_para = tf['no_para'] == 4 # remove files with only four paragraphs\n",
    "    indistinct_timestamp = tf['are_timestamps_distinct'] == False # remove files with indistinct timestamps\n",
    "    files_rm = [zero_final, empty_json, four_para, indistinct_timestamp] # store them all in a list\n",
    "    for file_rm in files_rm:\n",
    "        tf = tf.drop(tf[file_rm].index) # drop files sequentially\n",
    "    \n",
    "    print(\"Final length of training files: {}\".format(len(tf)))\n",
    "    tf.to_csv(\"clean_trainingfiles.csv\", index=False)\n",
    "    return tf\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling the data \n",
    "\n",
    "In this part, the data will be sampled by created at "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1b5248fad524>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"clean_trainingfiles.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "training_files = pd.read_csv(\"clean_trainingfiles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8     16\n",
       "23    16\n",
       "22    16\n",
       "2     16\n",
       "20    16\n",
       "19    16\n",
       "18    16\n",
       "3     16\n",
       "6     16\n",
       "24    16\n",
       "11    16\n",
       "7     16\n",
       "9     16\n",
       "25    15\n",
       "1     15\n",
       "10    15\n",
       "17    15\n",
       "12     9\n",
       "16     2\n",
       "21     1\n",
       "Name: user, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_files['user'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store all of user 1's test in a dictionary\n",
    "user_1 = array(training_files[training_files['user'] == 1][\"path\"])\n",
    "user_1_tests = {}\n",
    "file = \"annotated_EEG.csv\"\n",
    "for test in user_1:\n",
    "    dataset = test + \"/\" + file\n",
    "    user_1_tests[extract_test_number(test)] = dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through the dictionary of tests and then sample the tests\n",
    "\n",
    "# for key in user_1_tests: \n",
    "#     test = pd.read_csv(user_1_tests[key]).drop(columns=[\"Timestamp\", \" AdjustedUnix\"])\n",
    "#     sampled_test = sample(test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(slider=1, sample_size=120):\n",
    "    \"\"\"\n",
    "    Method for creating samples within a dataset\n",
    "    :df: test that is being sampled\n",
    "    :slider: the amount by which the window slides during sampling. The lower the number, the more samples.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(user_1_tests[1]).drop([\"Timestamp\", \" AdjustedUnix\"], axis=1) #remove unnecessary columns\n",
    "    Sample = namedtuple('Sample', 'inputs effort attention interest')\n",
    "    sampled_tests = []\n",
    "    \n",
    "    \n",
    "    # Group by paragraph and add each paragraph into an array\n",
    "    paragraphs = df.groupby('para') \n",
    "    paragraphs = [paragraphs.get_group(x) for x in paragraphs.groups]\n",
    "    \n",
    "    incorrect_length = 0\n",
    "    # Loop trough each paragraph to create samples\n",
    "    for para in paragraphs:\n",
    "        \n",
    "        if not len(para) > sample_size: # check the length of paragraph if it is bigger than the sample size\n",
    "            print (\"invalid\")\n",
    "            continue\n",
    "            \n",
    "        new_sample_length = len(para[0:sample_size])\n",
    "        counter = 0\n",
    "        while  new_sample_length >= sample_size:\n",
    "            \"\"\"\n",
    "            **Sliding window algorithm**\n",
    "            - Create new samples based on sample size and iterate using the slider size for size of overlap\n",
    "            - Create separate values for inputs, effort, attention, interest to add to a tuple\n",
    "            \"\"\"\n",
    "            new_sample = para[counter : counter + sample_size]\n",
    "            new_sample_length = len(new_sample)\n",
    "            \n",
    "            #checks new_sample length\n",
    "            if new_sample_length == sample_size:\n",
    "                # Extract the sample specific data\n",
    "                inputs = array(new_sample.iloc[:, :8])\n",
    "                effort, attention, interest = new_sample[[\"effort\", \"attention\", \"interest\"]].T.values\n",
    "                sampled_tests.append(Sample(inputs, int(max(effort)), int(max(attention)), int(max(interest))))\n",
    "            else:\n",
    "                incorrect_length += 1\n",
    "                continue             \n",
    "         \n",
    "            # increase by slider\n",
    "            counter += slider\n",
    "        \n",
    "        \n",
    "    sampled_tests_df = pd.DataFrame(sampled_tests)\n",
    "    inputs_and_labels = {}\n",
    "    \n",
    "    inputs_list = sampled_tests_df['inputs'].values\n",
    "    inputs_list = np.rollaxis(np.dstack(inputs_list),-1)\n",
    "    inputs_and_labels['inputs'] = inputs_list\n",
    "    labels = [\"effort\", \"attention\", \"interest\"]\n",
    "    for label in labels: \n",
    "        inputs_and_labels[label] = array(sampled_tests_df[label].values)\n",
    "    \n",
    "    \n",
    "    return inputs_and_labels\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
