{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import namedtuple\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traversing the folder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check the json data\n",
    "- check the whether the time start and time end are distinct\n",
    "- check whether there are 5 paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_user_number(s):\n",
    "    \"\"\"\n",
    "    Extracts the user number from a given path\n",
    "    :s: path of the file\n",
    "    \"\"\"\n",
    "    s = str(s).split(\"_\")[0] # extract the user from the path\n",
    "    s = s.split(\"/\")[6] # split the path to isolate the user\n",
    "    s = s.replace(\"User\",\"\") # remove the word \"User\"\n",
    "    s = int(re.sub(r'[a-z]+', '', s, re.I)) # remove alphabetical characters\n",
    "    return s \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_test_number(s):\n",
    "    \"\"\"\n",
    "    Extracts the test number from a given path\n",
    "    :s: path of the file\n",
    "    \"\"\"\n",
    "    s = s.split(\"/\")[7]\n",
    "    s = s.split(\"_\")[1]\n",
    "    s = int(re.sub(r'[a-z]+', '', s, re.I))\n",
    "    return s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path(\"/cs/home/ybk1/Dissertation/Experiment Anonymised Version\")\n",
    "# p = Path(\"C://Users//User//OneDrive - University of St Andrews//Modules//CS5099//4. Data Documents//dataset//INSTRUMENTED DIGITAL AND PAPER READINGDATASET//Experiment Anonymised Version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_datasets(p):\n",
    "    \"\"\"\n",
    "    Loops through all the folders to find all the datasets and json files for training\n",
    "    p: path of the root folder\n",
    "    \"\"\"\n",
    "    File = namedtuple('File', 'name path size')\n",
    "    files = []\n",
    "    for item in p.glob('**/*'): # loops thorough all the files in all the sub-directories\n",
    "        if item.match('*rawEEGData.csv')  and \"baseline\" not in item.name:\n",
    "            name = item.name\n",
    "            path = Path.resolve(item).parent\n",
    "            size = item.stat().st_size\n",
    "\n",
    "            files.append(File(name,path, size )) # stores the name, path and size in named tuple\n",
    "    \n",
    "    df = pd.DataFrame(files)\n",
    "    df['user'] = df[\"path\"].apply(extract_user_number)\n",
    "    df.to_csv(\"All EEG files.csv\", index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the value counts we can see that: \n",
    "- user 20, 23, 17, 7, 9, 15, 2 - all have duplicates\n",
    "- user 14 is one short\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the datasets using the alldatasets csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_json(labels):\n",
    "    \"\"\"\n",
    "    Method for checking the json to see how many paragraphs it contains\n",
    "    and whether the timestamps are distinct\n",
    "    :labels: annotations.json in a DataFrame\n",
    "    \"\"\"\n",
    "    #check whether there are 5 paragraphs\n",
    "    no_para = len(labels)\n",
    "    are_timestamps_distinct = False\n",
    "    if no_para == 0: # checks if the json is empty\n",
    "        return no_para, are_timestamps_distinct\n",
    "    else:\n",
    "        if len(labels) != 5:\n",
    "            print (\"There are less than 5 paragraphs in the dataset\")\n",
    "        else:\n",
    "            print(\"There are 5 paragraphs in the dataset\")\n",
    "\n",
    "        #check whether the timestamps are distincts\n",
    "        cols = [\"timeRangeStart\", \"timeRangeEnd\"]\n",
    "        for col in cols:\n",
    "            array_length= len(array(labels[col]))\n",
    "            set_length = len(set(array(labels[col])))\n",
    "            if array_length == set_length:\n",
    "                print(col + \" has distinct values\")\n",
    "                are_timestamps_distinct = True\n",
    "                break\n",
    "            else: \n",
    "                print(col + \" does not have distinct values\")\n",
    "                are_timestamps_distinct = False\n",
    "                break\n",
    "\n",
    "        print(are_timestamps_distinct)\n",
    "        return no_para, are_timestamps_distinct \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(timestamp, annotation, labels):\n",
    "    \"\"\"\n",
    "    Method for adding the labels to the dataframe\n",
    "    \n",
    "    :return: relevant score of attention/interest/effort within the correct time range\n",
    "    :timestamp: timestamp at the current iteration\n",
    "    :annotation: annotation at the current interation\n",
    "    :labels: annotations.json in array format\n",
    "    \"\"\"\n",
    "    for row in labels:\n",
    "        time_start = row[-2]\n",
    "        time_end = row[-1]\n",
    "        ann_dict = {\"effort\": row[-3], \"attention\": row[-4], \"interest\": row[-5],\"para\": row[-6]}\n",
    "        if timestamp >= time_start and timestamp < time_end: # checks if the timestamp is witin the start and end range\n",
    "            return ann_dict[annotation] # returns the relevant score stored in the dictionary\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(data, labels):\n",
    "    \"\"\"\n",
    "    Method for processing the dataset by creating columns for effort, attention, interest and paragraph\n",
    "    :data: EEG dataset\n",
    "    :labels: annotations.json in array format\n",
    "    \"\"\"\n",
    "    annotations = [\"effort\", \"attention\", \"interest\", \"para\"]\n",
    "    for ann in annotations: # loops through the labels, and creates new columns based on the values within the timestamp range\n",
    "        data[ann] = data[\"Timestamp\"].apply(label, annotation=ann, labels=labels)\n",
    "    \n",
    "    initial_rows = len(data)\n",
    "    data = data.dropna()\n",
    "    final_rows = len(data)\n",
    "    dropped_rows = initial_rows - final_rows\n",
    "    print(\"Initial rows: {0}\\nFinal rows: {1}\\nDropped rows: {2}\".format(initial_rows, \n",
    "                                                                         final_rows, \n",
    "                                                                         dropped_rows))\n",
    "    return data , initial_rows, final_rows, dropped_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_datasets_to_csv(labeled_data, path):\n",
    "    \"\"\"\n",
    "    Exports the labeled data as three separate csv files for interest, effort and attention\n",
    "    :labeled_data: annotated datasets with columns for interest, effort and attention\n",
    "    :path: path to save the dataset\n",
    "    \"\"\"\n",
    "    X = labeled_data.iloc[:,:9] # inputs i.e channels and timestamp\n",
    "    labels = ['interest', 'effort', 'attention']\n",
    "    for label in labels:\n",
    "        dataset = pd.concat([X, labeled_data[label]], axis=1).to_csv(path +  \"/\" +'EEG_' + label + '_dataset.csv', index=False)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_all_datasets(df):\n",
    "    \"\"\"\n",
    "    Method for building all the training sets by applying annotations at relevant timestamps\n",
    "    and splitting the training sets into attention, interest and effort\n",
    "    :df: DataFrame that includes all of the datasets\n",
    "    :ds: dataset\n",
    "    \"\"\"\n",
    "    ds = array(df)\n",
    "    json = \"annotations.json\" # json file that contains the labels and annotations for the data\n",
    "    File = namedtuple('File', 'path user test no_para are_timestamps_distinct initial_rows final_rows dropped_rows')\n",
    "    files = []\n",
    "    \n",
    "    for row in ds:\n",
    "        ds_path = str(row[1]) # Get the path of the dataset\n",
    "        ds_csv = row[0] # get the name of the csv file\n",
    "        print(\"Working on this csv: {}\".format(ds_csv))\n",
    "        ds_user = row[3] # get the user number\n",
    "        ds_test = extract_test_number(ds_path)\n",
    "        ds_data = pd.read_csv(ds_path + \"/\" + ds_csv)\n",
    "        ds_labels = pd.read_json(ds_path + \"/\" + json)\n",
    "        no_para, are_timestamps_distinct = check_json(ds_labels) # check the json file\n",
    "                \n",
    "        ds_labels = array(ds_labels)\n",
    "        if are_timestamps_distinct == False:\n",
    "            print(\"Don't process\")\n",
    "            initial_rows, final_rows  = len(ds_data), len(ds_data)\n",
    "            dropped_rows = 0\n",
    "            files.append(File(ds_path, ds_user, ds_test, no_para,are_timestamps_distinct,\n",
    "                             initial_rows, final_rows, dropped_rows))          \n",
    "        else:\n",
    "            labeled_data, initial_rows, final_rows, dropped_rows = add_labels(ds_data, ds_labels) # Gets the labeled data\n",
    "#             export_datasets_to_csv(labeled_data, ds_path) # Exports the 3 datasets to the appropriate path\n",
    "            labeled_data.to_csv(ds_path + \"/\" + \"annotated_EEG.csv\", index=False)\n",
    "            files.append(File(ds_path, ds_user, ds_test, no_para, are_timestamps_distinct,\n",
    "                         initial_rows, final_rows, dropped_rows))\n",
    "            \n",
    "    training_files = pd.DataFrame(files)\n",
    "    training_files.to_csv(\"Trainingfiles.csv\", index=False)\n",
    "    return training_files\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_all_datasets_test():\n",
    "    \"\"\"\n",
    "    Test method for building a single group of training sets by applying annotations at relevant timestamps\n",
    "    and splitting the training sets into attention, interest and effort\n",
    "    :df: DataFrame that includes all of the datasets\n",
    "    :ds: dataset\n",
    "    \"\"\"\n",
    "    # User001_test1_BET_01_30-08-2019_recording0_U1567156556_EYETRACKER_cleanFixationData.csv\n",
    "    # /cs/home/ybk1/Dissertation/Experiment Anonymised Version/User001_Group_Test_30-08-2019--10-07-00/User001_test1_BET_01_30-08-2019/User001_test1_BET_01_30-08-2019_recording0\n",
    "    ds_path = \"/cs/home/ybk1/Dissertation/Experiment Anonymised Version/User001_Group_Test_30-08-2019--10-07-00/User001_test1_BET_01_30-08-2019/User001_test1_BET_01_30-08-2019_recording0\"\n",
    "    ds_csv = \"User001_test1_BET_01_30-08-2019_recording0_U1567156556_EEG_rawEEGData.csv\"\n",
    "    json = \"annotations.json\"\n",
    "    \n",
    "    File = namedtuple('File', 'path user no_para are_timestamps_distinct initial_rows final_rows dropped_rows')\n",
    "    files = []\n",
    "        \n",
    "    ds_user = 1 # get the user number\n",
    "    ds_data = pd.read_csv(ds_path + \"/\" + ds_csv)\n",
    "    ds_labels = pd.read_json(ds_path + \"/\" + json)\n",
    "    no_para, are_timestamps_distinct = check_json(ds_labels) # check the json file\n",
    "\n",
    "\n",
    "    ds_labels = array(ds_labels)\n",
    "    if are_timestamps_distinct == False:\n",
    "        print(\"Don't process\")\n",
    "        initial_rows, final_rows  = len(ds_data), len(ds_data)\n",
    "        dropped_rows = 0\n",
    "        files.append(File(ds_path, ds_user, no_para,are_timestamps_distinct,\n",
    "                         initial_rows, final_rows, dropped_rows))\n",
    "    else:\n",
    "        labeled_data, initial_rows, final_rows, dropped_rows = add_labels(ds_data, ds_labels) # Gets the labeled data\n",
    "#         export_datasets_to_csv(labeled_data, ds_path) # Exports the 3 datasets to the appropriate path\n",
    "        labeled_data.to_csv(ds_path + \"/\" + \"annotated_EEG.csv\", index=False)\n",
    "        files.append(File(ds_path, ds_user, no_para, are_timestamps_distinct,\n",
    "                         initial_rows, final_rows, dropped_rows))\n",
    "        \n",
    "    training_files = pd.DataFrame(files)\n",
    "    return training_files\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run this block to rebuild the datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_datasets = get_all_datasets(p)\n",
    "# build_all_datasets(all_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_all_datasets_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract viable datasets from the training files \n",
    "\n",
    "- remove the files that have zero final rows\n",
    "- remove files that have empty JSON's\n",
    "- remove files that only have 4 paragraphs\n",
    "- remove files with indistince timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_training_files():\n",
    "    \"\"\"\n",
    "    Method for removing:\n",
    "    - Files with zero final rows\n",
    "    - remove files with empty Json's\n",
    "    - Files with only 4 paragraphs\n",
    "    - Files with indistinct timestamp\n",
    "    \"\"\"\n",
    "    tf = pd.read_csv(\"Trainingfiles.csv\")\n",
    "    print(\"Initial length of training files: {}\".format(len(tf)))\n",
    "    \n",
    "    zero_final = tf['final_rows'] == 0 # remove the files with zero final row\n",
    "    empty_json = tf['no_para'] == 0 # remove files with empty JSON's\n",
    "    four_para = tf['no_para'] == 4 # remove files with only four paragraphs\n",
    "    indistinct_timestamp = tf['are_timestamps_distinct'] == False # remove files with indistinct timestamps\n",
    "    files_rm = [zero_final, empty_json, four_para, indistinct_timestamp] # store them all in a list\n",
    "    for file_rm in files_rm:\n",
    "        tf = tf.drop(tf[file_rm].index) # drop files sequentially\n",
    "    \n",
    "    print(\"Final length of training files: {}\".format(len(tf)))\n",
    "    tf.to_csv(\"clean_trainingfiles.csv\", index=False)\n",
    "    return tf\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling the data \n",
    "\n",
    "In this part, the data will be sampled by created at "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files = pd.read_csv(\"clean_trainingfiles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8     16\n",
       "23    16\n",
       "22    16\n",
       "2     16\n",
       "20    16\n",
       "19    16\n",
       "18    16\n",
       "3     16\n",
       "6     16\n",
       "24    16\n",
       "11    16\n",
       "7     16\n",
       "9     16\n",
       "25    15\n",
       "1     15\n",
       "10    15\n",
       "17    15\n",
       "12     9\n",
       "16     2\n",
       "21     1\n",
       "Name: user, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_files['user'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store all of user 1's test in a dictionary\n",
    "user_1 = array(training_files[training_files['user'] == 1][\"path\"])\n",
    "user_1_tests = {}\n",
    "file = \"annotated_EEG.csv\"\n",
    "for test in user_1:\n",
    "    dataset = test + \"/\" + file\n",
    "    user_1_tests[extract_test_number(test)] = dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through the dictionary of tests and then sample the tests\n",
    "\n",
    "# for key in user_1_tests: \n",
    "#     test = pd.read_csv(user_1_tests[key]).drop(columns=[\"Timestamp\", \" AdjustedUnix\"])\n",
    "#     sampled_test = sample(test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(slider=1, sample_size=60):\n",
    "    \"\"\"\n",
    "    Method for creating samples within a dataset\n",
    "    :df: test that is being sampled\n",
    "    :slider: the amount by which the window slides during sampling. The lower the number, the more samples.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(user_1_tests[1]).drop([\"Timestamp\", \" AdjustedUnix\"], axis=1) #remove unnecessary columns\n",
    "    Sample = namedtuple('Sample', 'inputs effort attention interest')\n",
    "    sampled_tests = []\n",
    "    \n",
    "    \n",
    "    # Group by paragraph and add each paragraph into an array\n",
    "    paragraphs = df.groupby('para') \n",
    "    paragraphs = [paragraphs.get_group(x) for x in paragraphs.groups]\n",
    "    \n",
    "    incorrect_length = 0\n",
    "    # Loop trough each paragraph to create samples\n",
    "    for para in paragraphs:\n",
    "        \n",
    "        if not len(para) > sample_size: # check the length of paragraph if it is bigger than the sample size\n",
    "            print (\"invalid\")\n",
    "            continue\n",
    "            \n",
    "        new_sample_length = len(para[0:sample_size])\n",
    "        counter = 0\n",
    "        while  new_sample_length >= sample_size:\n",
    "            \"\"\"\n",
    "            **Sliding window algorithm**\n",
    "            - Create new samples based on sample size and iterate using the slider size for size of overlap\n",
    "            - Create separate values for inputs, effort, attention, interest to add to a tuple\n",
    "            \"\"\"\n",
    "            new_sample = para[counter : counter + sample_size]\n",
    "            new_sample_length = len(new_sample)\n",
    "            \n",
    "            #checks new_sample length\n",
    "            if new_sample_length == sample_size:\n",
    "                # Extract the sample specific data\n",
    "                inputs = array(new_sample.iloc[:, :8])\n",
    "                effort, attention, interest = new_sample[[\"effort\", \"attention\", \"interest\"]].T.values\n",
    "                sampled_tests.append(Sample(inputs, int(max(effort)), int(max(attention)), int(max(interest))))\n",
    "            else:\n",
    "                incorrect_length += 1\n",
    "                continue             \n",
    "         \n",
    "            # increase by slider\n",
    "            counter += slider\n",
    "        \n",
    "        \n",
    "    sampled_tests_df = pd.DataFrame(sampled_tests)\n",
    "    inputs_and_labels = {}\n",
    "    \n",
    "    inputs_list = sampled_tests_df['inputs'].values\n",
    "    inputs_list = np.rollaxis(np.dstack(inputs_list),-1)\n",
    "    inputs_and_labels['inputs'] = inputs_list\n",
    "    labels = [\"effort\", \"attention\", \"interest\"]\n",
    "    for label in labels: \n",
    "        inputs_and_labels[label] = array(sampled_tests_df[label].values)\n",
    "    \n",
    "    \n",
    "    return inputs_and_labels\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_and_labels = get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7249, 60, 8)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_and_labels['inputs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_example = pd.DataFrame(user1_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_list = df_example['inputs'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 8)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_list[1039].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = np.rollaxis(np.dstack(inputs_list),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[172495.625     , -16886.33007812, 200873.203125  ,\n",
       "        188091.96875   , 179837.921875  , 157221.484375  ,\n",
       "        172901.453125  , 205096.40625   ],\n",
       "       [172504.875     , -16903.59375   , 200863.953125  ,\n",
       "        188080.625     , 179839.96875   , 157228.671875  ,\n",
       "        172908.046875  , 205104.46875   ],\n",
       "       [172504.875     , -16900.97070312, 200869.828125  ,\n",
       "        188084.484375  , 179827.421875  , 157230.4375    ,\n",
       "        172899.984375  , 205099.328125  ],\n",
       "       [172494.234375  , -16878.89257812, 200887.421875  ,\n",
       "        188101.03125   , 179807.015625  , 157225.671875  ,\n",
       "        172880.484375  , 205080.203125  ],\n",
       "       [172478.03125   , -16841.74609375, 200913.21875   ,\n",
       "        188126.875     , 179780.84375   , 157215.75      ,\n",
       "        172855.921875  , 205053.78125   ],\n",
       "       [172463.390625  , -16800.78710938, 200941.6875    ,\n",
       "        188154.4375    , 179762.28125   , 157205.265625  ,\n",
       "        172833.265625  , 205030.65625   ],\n",
       "       [172444.453125  , -16778.66015625, 200952.359375  ,\n",
       "        188168.171875  , 179743.453125  , 157190.296875  ,\n",
       "        172812.484375  , 205007.90625   ],\n",
       "       [172434.96875   , -16773.98828125, 200955.21875   ,\n",
       "        188171.359375  , 179739.0625    , 157183.46875   ,\n",
       "        172807.515625  , 205004.671875  ],\n",
       "       [172439.40625   , -16799.78515625, 200936.296875  ,\n",
       "        188153.8125    , 179749.40625   , 157181.5625    ,\n",
       "        172819.203125  , 205020.78125   ],\n",
       "       [172452.375     , -16844.03515625, 200904.875     ,\n",
       "        188124.828125  , 179776.828125  , 157188.        ,\n",
       "        172847.046875  , 205050.640625  ],\n",
       "       [172455.046875  , -16884.18554688, 200874.921875  ,\n",
       "        188095.734375  , 179788.5625    , 157190.25      ,\n",
       "        172857.96875   , 205060.84375   ],\n",
       "       [172460.234375  , -16909.31445312, 200858.046875  ,\n",
       "        188075.703125  , 179796.953125  , 157195.015625  ,\n",
       "        172864.5       , 205065.984375  ],\n",
       "       [172464.921875  , -16907.88476562, 200862.        ,\n",
       "        188077.609375  , 179802.34375   , 157201.6875    ,\n",
       "        172867.5625    , 205068.1875    ],\n",
       "       [172459.4375    , -16887.38085938, 200876.15625   ,\n",
       "        188092.546875  , 179791.65625   , 157199.3125    ,\n",
       "        172854.484375  , 205053.015625  ],\n",
       "       [172450.46875   , -16850.33007812, 200901.859375  ,\n",
       "        188118.390625  , 179770.390625  , 157192.53125   ,\n",
       "        172832.40625   , 205028.265625  ],\n",
       "       [172433.78125   , -16813.375     , 200924.9375    ,\n",
       "        188141.65625   , 179741.21875   , 157176.65625   ,\n",
       "        172803.375     , 204999.90625   ],\n",
       "       [172432.828125  , -16778.08984375, 200950.03125   ,\n",
       "        188167.40625   , 179734.96875   , 157174.265625  ,\n",
       "        172794.890625  , 204993.21875   ],\n",
       "       [172440.5       , -16769.98242188, 200957.125     ,\n",
       "        188176.03125   , 179745.03125   , 157177.375     ,\n",
       "        172802.65625   , 205002.46875   ],\n",
       "       [172453.421875  , -16795.875     , 200939.109375  ,\n",
       "        188159.015625  , 179764.71875   , 157182.515625  ,\n",
       "        172823.359375  , 205026.796875  ],\n",
       "       [172469.296875  , -16841.22265625, 200910.5       ,\n",
       "        188130.265625  , 179791.859375  , 157192.15625   ,\n",
       "        172853.25      , 205059.265625  ],\n",
       "       [172484.28125   , -16880.22851562, 200883.796875  ,\n",
       "        188102.359375  , 179815.796875  , 157203.265625  ,\n",
       "        172878.71875   , 205084.921875  ],\n",
       "       [172498.625     , -16897.6328125 , 200874.15625   ,\n",
       "        188090.734375  , 179831.625     , 157217.328125  ,\n",
       "        172895.40625   , 205100.984375  ],\n",
       "       [172501.296875  , -16896.10546875, 200879.171875  ,\n",
       "        188094.453125  , 179832.140625  , 157223.953125  ,\n",
       "        172892.96875   , 205097.359375  ],\n",
       "       [172483.65625   , -16885.04492188, 200885.359375  ,\n",
       "        188099.21875   , 179806.4375    , 157211.375     ,\n",
       "        172864.125     , 205067.03125   ],\n",
       "       [172469.390625  , -16849.37695312, 200910.265625  ,\n",
       "        188124.296875  , 179780.59375   , 157202.125     ,\n",
       "        172837.09375   , 205038.625     ],\n",
       "       [172450.09375   , -16809.41796875, 200937.109375  ,\n",
       "        188150.578125  , 179749.125     , 157185.953125  ,\n",
       "        172806.28125   , 205007.34375   ],\n",
       "       [172429.015625  , -16782.5703125 , 200952.59375   ,\n",
       "        188166.5       , 179732.8125    , 157173.9375    ,\n",
       "        172787.25      , 204989.359375  ],\n",
       "       [172423.90625   , -16773.75      , 200958.75      ,\n",
       "        188175.65625   , 179741.453125  , 157176.46875   ,\n",
       "        172792.359375  , 204994.9375    ],\n",
       "       [172431.0625    , -16794.4453125 , 200943.453125  ,\n",
       "        188162.96875   , 179762.4375    , 157183.625     ,\n",
       "        172812.140625  , 205014.59375   ],\n",
       "       [172449.65625   , -16835.78515625, 200915.40625   ,\n",
       "        188136.9375    , 179794.046875  , 157196.96875   ,\n",
       "        172844.        , 205046.578125  ],\n",
       "       [172458.90625   , -16876.31835938, 200886.890625  ,\n",
       "        188108.75      , 179816.5       , 157207.078125  ,\n",
       "        172867.078125  , 205069.140625  ],\n",
       "       [172468.34375   , -16896.05859375, 200874.203125  ,\n",
       "        188094.78125   , 179828.046875  , 157216.515625  ,\n",
       "        172880.        , 205081.875     ],\n",
       "       [172473.875     , -16896.82226562, 200874.640625  ,\n",
       "        188092.828125  , 179827.609375  , 157222.578125  ,\n",
       "        172880.4375    , 205082.625     ],\n",
       "       [172471.5       , -16874.6953125 , 200892.28125   ,\n",
       "        188109.328125  , 179817.265625  , 157224.71875   ,\n",
       "        172871.1875    , 205072.375     ],\n",
       "       [172464.015625  , -16836.21679688, 200920.84375   ,\n",
       "        188138.984375  , 179797.140625  , 157221.765625  ,\n",
       "        172853.78125   , 205054.734375  ],\n",
       "       [172456.28125   , -16792.5859375 , 200949.984375  ,\n",
       "        188168.59375   , 179775.921875  , 157213.28125   ,\n",
       "        172834.171875  , 205036.46875   ],\n",
       "       [172448.125     , -16768.40820312, 200963.65625   ,\n",
       "        188182.5625    , 179760.28125   , 157202.5       ,\n",
       "        172820.828125  , 205025.75      ],\n",
       "       [172450.984375  , -16771.79492188, 200960.234375  ,\n",
       "        188179.328125  , 179764.34375   , 157199.96875   ,\n",
       "        172828.984375  , 205033.796875  ],\n",
       "       [172463.96875   , -16800.69140625, 200939.921875  ,\n",
       "        188160.578125  , 179788.09375   , 157206.984375  ,\n",
       "        172853.828125  , 205061.75      ],\n",
       "       [172482.65625   , -16844.703125  , 200909.96875   ,\n",
       "        188132.40625   , 179821.46875   , 157220.671875  ,\n",
       "        172885.25      , 205095.65625   ],\n",
       "       [172504.59375   , -16881.13476562, 200886.078125  ,\n",
       "        188108.1875    , 179848.734375  , 157236.015625  ,\n",
       "        172912.90625   , 205124.78125   ],\n",
       "       [172520.328125  , -16903.06835938, 200872.734375  ,\n",
       "        188094.40625   , 179861.90625   , 157249.375     ,\n",
       "        172927.640625  , 205140.046875  ],\n",
       "       [172525.28125   , -16907.21679688, 200874.25      ,\n",
       "        188092.390625  , 179861.90625   , 157253.671875  ,\n",
       "        172926.59375   , 205139.859375  ],\n",
       "       [172521.75      , -16881.0859375 , 200894.671875  ,\n",
       "        188112.65625   , 179849.9375    , 157254.625     ,\n",
       "        172912.671875  , 205126.265625  ],\n",
       "       [172504.109375  , -16842.93945312, 200922.125     ,\n",
       "        188138.796875  , 179820.03125   , 157246.609375  ,\n",
       "        172885.71875   , 205097.890625  ],\n",
       "       [172479.5       , -16798.06835938, 200951.890625  ,\n",
       "        188169.171875  , 179788.09375   , 157232.25      ,\n",
       "        172854.34375   , 205066.1875    ],\n",
       "       [172460.046875  , -16766.59765625, 200972.25      ,\n",
       "        188190.859375  , 179767.8125    , 157220.90625   ,\n",
       "        172834.21875   , 205045.4375    ],\n",
       "       [172453.09375   , -16764.45117188, 200971.90625   ,\n",
       "        188192.109375  , 179768.109375  , 157213.890625  ,\n",
       "        172834.796875  , 205044.96875   ],\n",
       "       [172454.765625  , -16795.92382812, 200947.109375  ,\n",
       "        188169.171875  , 179786.078125  , 157211.984375  ,\n",
       "        172850.34375   , 205062.125     ],\n",
       "       [172471.109375  , -16843.60742188, 200912.921875  ,\n",
       "        188137.265625  , 179819.609375  , 157222.484375  ,\n",
       "        172879.53125   , 205093.359375  ],\n",
       "       [172501.296875  , -16877.31835938, 200892.65625   ,\n",
       "        188116.140625  , 179855.171875  , 157242.359375  ,\n",
       "        172915.578125  , 205129.984375  ],\n",
       "       [172513.40625   , -16894.53320312, 200884.65625   ,\n",
       "        188104.515625  , 179865.421875  , 157254.09375   ,\n",
       "        172927.78125   , 205142.5625    ],\n",
       "       [172510.21875   , -16897.44140625, 200884.609375  ,\n",
       "        188103.5       , 179857.984375  , 157254.671875  ,\n",
       "        172919.203125  , 205133.84375   ],\n",
       "       [172504.25      , -16874.36328125, 200900.34375   ,\n",
       "        188118.90625   , 179838.96875   , 157251.375     ,\n",
       "        172900.265625  , 205113.        ],\n",
       "       [172491.71875   , -16837.02539062, 200927.09375   ,\n",
       "        188146.234375  , 179814.546875  , 157243.171875  ,\n",
       "        172872.609375  , 205084.015625  ],\n",
       "       [172486.421875  , -16798.06835938, 200953.453125  ,\n",
       "        188172.359375  , 179798.046875  , 157235.59375   ,\n",
       "        172853.484375  , 205065.421875  ],\n",
       "       [172476.640625  , -16770.36328125, 200969.859375  ,\n",
       "        188190.4375    , 179781.265625  , 157223.34375   ,\n",
       "        172838.421875  , 205049.96875   ],\n",
       "       [172466.625     , -16763.97460938, 200972.109375  ,\n",
       "        188194.296875  , 179775.15625   , 157215.5625    ,\n",
       "        172833.609375  , 205044.8125    ],\n",
       "       [172475.453125  , -16784.71679688, 200958.796875  ,\n",
       "        188182.515625  , 179793.609375  , 157221.390625  ,\n",
       "        172851.203125  , 205065.28125   ],\n",
       "       [172490.765625  , -16837.265625  , 200920.796875  ,\n",
       "        188144.421875  , 179818.65625   , 157223.484375  ,\n",
       "        172873.046875  , 205089.640625  ]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[1,2,3],[4,5,6], [7,8,9]], columns=[\"A\", \"B\",\"C\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "a , b , c = df[[\"A\", \"B\", \"C\"]].T.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 6, 9])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
