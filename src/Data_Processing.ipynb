{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import namedtuple\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traversing the folder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check the json data\n",
    "- check the whether the time start and time end are distinct\n",
    "- check whether there are 5 paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_user_number(s):\n",
    "    \"\"\"\n",
    "    Extracts the user number from a given path\n",
    "    :s: path of the file\n",
    "    \"\"\"\n",
    "    s = str(s).split(\"_\")[0] # extract the user from the path\n",
    "    s = s.split(\"/\")[6] # split the path to isolate the user\n",
    "    s = s.replace(\"User\",\"\") # remove the word \"User\"\n",
    "    s = int(re.sub(r'[a-z]+', '', s, re.I)) # remove alphabetical characters\n",
    "    return s \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_test_number(s):\n",
    "    \"\"\"\n",
    "    Extracts the test number from a given path\n",
    "    :s: path of the file\n",
    "    \"\"\"\n",
    "    s = s.split(\"/\")[7]\n",
    "    s = s.split(\"_\")[1]\n",
    "    s = int(re.sub(r'[a-z]+', '', s, re.I))\n",
    "    return s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path(\"/cs/home/ybk1/Dissertation/Experiment Anonymised Version\")\n",
    "# p = Path(\"C://Users//User//OneDrive - University of St Andrews//Modules//CS5099//4. Data Documents//dataset//INSTRUMENTED DIGITAL AND PAPER READINGDATASET//Experiment Anonymised Version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_datasets(p):\n",
    "    \"\"\"\n",
    "    Loops through all the folders to find all the datasets and json files for training\n",
    "    p: path of the root folder\n",
    "    \"\"\"\n",
    "    File = namedtuple('File', 'name path size')\n",
    "    files = []\n",
    "    for item in p.glob('**/*'): # loops thorough all the files in all the sub-directories\n",
    "        if item.match('*rawEEGData.csv')  and \"baseline\" not in item.name:\n",
    "            name = item.name\n",
    "            path = Path.resolve(item).parent\n",
    "            size = item.stat().st_size\n",
    "\n",
    "            files.append(File(name,path, size )) # stores the name, path and size in named tuple\n",
    "    \n",
    "    df = pd.DataFrame(files)\n",
    "    df['user'] = df[\"path\"].apply(extract_user_number)\n",
    "    df.to_csv(\"All EEG files.csv\", index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the value counts we can see that: \n",
    "- user 20, 23, 17, 7, 9, 15, 2 - all have duplicates\n",
    "- user 14 is one short\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the datasets using the alldatasets csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_json(labels):\n",
    "    \"\"\"\n",
    "    Method for checking the json to see how many paragraphs it contains\n",
    "    and whether the timestamps are distinct\n",
    "    :labels: annotations.json in a DataFrame\n",
    "    \"\"\"\n",
    "    #check whether there are 5 paragraphs\n",
    "    no_para = len(labels)\n",
    "    are_timestamps_distinct = False\n",
    "    if no_para == 0: # checks if the json is empty\n",
    "        return no_para, are_timestamps_distinct\n",
    "    else:\n",
    "        if len(labels) != 5:\n",
    "            print (\"There are less than 5 paragraphs in the dataset\")\n",
    "        else:\n",
    "            print(\"There are 5 paragraphs in the dataset\")\n",
    "\n",
    "        #check whether the timestamps are distincts\n",
    "        cols = [\"timeRangeStart\", \"timeRangeEnd\"]\n",
    "        for col in cols:\n",
    "            array_length= len(array(labels[col]))\n",
    "            set_length = len(set(array(labels[col])))\n",
    "            if array_length == set_length:\n",
    "                print(col + \" has distinct values\")\n",
    "                are_timestamps_distinct = True\n",
    "                break\n",
    "            else: \n",
    "                print(col + \" does not have distinct values\")\n",
    "                are_timestamps_distinct = False\n",
    "                break\n",
    "\n",
    "        print(are_timestamps_distinct)\n",
    "        return no_para, are_timestamps_distinct \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(timestamp, annotation, labels):\n",
    "    \"\"\"\n",
    "    Method for adding the labels to the dataframe\n",
    "    \n",
    "    :return: relevant score of attention/interest/effort within the correct time range\n",
    "    :timestamp: timestamp at the current iteration\n",
    "    :annotation: annotation at the current interation\n",
    "    :labels: annotations.json in array format\n",
    "    \"\"\"\n",
    "    for row in labels:\n",
    "        time_start = row[-2]\n",
    "        time_end = row[-1]\n",
    "        ann_dict = {\"effort\": row[-3], \"attention\": row[-4], \"interest\": row[-5],\"para\": row[-6]}\n",
    "        if timestamp >= time_start and timestamp < time_end: # checks if the timestamp is witin the start and end range\n",
    "            return ann_dict[annotation] # returns the relevant score stored in the dictionary\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(data, labels):\n",
    "    \"\"\"\n",
    "    Method for processing the dataset by creating columns for effort, attention, interest and paragraph\n",
    "    :data: EEG dataset\n",
    "    :labels: annotations.json in array format\n",
    "    \"\"\"\n",
    "    annotations = [\"effort\", \"attention\", \"interest\", \"para\"]\n",
    "    for ann in annotations: # loops through the labels, and creates new columns based on the values within the timestamp range\n",
    "        data[ann] = data[\"Timestamp\"].apply(label, annotation=ann, labels=labels)\n",
    "    \n",
    "    initial_rows = len(data)\n",
    "    data = data.dropna()\n",
    "    final_rows = len(data)\n",
    "    dropped_rows = initial_rows - final_rows\n",
    "    print(\"Initial rows: {0}\\nFinal rows: {1}\\nDropped rows: {2}\".format(initial_rows, \n",
    "                                                                         final_rows, \n",
    "                                                                         dropped_rows))\n",
    "    return data , initial_rows, final_rows, dropped_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_datasets_to_csv(labeled_data, path):\n",
    "    \"\"\"\n",
    "    Exports the labeled data as three separate csv files for interest, effort and attention\n",
    "    :labeled_data: annotated datasets with columns for interest, effort and attention\n",
    "    :path: path to save the dataset\n",
    "    \"\"\"\n",
    "    X = labeled_data.iloc[:,:9] # inputs i.e channels and timestamp\n",
    "    labels = ['interest', 'effort', 'attention']\n",
    "    for label in labels:\n",
    "        dataset = pd.concat([X, labeled_data[label]], axis=1).to_csv(path +  \"/\" +'EEG_' + label + '_dataset.csv', index=False)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_all_datasets(df):\n",
    "    \"\"\"\n",
    "    Method for building all the training sets by applying annotations at relevant timestamps\n",
    "    and splitting the training sets into attention, interest and effort\n",
    "    :df: DataFrame that includes all of the datasets\n",
    "    :ds: dataset\n",
    "    \"\"\"\n",
    "    ds = array(df)\n",
    "    json = \"annotations.json\" # json file that contains the labels and annotations for the data\n",
    "    File = namedtuple('File', 'path user test no_para are_timestamps_distinct initial_rows final_rows dropped_rows')\n",
    "    files = []\n",
    "    \n",
    "    for row in ds:\n",
    "        ds_path = str(row[1]) # Get the path of the dataset\n",
    "        ds_csv = row[0] # get the name of the csv file\n",
    "        print(\"Working on this csv: {}\".format(ds_csv))\n",
    "        ds_user = row[3] # get the user number\n",
    "        ds_test = extract_test_number(ds_path)\n",
    "        ds_data = pd.read_csv(ds_path + \"/\" + ds_csv)\n",
    "        ds_labels = pd.read_json(ds_path + \"/\" + json)\n",
    "        no_para, are_timestamps_distinct = check_json(ds_labels) # check the json file\n",
    "                \n",
    "        ds_labels = array(ds_labels)\n",
    "        if are_timestamps_distinct == False:\n",
    "            print(\"Don't process\")\n",
    "            initial_rows, final_rows  = len(ds_data), len(ds_data)\n",
    "            dropped_rows = 0\n",
    "            files.append(File(ds_path, ds_user, ds_test, no_para,are_timestamps_distinct,\n",
    "                             initial_rows, final_rows, dropped_rows))          \n",
    "        else:\n",
    "            labeled_data, initial_rows, final_rows, dropped_rows = add_labels(ds_data, ds_labels) # Gets the labeled data\n",
    "            export_datasets_to_csv(labeled_data, ds_path) # Exports the 3 datasets to the appropriate path\n",
    "            files.append(File(ds_path, ds_user, ds_test, no_para, are_timestamps_distinct,\n",
    "                         initial_rows, final_rows, dropped_rows))\n",
    "            \n",
    "    training_files = pd.DataFrame(files)\n",
    "    training_files.to_csv(\"Trainingfiles.csv\", index=False)\n",
    "    return training_files\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def build_all_datasets_test():\n",
    "    \"\"\"\n",
    "    Test method for building a single group of training sets by applying annotations at relevant timestamps\n",
    "    and splitting the training sets into attention, interest and effort\n",
    "    :df: DataFrame that includes all of the datasets\n",
    "    :ds: dataset\n",
    "    \"\"\"\n",
    "    # User001_test1_BET_01_30-08-2019_recording0_U1567156556_EYETRACKER_cleanFixationData.csv\n",
    "    # /cs/home/ybk1/Dissertation/Experiment Anonymised Version/User001_Group_Test_30-08-2019--10-07-00/User001_test1_BET_01_30-08-2019/User001_test1_BET_01_30-08-2019_recording0\n",
    "    ds_path = \"/cs/home/ybk1/Dissertation/Experiment Anonymised Version/User001_Group_Test_30-08-2019--10-07-00/User001_test1_BET_01_30-08-2019/User001_test1_BET_01_30-08-2019_recording0\"\n",
    "    ds_csv = \"User001_test1_BET_01_30-08-2019_recording0_U1567156556_EEG_rawEEGData.csv\"\n",
    "    json = \"annotations.json\"\n",
    "    \n",
    "    File = namedtuple('File', 'path user no_para are_timestamps_distinct initial_rows final_rows dropped_rows')\n",
    "    files = []\n",
    "        \n",
    "    ds_user = 1 # get the user number\n",
    "    ds_data = pd.read_csv(ds_path + \"/\" + ds_csv)\n",
    "    ds_labels = pd.read_json(ds_path + \"/\" + json)\n",
    "    no_para, are_timestamps_distinct = check_json(ds_labels) # check the json file\n",
    "\n",
    "\n",
    "    ds_labels = array(ds_labels)\n",
    "    if are_timestamps_distinct == False:\n",
    "        print(\"Don't process\")\n",
    "        initial_rows, final_rows  = len(ds_data), len(ds_data)\n",
    "        dropped_rows = 0\n",
    "        files.append(File(ds_path, ds_user, no_para,are_timestamps_distinct,\n",
    "                         initial_rows, final_rows, dropped_rows))\n",
    "    else:\n",
    "        labeled_data, initial_rows, final_rows, dropped_rows = add_labels(ds_data, ds_labels) # Gets the labeled data\n",
    "        export_datasets_to_csv(labeled_data, ds_path) # Exports the 3 datasets to the appropriate path\n",
    "        files.append(File(ds_path, ds_user, no_para, are_timestamps_distinct,\n",
    "                         initial_rows, final_rows, dropped_rows))\n",
    "        \n",
    "    training_files = pd.DataFrame(files)\n",
    "    return training_files\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_datasets = get_all_datasets(p)\n",
    "# build_all_datasets(all_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract viable datasets from the training files \n",
    "\n",
    "- remove the files that have zero final rows\n",
    "- remove files that have empty JSON's\n",
    "- remove files that only have 4 paragraphs\n",
    "- remove files with indistince timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_training_files():\n",
    "    \"\"\"\n",
    "    Method for removing:\n",
    "    - Files with zero final rows\n",
    "    - remove files with empty Json's\n",
    "    - Files with only 4 paragraphs\n",
    "    - Files with indistinct timestamp\n",
    "    \"\"\"\n",
    "    tf = pd.read_csv(\"Trainingfiles.csv\")\n",
    "    print(\"Initial length of training files: {}\".format(len(tf)))\n",
    "    \n",
    "    zero_final = tf['final_rows'] == 0 # remove the files with zero final row\n",
    "    empty_json = tf['no_para'] == 0 # remove files with empty JSON's\n",
    "    four_para = tf['no_para'] == 4 # remove files with only four paragraphs\n",
    "    indistinct_timestamp = tf['are_timestamps_distinct'] == False # remove files with indistinct timestamps\n",
    "    files_rm = [zero_final, empty_json, four_para, indistinct_timestamp] # store them all in a list\n",
    "    for file_rm in files_rm:\n",
    "        tf = tf.drop(tf[file_rm].index) # drop files sequentially\n",
    "    \n",
    "    print(\"Final length of training files: {}\".format(len(tf)))\n",
    "    tf.to_csv(\"clean_trainingfiles.csv\", index=False)\n",
    "    return tf\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
