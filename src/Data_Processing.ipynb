{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import namedtuple\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traversing the folder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check the json data\n",
    "- check the whether the time start and time end are distinct\n",
    "- check whether there are 5 paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_user_number(s):\n",
    "    \"\"\"\n",
    "    Extracts the user number from a given path\n",
    "    :s: path of the file\n",
    "    \"\"\"\n",
    "    s = str(s).split(\"_\")[0] # extract the user from the path\n",
    "    s = s.split(\"/\")[6] # split the path to isolate the user\n",
    "    s = s.replace(\"User\",\"\") # remove the word \"User\"\n",
    "    s = int(re.sub(r'[a-z]+', '', s, re.I)) # remove alphabetical characters\n",
    "    return s \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_test_number(s):\n",
    "    \"\"\"\n",
    "    Extracts the test number from a given path\n",
    "    :s: path of the file\n",
    "    \"\"\"\n",
    "    s = s.split(\"/\")[7]\n",
    "    s = s.split(\"_\")[1]\n",
    "    s = int(re.sub(r'[a-z]+', '', s, re.I))\n",
    "    return s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path(\"/cs/home/ybk1/Dissertation/Experiment Anonymised Version\")\n",
    "# p = Path(\"C://Users//User//OneDrive - University of St Andrews//Modules//CS5099//4. Data Documents//dataset//INSTRUMENTED DIGITAL AND PAPER READINGDATASET//Experiment Anonymised Version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_datasets(p):\n",
    "    \"\"\"\n",
    "    Loops through all the folders to find all the datasets and json files for training\n",
    "    p: path of the root folder\n",
    "    \"\"\"\n",
    "    File = namedtuple('File', 'name path size')\n",
    "    files = []\n",
    "    for item in p.glob('**/*'): # loops thorough all the files in all the sub-directories\n",
    "        if item.match('*rawEEGData.csv')  and \"baseline\" not in item.name:\n",
    "            name = item.name\n",
    "            path = Path.resolve(item).parent\n",
    "            size = item.stat().st_size\n",
    "\n",
    "            files.append(File(name,path, size )) # stores the name, path and size in named tuple\n",
    "    \n",
    "    df = pd.DataFrame(files)\n",
    "    df['user'] = df[\"path\"].apply(extract_user_number)\n",
    "    df.to_csv(\"All EEG files.csv\", index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the value counts we can see that: \n",
    "- user 20, 23, 17, 7, 9, 15, 2 - all have duplicates\n",
    "- user 14 is one short\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the datasets using the alldatasets csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_json(labels):\n",
    "    \"\"\"\n",
    "    Method for checking the json to see how many paragraphs it contains\n",
    "    and whether the timestamps are distinct\n",
    "    :labels: annotations.json in a DataFrame\n",
    "    \"\"\"\n",
    "    #check whether there are 5 paragraphs\n",
    "    no_para = len(labels)\n",
    "    are_timestamps_distinct = False\n",
    "    if no_para == 0: # checks if the json is empty\n",
    "        return no_para, are_timestamps_distinct\n",
    "    else:\n",
    "        if len(labels) != 5:\n",
    "            print (\"There are less than 5 paragraphs in the dataset\")\n",
    "        else:\n",
    "            print(\"There are 5 paragraphs in the dataset\")\n",
    "\n",
    "        #check whether the timestamps are distincts\n",
    "        cols = [\"timeRangeStart\", \"timeRangeEnd\"]\n",
    "        for col in cols:\n",
    "            array_length= len(array(labels[col]))\n",
    "            set_length = len(set(array(labels[col])))\n",
    "            if array_length == set_length:\n",
    "                print(col + \" has distinct values\")\n",
    "                are_timestamps_distinct = True\n",
    "                break\n",
    "            else: \n",
    "                print(col + \" does not have distinct values\")\n",
    "                are_timestamps_distinct = False\n",
    "                break\n",
    "\n",
    "        print(are_timestamps_distinct)\n",
    "        return no_para, are_timestamps_distinct \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(timestamp, annotation, labels):\n",
    "    \"\"\"\n",
    "    Method for adding the labels to the dataframe\n",
    "    \n",
    "    :return: relevant score of attention/interest/effort within the correct time range\n",
    "    :timestamp: timestamp at the current iteration\n",
    "    :annotation: annotation at the current interation\n",
    "    :labels: annotations.json in array format\n",
    "    \"\"\"\n",
    "    for row in labels:\n",
    "        time_start = row[-2]\n",
    "        time_end = row[-1]\n",
    "        ann_dict = {\"effort\": row[-3], \"attention\": row[-4], \"interest\": row[-5],\"para\": row[-6]}\n",
    "        if timestamp >= time_start and timestamp < time_end: # checks if the timestamp is witin the start and end range\n",
    "            return ann_dict[annotation] # returns the relevant score stored in the dictionary\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(data, labels):\n",
    "    \"\"\"\n",
    "    Method for processing the dataset by creating columns for effort, attention, interest and paragraph\n",
    "    :data: EEG dataset\n",
    "    :labels: annotations.json in array format\n",
    "    \"\"\"\n",
    "    annotations = [\"effort\", \"attention\", \"interest\", \"para\"]\n",
    "    for ann in annotations: # loops through the labels, and creates new columns based on the values within the timestamp range\n",
    "        data[ann] = data[\"Timestamp\"].apply(label, annotation=ann, labels=labels)\n",
    "    \n",
    "    initial_rows = len(data)\n",
    "    data = data.dropna()\n",
    "    final_rows = len(data)\n",
    "    dropped_rows = initial_rows - final_rows\n",
    "    print(\"Initial rows: {0}\\nFinal rows: {1}\\nDropped rows: {2}\".format(initial_rows, \n",
    "                                                                         final_rows, \n",
    "                                                                         dropped_rows))\n",
    "    return data , initial_rows, final_rows, dropped_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_datasets_to_csv(labeled_data, path):\n",
    "    \"\"\"\n",
    "    Exports the labeled data as three separate csv files for interest, effort and attention\n",
    "    :labeled_data: annotated datasets with columns for interest, effort and attention\n",
    "    :path: path to save the dataset\n",
    "    \"\"\"\n",
    "    X = labeled_data.iloc[:,:9] # inputs i.e channels and timestamp\n",
    "    labels = ['interest', 'effort', 'attention']\n",
    "    for label in labels:\n",
    "        dataset = pd.concat([X, labeled_data[label]], axis=1).to_csv(path +  \"/\" +'EEG_' + label + '_dataset.csv', index=False)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_all_datasets(df):\n",
    "    \"\"\"\n",
    "    Method for building all the training sets by applying annotations at relevant timestamps\n",
    "    and splitting the training sets into attention, interest and effort\n",
    "    :df: DataFrame that includes all of the datasets\n",
    "    :ds: dataset\n",
    "    \"\"\"\n",
    "    ds = array(df)\n",
    "    json = \"annotations.json\" # json file that contains the labels and annotations for the data\n",
    "    File = namedtuple('File', 'path user test no_para are_timestamps_distinct initial_rows final_rows dropped_rows')\n",
    "    files = []\n",
    "    \n",
    "    for row in ds:\n",
    "        ds_path = str(row[1]) # Get the path of the dataset\n",
    "        ds_csv = row[0] # get the name of the csv file\n",
    "        print(\"Working on this csv: {}\".format(ds_csv))\n",
    "        ds_user = row[3] # get the user number\n",
    "        ds_test = extract_test_number(ds_path)\n",
    "        ds_data = pd.read_csv(ds_path + \"/\" + ds_csv)\n",
    "        ds_labels = pd.read_json(ds_path + \"/\" + json)\n",
    "        no_para, are_timestamps_distinct = check_json(ds_labels) # check the json file\n",
    "                \n",
    "        ds_labels = array(ds_labels)\n",
    "        if are_timestamps_distinct == False:\n",
    "            print(\"Don't process\")\n",
    "            initial_rows, final_rows  = len(ds_data), len(ds_data)\n",
    "            dropped_rows = 0\n",
    "            files.append(File(ds_path, ds_user, ds_test, no_para,are_timestamps_distinct,\n",
    "                             initial_rows, final_rows, dropped_rows))          \n",
    "        else:\n",
    "            labeled_data, initial_rows, final_rows, dropped_rows = add_labels(ds_data, ds_labels) # Gets the labeled data\n",
    "#             export_datasets_to_csv(labeled_data, ds_path) # Exports the 3 datasets to the appropriate path\n",
    "            labeled_data.to_csv(ds_path + \"/\" + \"annotated_EEG.csv\", index=False)\n",
    "            files.append(File(ds_path, ds_user, ds_test, no_para, are_timestamps_distinct,\n",
    "                         initial_rows, final_rows, dropped_rows))\n",
    "            \n",
    "    training_files = pd.DataFrame(files)\n",
    "    training_files.to_csv(\"Trainingfiles.csv\", index=False)\n",
    "    return training_files\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_all_datasets_test():\n",
    "    \"\"\"\n",
    "    Test method for building a single group of training sets by applying annotations at relevant timestamps\n",
    "    and splitting the training sets into attention, interest and effort\n",
    "    :df: DataFrame that includes all of the datasets\n",
    "    :ds: dataset\n",
    "    \"\"\"\n",
    "    # User001_test1_BET_01_30-08-2019_recording0_U1567156556_EYETRACKER_cleanFixationData.csv\n",
    "    # /cs/home/ybk1/Dissertation/Experiment Anonymised Version/User001_Group_Test_30-08-2019--10-07-00/User001_test1_BET_01_30-08-2019/User001_test1_BET_01_30-08-2019_recording0\n",
    "    ds_path = \"/cs/home/ybk1/Dissertation/Experiment Anonymised Version/User001_Group_Test_30-08-2019--10-07-00/User001_test1_BET_01_30-08-2019/User001_test1_BET_01_30-08-2019_recording0\"\n",
    "    ds_csv = \"User001_test1_BET_01_30-08-2019_recording0_U1567156556_EEG_rawEEGData.csv\"\n",
    "    json = \"annotations.json\"\n",
    "    \n",
    "    File = namedtuple('File', 'path user no_para are_timestamps_distinct initial_rows final_rows dropped_rows')\n",
    "    files = []\n",
    "        \n",
    "    ds_user = 1 # get the user number\n",
    "    ds_data = pd.read_csv(ds_path + \"/\" + ds_csv)\n",
    "    ds_labels = pd.read_json(ds_path + \"/\" + json)\n",
    "    no_para, are_timestamps_distinct = check_json(ds_labels) # check the json file\n",
    "\n",
    "\n",
    "    ds_labels = array(ds_labels)\n",
    "    if are_timestamps_distinct == False:\n",
    "        print(\"Don't process\")\n",
    "        initial_rows, final_rows  = len(ds_data), len(ds_data)\n",
    "        dropped_rows = 0\n",
    "        files.append(File(ds_path, ds_user, no_para,are_timestamps_distinct,\n",
    "                         initial_rows, final_rows, dropped_rows))\n",
    "    else:\n",
    "        labeled_data, initial_rows, final_rows, dropped_rows = add_labels(ds_data, ds_labels) # Gets the labeled data\n",
    "#         export_datasets_to_csv(labeled_data, ds_path) # Exports the 3 datasets to the appropriate path\n",
    "        labeled_data.to_csv(ds_path + \"/\" + \"annotated_EEG.csv\", index=False)\n",
    "        files.append(File(ds_path, ds_user, no_para, are_timestamps_distinct,\n",
    "                         initial_rows, final_rows, dropped_rows))\n",
    "        \n",
    "    training_files = pd.DataFrame(files)\n",
    "    return training_files\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run this block to rebuild the datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_datasets = get_all_datasets(p)\n",
    "# build_all_datasets(all_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_all_datasets_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract viable datasets from the training files \n",
    "\n",
    "- remove the files that have zero final rows\n",
    "- remove files that have empty JSON's\n",
    "- remove files that only have 4 paragraphs\n",
    "- remove files with indistince timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_training_files():\n",
    "    \"\"\"\n",
    "    Method for removing:\n",
    "    - Files with zero final rows\n",
    "    - remove files with empty Json's\n",
    "    - Files with only 4 paragraphs\n",
    "    - Files with indistinct timestamp\n",
    "    \"\"\"\n",
    "    tf = pd.read_csv(\"Trainingfiles.csv\")\n",
    "    print(\"Initial length of training files: {}\".format(len(tf)))\n",
    "    \n",
    "    zero_final = tf['final_rows'] == 0 # remove the files with zero final row\n",
    "    empty_json = tf['no_para'] == 0 # remove files with empty JSON's\n",
    "    four_para = tf['no_para'] == 4 # remove files with only four paragraphs\n",
    "    indistinct_timestamp = tf['are_timestamps_distinct'] == False # remove files with indistinct timestamps\n",
    "    files_rm = [zero_final, empty_json, four_para, indistinct_timestamp] # store them all in a list\n",
    "    for file_rm in files_rm:\n",
    "        tf = tf.drop(tf[file_rm].index) # drop files sequentially\n",
    "    \n",
    "    tf = tf[(tf['user'] != 21) & (tf['user'] != 16)] # remove users 16 and 21 as they only have 2 and 1 test respectively\n",
    "    print(\"Final length of training files: {}\".format(len(tf)))\n",
    "    tf.to_csv(\"clean_trainingfiles.csv\", index=False)\n",
    "    return tf\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling the data \n",
    "\n",
    "In this part, the data will be sampled by created at "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial length of training files: 424\n",
      "Final length of training files: 277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/home/ybk1/python/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>user</th>\n",
       "      <th>test</th>\n",
       "      <th>no_para</th>\n",
       "      <th>are_timestamps_distinct</th>\n",
       "      <th>initial_rows</th>\n",
       "      <th>final_rows</th>\n",
       "      <th>dropped_rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>/cs/home/ybk1/Dissertation/Experiment Anonymis...</td>\n",
       "      <td>24</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>11291</td>\n",
       "      <td>7242</td>\n",
       "      <td>4049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>/cs/home/ybk1/Dissertation/Experiment Anonymis...</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>11021</td>\n",
       "      <td>9556</td>\n",
       "      <td>1465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>/cs/home/ybk1/Dissertation/Experiment Anonymis...</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>8709</td>\n",
       "      <td>7223</td>\n",
       "      <td>1486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>/cs/home/ybk1/Dissertation/Experiment Anonymis...</td>\n",
       "      <td>24</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>9366</td>\n",
       "      <td>8011</td>\n",
       "      <td>1355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>/cs/home/ybk1/Dissertation/Experiment Anonymis...</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>7188</td>\n",
       "      <td>5405</td>\n",
       "      <td>1783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>/cs/home/ybk1/Dissertation/Experiment Anonymis...</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>16432</td>\n",
       "      <td>6542</td>\n",
       "      <td>9890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>/cs/home/ybk1/Dissertation/Experiment Anonymis...</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>16963</td>\n",
       "      <td>5046</td>\n",
       "      <td>11917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>/cs/home/ybk1/Dissertation/Experiment Anonymis...</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>9291</td>\n",
       "      <td>2721</td>\n",
       "      <td>6570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>/cs/home/ybk1/Dissertation/Experiment Anonymis...</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>13667</td>\n",
       "      <td>3681</td>\n",
       "      <td>9986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>/cs/home/ybk1/Dissertation/Experiment Anonymis...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>11086</td>\n",
       "      <td>3768</td>\n",
       "      <td>7318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>277 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  path  user  test  no_para  \\\n",
       "50   /cs/home/ybk1/Dissertation/Experiment Anonymis...    24    15        5   \n",
       "51   /cs/home/ybk1/Dissertation/Experiment Anonymis...    24    13        5   \n",
       "52   /cs/home/ybk1/Dissertation/Experiment Anonymis...    24    12        5   \n",
       "53   /cs/home/ybk1/Dissertation/Experiment Anonymis...    24    14        5   \n",
       "54   /cs/home/ybk1/Dissertation/Experiment Anonymis...    24     1        5   \n",
       "..                                                 ...   ...   ...      ...   \n",
       "419  /cs/home/ybk1/Dissertation/Experiment Anonymis...    11    14        5   \n",
       "420  /cs/home/ybk1/Dissertation/Experiment Anonymis...    11     8        5   \n",
       "421  /cs/home/ybk1/Dissertation/Experiment Anonymis...    11    10        5   \n",
       "422  /cs/home/ybk1/Dissertation/Experiment Anonymis...    11     9        5   \n",
       "423  /cs/home/ybk1/Dissertation/Experiment Anonymis...    11    11        5   \n",
       "\n",
       "     are_timestamps_distinct  initial_rows  final_rows  dropped_rows  \n",
       "50                      True         11291        7242          4049  \n",
       "51                      True         11021        9556          1465  \n",
       "52                      True          8709        7223          1486  \n",
       "53                      True          9366        8011          1355  \n",
       "54                      True          7188        5405          1783  \n",
       "..                       ...           ...         ...           ...  \n",
       "419                     True         16432        6542          9890  \n",
       "420                     True         16963        5046         11917  \n",
       "421                     True          9291        2721          6570  \n",
       "422                     True         13667        3681          9986  \n",
       "423                     True         11086        3768          7318  \n",
       "\n",
       "[277 rows x 8 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_training_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tf = pd.read_csv(\"clean_trainingfiles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         with open('filename.pickle', 'rb') as handle:\n",
    "#             b = pickle.load(handle)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store all of user 1's test in a dictionary\n",
    "user_1 = array(training_files[training_files['user'] == 1][\"path\"])\n",
    "user_1_tests = {}\n",
    "\n",
    "for test in user_1:\n",
    "    dataset = test + \"/\" + file\n",
    "    user_1_tests[extract_test_number(test)] = dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(df, slider=1, sample_size=120):\n",
    "    \"\"\"\n",
    "    Method for creating samples within a dataset\n",
    "    :df: test that is being sampled\n",
    "    :slider: the amount by which the window slides during sampling. The lower the number, the more samples.\n",
    "    \"\"\"\n",
    "    df = df.drop([\"Timestamp\", \" AdjustedUnix\"], axis=1) #remove unnecessary columns\n",
    "    Sample = namedtuple('Sample', 'inputs effort attention interest')\n",
    "    sampled_tests = []\n",
    "    \n",
    "    \n",
    "    # Group by paragraph and add each paragraph into an array\n",
    "    paragraphs = df.groupby('para') \n",
    "    paragraphs = [paragraphs.get_group(x) for x in paragraphs.groups]\n",
    "    \n",
    "    incorrect_length = 0\n",
    "    # Loop trough each paragraph to create samples\n",
    "    for para in paragraphs:\n",
    "        \n",
    "        if not len(para) > sample_size: # check the length of paragraph if it is bigger than the sample size\n",
    "            print (\"invalid\")\n",
    "            continue\n",
    "            \n",
    "        new_sample_length = len(para[0:sample_size])\n",
    "        counter = 0\n",
    "        while  new_sample_length >= sample_size:\n",
    "            \"\"\"\n",
    "            **Sliding window algorithm**\n",
    "            - Create new samples based on sample size and iterate using the slider size for size of overlap\n",
    "            - Create separate values for inputs, effort, attention, interest to add to a tuple\n",
    "            \"\"\"\n",
    "            new_sample = para[counter : counter + sample_size]\n",
    "            new_sample_length = len(new_sample)\n",
    "            \n",
    "            #checks new_sample length\n",
    "            if new_sample_length == sample_size:\n",
    "                # Extract the sample specific data\n",
    "                inputs = array(new_sample.iloc[:, :8])\n",
    "                effort, attention, interest = new_sample[[\"effort\", \"attention\", \"interest\"]].T.values\n",
    "                sampled_tests.append(Sample(inputs, int(max(effort)), int(max(attention)), int(max(interest))))\n",
    "            else:\n",
    "                incorrect_length += 1\n",
    "                continue             \n",
    "         \n",
    "            # increase by slider\n",
    "            counter += slider\n",
    "        \n",
    "        \n",
    "    sampled_tests_df = pd.DataFrame(sampled_tests)\n",
    "    inputs_and_labels = {}\n",
    "    \n",
    "    inputs_list = sampled_tests_df['inputs'].values\n",
    "    inputs_list = np.rollaxis(np.dstack(inputs_list),-1)\n",
    "    inputs_and_labels['inputs'] = inputs_list\n",
    "    labels = [\"effort\", \"attention\", \"interest\"]\n",
    "    for label in labels: \n",
    "        inputs_and_labels[label] = array(sampled_tests_df[label].values)\n",
    "    \n",
    "    \n",
    "    return inputs_and_labels\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_samples(clean_tf):\n",
    "    \"\"\"\n",
    "    Method for generating samples for all the tests with a default sample size of 120 and slide of 1. \n",
    "    Saves the sample as dictionary using pickle in the relevant directory\n",
    "    :clean_tf: clean training files\n",
    "    \"\"\"\n",
    "    test_paths = array(clean_tf['path'])\n",
    "    file = \"annotated_EEG.csv\"\n",
    "    sampled_file = \"sampled_annotated_EEG.pickle\"\n",
    "    for test_path in test_paths:\n",
    "        test_file = test_path + \"/\" + file\n",
    "        test_dataset = pd.read_csv(test_file)\n",
    "        sampled_test_dataset = get_samples(test_dataset)\n",
    "        with open(test_path + \"/\" + sampled_file, 'wb') as handle:            \n",
    "            pickle.dump(sampled_test_dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_all_samples(clean_tf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
